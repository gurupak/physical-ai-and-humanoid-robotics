"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[1294],{8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}},9414:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-4-vla/vision-integration","title":"4.4 Vision Integration & Object Detection","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/04-vision-integration.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/vision-integration","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/04-vision-integration.md","tags":[{"inline":true,"label":"Computer Vision","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/computer-vision"},{"inline":true,"label":"CLIP","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/clip"},{"inline":true,"label":"Object Detection","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/object-detection"},{"inline":true,"label":"Depth Sensing","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/depth-sensing"},{"inline":true,"label":"Multimodal Fusion","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/multimodal-fusion"}],"version":"current","sidebarPosition":4,"frontMatter":{"id":"vision-integration","title":"4.4 Vision Integration & Object Detection","sidebar_label":"Vision Integration","sidebar_position":4,"sidebar_custom_props":{"difficulty":"Intermediate","readingTime":"16 minutes","prerequisites":["Computer vision basics","ROS 2 camera topics","NumPy and OpenCV"]},"tags":["Computer Vision","CLIP","Object Detection","Depth Sensing","Multimodal Fusion"]},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning"},"next":{"title":"Capstone Project","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/capstone-project"}}');var s=r(4848),t=r(8453);const o={id:"vision-integration",title:"4.4 Vision Integration & Object Detection",sidebar_label:"Vision Integration",sidebar_position:4,sidebar_custom_props:{difficulty:"Intermediate",readingTime:"16 minutes",prerequisites:["Computer vision basics","ROS 2 camera topics","NumPy and OpenCV"]},tags:["Computer Vision","CLIP","Object Detection","Depth Sensing","Multimodal Fusion"]},a="4.4 Vision Integration & Object Detection",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Vision Backbones in VLA Models",id:"vision-backbones-in-vla-models",level:2},{value:"Why Vision Matters for VLA",id:"why-vision-matters-for-vla",level:3},{value:"Vision Backbone Comparison",id:"vision-backbone-comparison",level:3},{value:"CLIP Architecture",id:"clip-architecture",level:3},{value:"ROS 2 Camera Integration",id:"ros-2-camera-integration",level:2},{value:"Camera Data Flow",id:"camera-data-flow",level:3},{value:"Complete Vision Node Implementation",id:"complete-vision-node-implementation",level:3},{value:"Testing the Vision Node",id:"testing-the-vision-node",level:3},{value:"Object Detection for Manipulation",id:"object-detection-for-manipulation",level:2},{value:"From Detection to 3D Localization",id:"from-detection-to-3d-localization",level:3},{value:"3D Localization Implementation",id:"3d-localization-implementation",level:3},{value:"Enhanced Vision Node with 3D Localization",id:"enhanced-vision-node-with-3d-localization",level:3},{value:"Multimodal Fusion",id:"multimodal-fusion",level:2},{value:"Combining Vision + Language Features",id:"combining-vision--language-features",level:3},{value:"Simple Fusion Implementation",id:"simple-fusion-implementation",level:3},{value:"Common Errors",id:"common-errors",level:2},{value:"Error 1: &quot;No objects detected&quot;",id:"error-1-no-objects-detected",level:3},{value:"Error 2: &quot;Noisy depth measurements&quot;",id:"error-2-noisy-depth-measurements",level:3},{value:"Error 3: &quot;High vision processing latency&quot;",id:"error-3-high-vision-processing-latency",level:3},{value:"Error 4: &quot;Camera calibration errors&quot;",id:"error-4-camera-calibration-errors",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Swap Vision Model (Easy, 30 minutes)",id:"exercise-1-swap-vision-model-easy-30-minutes",level:3},{value:"Exercise 2: Color-based Filtering (Medium, 45 minutes)",id:"exercise-2-color-based-filtering-medium-45-minutes",level:3},{value:"Exercise 3: Kalman Filter Tracking (Hard, 60 minutes)",id:"exercise-3-kalman-filter-tracking-hard-60-minutes",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"44-vision-integration--object-detection",children:"4.4 Vision Integration & Object Detection"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By completing this sub-chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integrate"})," vision backbones (CLIP, PaliGemma) into VLA pipelines for object recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Process"})," ROS 2 camera streams and convert them for vision model inference"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," 3D object localization using depth sensors and camera intrinsics"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Combine"})," visual and language features through multimodal fusion architectures"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"vision-backbones-in-vla-models",children:"Vision Backbones in VLA Models"}),"\n",(0,s.jsx)(n.h3,{id:"why-vision-matters-for-vla",children:"Why Vision Matters for VLA"}),"\n",(0,s.jsx)(n.p,{children:'When you say "Pick up the red cup," the robot needs to:'}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"See"})," the environment through cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Identify"}),' which object is the "red cup"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Localize"})," the cup's 3D position for grasping"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verify"})," the action succeeded after execution"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:['Vision enables robots to ground natural language commands in the physical world. Without vision, "pick up the cup" is just text\u2014with vision, the robot knows ',(0,s.jsx)(n.strong,{children:"which cup, where it is, and how to grasp it"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"vision-backbone-comparison",children:"Vision Backbone Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Type"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"Strengths"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"CLIP"})}),(0,s.jsx)(n.td,{children:"Vision-Language"}),(0,s.jsx)(n.td,{children:"400M"}),(0,s.jsx)(n.td,{children:"Zero-shot classification, fast inference"}),(0,s.jsx)(n.td,{children:"Object identification from text"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"PaliGemma"})}),(0,s.jsx)(n.td,{children:"Vision-Language"}),(0,s.jsx)(n.td,{children:"3B"}),(0,s.jsx)(n.td,{children:"Spatial reasoning, detailed captions"}),(0,s.jsx)(n.td,{children:"Scene understanding"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Grounded-SAM2"})}),(0,s.jsx)(n.td,{children:"Vision-Language-Segmentation"}),(0,s.jsx)(n.td,{children:"1.2B"}),(0,s.jsx)(n.td,{children:"Pixel-level segmentation, prompting"}),(0,s.jsx)(n.td,{children:"Precise object boundaries"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"DINOv2"})}),(0,s.jsx)(n.td,{children:"Vision-only"}),(0,s.jsx)(n.td,{children:"300M"}),(0,s.jsx)(n.td,{children:"Feature extraction, no labels needed"}),(0,s.jsx)(n.td,{children:"Visual embeddings"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Recommendation for Robotics"}),": Start with ",(0,s.jsx)(n.strong,{children:"CLIP"})," for object identification (fast, accurate) + ",(0,s.jsx)(n.strong,{children:"Grounded-SAM2"})," for segmentation when you need precise boundaries for grasping."]}),"\n",(0,s.jsx)(n.h3,{id:"clip-architecture",children:"CLIP Architecture"}),"\n",(0,s.jsx)(n.p,{children:"CLIP (Contrastive Language-Image Pre-training) learns to match images with text descriptions:"}),"\n",(0,s.jsx)(n.mermaid,{value:"flowchart LR\r\n    A[Image] --\x3e|Vision Encoder| B[Image Features 512D]\r\n    C[Text: 'red cup'] --\x3e|Text Encoder| D[Text Features 512D]\r\n    B --\x3e|Cosine Similarity| E[Match Score]\r\n    D --\x3e|Cosine Similarity| E\r\n    E --\x3e|> 0.7| F[Object Identified]\r\n    \r\n    style A fill:#E3F2FD\r\n    style C fill:#F3E5F5\r\n    style F fill:#E8F5E9"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision Encoder"})," (ViT or ResNet) converts image \u2192 512D vector"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Text Encoder"}),' (Transformer) converts "red cup" \u2192 512D vector']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cosine similarity"})," measures how well image matches text"]}),"\n",(0,s.jsx)(n.li,{children:"High similarity (>0.7) = object identified"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Advantage"}),": Zero-shot recognition\u2014can identify objects never seen during training by comparing to text descriptions."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-camera-integration",children:"ROS 2 Camera Integration"}),"\n",(0,s.jsx)(n.h3,{id:"camera-data-flow",children:"Camera Data Flow"}),"\n",(0,s.jsx)(n.mermaid,{value:"sequenceDiagram\r\n    participant Camera as Camera Driver\r\n    participant Bridge as cv_bridge\r\n    participant VisionNode as Vision Node\r\n    participant CLIP as CLIP Model\r\n    participant PlanNode as Planning Node\r\n    \r\n    Camera->>Bridge: sensor_msgs/Image (RGB)\r\n    Bridge->>VisionNode: cv2.Mat (numpy array)\r\n    VisionNode->>CLIP: Preprocessed tensor\r\n    CLIP->>VisionNode: Object detections\r\n    VisionNode->>PlanNode: ObjectDetection msg\r\n    PlanNode->>PlanNode: Update world model"}),"\n",(0,s.jsx)(n.h3,{id:"complete-vision-node-implementation",children:"Complete Vision Node Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# vision_node.py\r\n# ROS 2 node for CLIP-based object detection\r\n# Requirements: torch, torchvision, transformers, cv_bridge\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport torch\r\nfrom transformers import CLIPProcessor, CLIPModel\r\nfrom PIL import Image as PILImage\r\nimport json\r\n\r\nclass VisionNode(Node):\r\n    """\r\n    Processes camera images and detects objects using CLIP.\r\n    \r\n    Subscribes:\r\n        /camera/color/image_raw (sensor_msgs/Image): RGB camera feed\r\n        /vision/query (std_msgs/String): Object to search for (e.g., "red cup")\r\n    \r\n    Publishes:\r\n        /vision/detections (std_msgs/String): JSON with detected objects\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'vision_node\')\r\n        \r\n        # Parameters\r\n        self.declare_parameter(\'model_name\', \'openai/clip-vit-base-patch32\')\r\n        self.declare_parameter(\'confidence_threshold\', 0.25)\r\n        self.declare_parameter(\'device\', \'cuda\' if torch.cuda.is_available() else \'cpu\')\r\n        \r\n        model_name = self.get_parameter(\'model_name\').value\r\n        self.confidence_threshold = self.get_parameter(\'confidence_threshold\').value\r\n        device = self.get_parameter(\'device\').value\r\n        \r\n        # Load CLIP model\r\n        self.get_logger().info(f"Loading CLIP model: {model_name}")\r\n        self.model = CLIPModel.from_pretrained(model_name).to(device)\r\n        self.processor = CLIPProcessor.from_pretrained(model_name)\r\n        self.device = device\r\n        \r\n        # CV Bridge for ROS image conversion\r\n        self.bridge = CvBridge()\r\n        \r\n        # Current search query\r\n        self.search_query = ["object"]  # Default fallback\r\n        \r\n        # Subscribers\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/color/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        self.query_sub = self.create_subscription(\r\n            String,\r\n            \'/vision/query\',\r\n            self.query_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.detection_pub = self.create_publisher(String, \'/vision/detections\', 10)\r\n        \r\n        self.get_logger().info(f"Vision Node ready on {device}")\r\n    \r\n    def query_callback(self, msg):\r\n        """Update search query from planning node"""\r\n        self.search_query = [msg.data]\r\n        self.get_logger().info(f"Updated search query: {msg.data}")\r\n    \r\n    def image_callback(self, msg):\r\n        """Process incoming camera image"""\r\n        try:\r\n            # Convert ROS Image to OpenCV format\r\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\r\n            \r\n            # Convert to PIL Image for CLIP\r\n            pil_image = PILImage.fromarray(cv_image)\r\n            \r\n            # Run detection\r\n            detections = self.detect_objects(pil_image, self.search_query)\r\n            \r\n            # Publish results\r\n            if detections:\r\n                detection_msg = String()\r\n                detection_msg.data = json.dumps(detections)\r\n                self.detection_pub.publish(detection_msg)\r\n                \r\n                self.get_logger().info(f"Detected: {detections}")\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f"Image processing error: {e}")\r\n    \r\n    def detect_objects(self, image, text_queries):\r\n        """\r\n        Detect objects in image matching text queries.\r\n        \r\n        Args:\r\n            image: PIL Image\r\n            text_queries: List of text descriptions (e.g., ["red cup", "blue box"])\r\n        \r\n        Returns:\r\n            List of detections with confidence scores\r\n        """\r\n        # Preprocess inputs\r\n        inputs = self.processor(\r\n            text=text_queries,\r\n            images=image,\r\n            return_tensors="pt",\r\n            padding=True\r\n        ).to(self.device)\r\n        \r\n        # Run inference\r\n        with torch.no_grad():\r\n            outputs = self.model(**inputs)\r\n            logits_per_image = outputs.logits_per_image\r\n            probs = logits_per_image.softmax(dim=1)\r\n        \r\n        # Extract detections above threshold\r\n        detections = []\r\n        for i, query in enumerate(text_queries):\r\n            confidence = probs[0][i].item()\r\n            if confidence > self.confidence_threshold:\r\n                detections.append({\r\n                    "object": query,\r\n                    "confidence": round(confidence, 3)\r\n                })\r\n        \r\n        return detections\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VisionNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"testing-the-vision-node",children:"Testing the Vision Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start camera driver (or simulator)\r\nros2 run usb_cam usb_cam_node_exe\r\n\r\n# Terminal 2: Run vision node\r\npython vision_node.py\r\n\r\n# Terminal 3: Send search query\r\nros2 topic pub /vision/query std_msgs/String \"data: 'red cup'\" --once\r\n\r\n# Terminal 4: Monitor detections\r\nros2 topic echo /vision/detections\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\r\n  "object": "red cup",\r\n  "confidence": 0.873\r\n}\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"object-detection-for-manipulation",children:"Object Detection for Manipulation"}),"\n",(0,s.jsx)(n.h3,{id:"from-detection-to-3d-localization",children:"From Detection to 3D Localization"}),"\n",(0,s.jsxs)(n.p,{children:["CLIP tells us ",(0,s.jsx)(n.strong,{children:"what"})," is in the image, but not ",(0,s.jsx)(n.strong,{children:"where"})," in 3D space. For manipulation, we need (x, y, z) coordinates."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CLIP"}),": Identify object in RGB image \u2192 get 2D bounding box"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Depth sensor"}),": Get depth value at object center"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Camera intrinsics"}),": Unproject 2D pixel + depth \u2192 3D point"]}),"\n"]}),"\n",(0,s.jsx)(n.mermaid,{value:"flowchart TD\r\n    A[RGB Image] --\x3e|CLIP| B[2D Bounding Box]\r\n    C[Depth Image] --\x3e|Lookup| D[Depth Value]\r\n    B --\x3e|Center Pixel u,v| E[2D Point]\r\n    D --\x3e|Depth Z| E\r\n    F[Camera Intrinsics K] --\x3e|Unproject| G[3D Point x,y,z]\r\n    E --\x3e|Combine| G\r\n    \r\n    style A fill:#E3F2FD\r\n    style C fill:#E3F2FD\r\n    style G fill:#E8F5E9"}),"\n",(0,s.jsx)(n.h3,{id:"3d-localization-implementation",children:"3D Localization Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# localization_helper.py\r\nimport numpy as np\r\n\r\nclass Camera3DLocalizer:\r\n    """Convert 2D image coordinates + depth to 3D world coordinates"""\r\n    \r\n    def __init__(self, camera_intrinsics):\r\n        """\r\n        Args:\r\n            camera_intrinsics: Dict with keys \'fx\', \'fy\', \'cx\', \'cy\'\r\n                fx, fy: Focal lengths in pixels\r\n                cx, cy: Principal point (image center)\r\n        """\r\n        self.fx = camera_intrinsics[\'fx\']\r\n        self.fy = camera_intrinsics[\'fy\']\r\n        self.cx = camera_intrinsics[\'cx\']\r\n        self.cy = camera_intrinsics[\'cy\']\r\n    \r\n    def unproject(self, u, v, depth):\r\n        """\r\n        Convert 2D pixel (u, v) + depth to 3D point (x, y, z).\r\n        \r\n        Args:\r\n            u: Pixel x-coordinate (column)\r\n            v: Pixel y-coordinate (row)\r\n            depth: Depth value in meters\r\n        \r\n        Returns:\r\n            np.array: [x, y, z] in camera frame (meters)\r\n        """\r\n        # Pinhole camera model: x = (u - cx) * Z / fx\r\n        x = (u - self.cx) * depth / self.fx\r\n        y = (v - self.cy) * depth / self.fy\r\n        z = depth\r\n        \r\n        return np.array([x, y, z])\r\n    \r\n    def project(self, point_3d):\r\n        """\r\n        Project 3D point to 2D pixel coordinates.\r\n        \r\n        Args:\r\n            point_3d: np.array [x, y, z] in camera frame\r\n        \r\n        Returns:\r\n            (u, v): Pixel coordinates\r\n        """\r\n        x, y, z = point_3d\r\n        u = int(self.fx * x / z + self.cx)\r\n        v = int(self.fy * y / z + self.cy)\r\n        return (u, v)\r\n\r\n# Example usage\r\nintrinsics = {\r\n    \'fx\': 615.0,  # RealSense D435 typical values\r\n    \'fy\': 615.0,\r\n    \'cx\': 320.0,\r\n    \'cy\': 240.0\r\n}\r\n\r\nlocalizer = Camera3DLocalizer(intrinsics)\r\n\r\n# Object detected at pixel (350, 200) with depth 0.8m\r\npixel_u, pixel_v = 350, 200\r\ndepth_m = 0.8\r\n\r\npoint_3d = localizer.unproject(pixel_u, pixel_v, depth_m)\r\nprint(f"3D position: x={point_3d[0]:.3f}, y={point_3d[1]:.3f}, z={point_3d[2]:.3f}")\r\n# Output: 3D position: x=0.039, y=-0.052, z=0.800\n'})}),"\n",(0,s.jsx)(n.h3,{id:"enhanced-vision-node-with-3d-localization",children:"Enhanced Vision Node with 3D Localization"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# vision_node_3d.py\r\nfrom sensor_msgs.msg import Image\r\nfrom geometry_msgs.msg import Point\r\nimport numpy as np\r\n\r\nclass VisionNode3D(VisionNode):\r\n    \"\"\"Extended vision node with 3D localization\"\"\"\r\n    \r\n    def __init__(self):\r\n        super().__init__()\r\n        \r\n        # Subscribe to depth\r\n        self.depth_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/depth/image_raw',\r\n            self.depth_callback,\r\n            10\r\n        )\r\n        \r\n        # Publisher for 3D positions\r\n        self.position_pub = self.create_publisher(Point, '/vision/object_position', 10)\r\n        \r\n        # Cache latest depth image\r\n        self.latest_depth = None\r\n        \r\n        # Camera intrinsics (load from calibration file in practice)\r\n        self.localizer = Camera3DLocalizer({\r\n            'fx': 615.0, 'fy': 615.0, 'cx': 320.0, 'cy': 240.0\r\n        })\r\n    \r\n    def depth_callback(self, msg):\r\n        \"\"\"Cache depth image\"\"\"\r\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\r\n    \r\n    def get_object_3d_position(self, bbox, depth_image):\r\n        \"\"\"\r\n        Get 3D position of object from bounding box + depth.\r\n        \r\n        Args:\r\n            bbox: Dict with 'x_min', 'y_min', 'x_max', 'y_max'\r\n            depth_image: Depth image (numpy array)\r\n        \r\n        Returns:\r\n            np.array: [x, y, z] 3D position\r\n        \"\"\"\r\n        # Get bounding box center\r\n        center_u = int((bbox['x_min'] + bbox['x_max']) / 2)\r\n        center_v = int((bbox['y_min'] + bbox['y_max']) / 2)\r\n        \r\n        # Get median depth in central region (more robust than single pixel)\r\n        roi_size = 10\r\n        depth_roi = depth_image[\r\n            center_v-roi_size:center_v+roi_size,\r\n            center_u-roi_size:center_u+roi_size\r\n        ]\r\n        depth_m = np.median(depth_roi[depth_roi > 0])  # Ignore zero depths\r\n        \r\n        # Unproject to 3D\r\n        return self.localizer.unproject(center_u, center_v, depth_m)\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"multimodal-fusion",children:"Multimodal Fusion"}),"\n",(0,s.jsx)(n.h3,{id:"combining-vision--language-features",children:"Combining Vision + Language Features"}),"\n",(0,s.jsx)(n.p,{children:"VLA models fuse visual and language features to enable grounding:"}),"\n",(0,s.jsx)(n.mermaid,{value:"flowchart TD\r\n    A[Image Features 512D] --\x3e|Projection| C[Shared Space 768D]\r\n    B[Text Features 512D] --\x3e|Projection| C\r\n    C --\x3e|Cross-Attention| D[Fused Features 768D]\r\n    D --\x3e|Action Decoder| E[Robot Actions]\r\n    \r\n    style A fill:#E3F2FD\r\n    style B fill:#F3E5F5\r\n    style D fill:#FFF3E0\r\n    style E fill:#E8F5E9"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Cross-Attention Mechanism"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Query"}),': Language features ("pick up")']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Key/Value"}),": Vision features (object locations)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Output"}),": Grounded representation (language + visual context)"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"simple-fusion-implementation",children:"Simple Fusion Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# multimodal_fusion.py\r\nimport torch\r\nimport torch.nn as nn\r\n\r\nclass SimpleVLFusion(nn.Module):\r\n    """Fuse vision and language features for VLA"""\r\n    \r\n    def __init__(self, vision_dim=512, language_dim=512, hidden_dim=768):\r\n        super().__init__()\r\n        \r\n        # Project to shared dimensionality\r\n        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\r\n        self.language_proj = nn.Linear(language_dim, hidden_dim)\r\n        \r\n        # Cross-attention layer\r\n        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8)\r\n        \r\n        # Output projection\r\n        self.output_proj = nn.Linear(hidden_dim, 256)\r\n    \r\n    def forward(self, vision_features, language_features):\r\n        """\r\n        Args:\r\n            vision_features: Tensor [batch, vision_dim]\r\n            language_features: Tensor [batch, language_dim]\r\n        \r\n        Returns:\r\n            Fused features [batch, 256]\r\n        """\r\n        # Project to shared space\r\n        vision_emb = self.vision_proj(vision_features).unsqueeze(0)  # [1, batch, hidden]\r\n        language_emb = self.language_proj(language_features).unsqueeze(0)\r\n        \r\n        # Cross-attend: language attends to vision\r\n        fused, _ = self.cross_attention(\r\n            query=language_emb,\r\n            key=vision_emb,\r\n            value=vision_emb\r\n        )\r\n        \r\n        # Project to output\r\n        output = self.output_proj(fused.squeeze(0))\r\n        return output\r\n\r\n# Example usage\r\nmodel = SimpleVLFusion()\r\n\r\n# Simulate CLIP features\r\nvision_feat = torch.randn(1, 512)  # From CLIP vision encoder\r\nlanguage_feat = torch.randn(1, 512)  # From CLIP text encoder\r\n\r\nfused = model(vision_feat, language_feat)\r\nprint(f"Fused features shape: {fused.shape}")  # [1, 256]\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"common-errors",children:"Common Errors"}),"\n",(0,s.jsx)(n.h3,{id:"error-1-no-objects-detected",children:'Error 1: "No objects detected"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"CLIP returns low confidence for all queries"}),"\n",(0,s.jsx)(n.li,{children:"Empty detection results"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Image too dark/bright"}),"\n",(0,s.jsx)(n.li,{children:"Object outside camera field of view"}),"\n",(0,s.jsx)(n.li,{children:'Text query too specific ("crimson cup" vs "red cup")'}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Adjust confidence threshold"}),": Lower from 0.7 to 0.3"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Try multiple queries"}),': ["cup", "red object", "container"]']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Check image preprocessing"}),": Ensure RGB not BGR"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Multi-query fallback\r\nqueries = ["red cup", "cup", "red object", "mug"]\r\nfor query in queries:\r\n    detections = detect_objects(image, [query])\r\n    if detections:\r\n        break\n'})}),"\n",(0,s.jsx)(n.h3,{id:"error-2-noisy-depth-measurements",children:'Error 2: "Noisy depth measurements"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"3D positions jump randomly"}),"\n",(0,s.jsx)(n.li,{children:"Depth values at object edges are incorrect"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Depth sensor noise (especially on reflective/transparent objects)"}),"\n",(0,s.jsx)(n.li,{children:"Using single pixel depth (not robust)"}),"\n",(0,s.jsx)(n.li,{children:"Object edges have invalid depth"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Solution: Median filtering in ROI\r\ndef get_robust_depth(depth_image, center_u, center_v, roi_size=15):\r\n    """Get robust depth using median filter"""\r\n    roi = depth_image[\r\n        center_v-roi_size:center_v+roi_size,\r\n        center_u-roi_size:center_u+roi_size\r\n    ]\r\n    # Filter out zeros and outliers\r\n    valid_depths = roi[(roi > 0.1) & (roi < 5.0)]\r\n    return np.median(valid_depths) if len(valid_depths) > 0 else None\n'})}),"\n",(0,s.jsx)(n.h3,{id:"error-3-high-vision-processing-latency",children:'Error 3: "High vision processing latency"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Camera callback queue fills up"}),"\n",(0,s.jsx)(n.li,{children:"Detections lag behind live feed"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Running CLIP on CPU (slow)"}),"\n",(0,s.jsx)(n.li,{children:"Processing every frame (30 FPS = 30 inferences/sec)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Solution: Throttle processing rate\r\nimport time\r\n\r\nclass VisionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('vision_node')\r\n        self.last_process_time = 0\r\n        self.process_interval = 0.5  # Process every 500ms\r\n    \r\n    def image_callback(self, msg):\r\n        # Throttle to 2 Hz\r\n        now = time.time()\r\n        if now - self.last_process_time < self.process_interval:\r\n            return  # Skip this frame\r\n        \r\n        self.last_process_time = now\r\n        # Process image...\n"})}),"\n",(0,s.jsx)(n.h3,{id:"error-4-camera-calibration-errors",children:'Error 4: "Camera calibration errors"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"3D positions systematically offset"}),"\n",(0,s.jsx)(n.li,{children:"Grasp attempts miss object"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Wrong camera intrinsics (fx, fy, cx, cy)"}),"\n",(0,s.jsx)(n.li,{children:"Camera moved after calibration"}),"\n",(0,s.jsx)(n.li,{children:"Using default values instead of calibrated"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Calibrate camera using ROS 2 camera_calibration\r\nros2 run camera_calibration cameracalibrator --size 8x6 --square 0.025\r\n\r\n# Save intrinsics to YAML, load in code\r\nimport yaml\r\nwith open('camera_info.yaml') as f:\r\n    calib = yaml.safe_load(f)\r\n    intrinsics = {\r\n        'fx': calib['camera_matrix']['data'][0],\r\n        'fy': calib['camera_matrix']['data'][4],\r\n        'cx': calib['camera_matrix']['data'][2],\r\n        'cy': calib['camera_matrix']['data'][5]\r\n    }\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-swap-vision-model-easy-30-minutes",children:"Exercise 1: Swap Vision Model (Easy, 30 minutes)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Replace CLIP with a different vision-language model."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Try PaliGemma instead of CLIP"}),"\n",(0,s.jsx)(n.li,{children:"Compare detection accuracy on test images"}),"\n",(0,s.jsx)(n.li,{children:"Measure inference latency difference"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\r\n\r\nmodel = PaliGemmaForConditionalGeneration.from_pretrained("google/paligemma-3b-pt-224")\r\nprocessor = PaliGemmaProcessor.from_pretrained("google/paligemma-3b-pt-224")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-color-based-filtering-medium-45-minutes",children:"Exercise 2: Color-based Filtering (Medium, 45 minutes)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),': Enhance CLIP with HSV color filtering for "red cup" vs "blue cup".']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Convert RGB image to HSV color space"}),"\n",(0,s.jsx)(n.li,{children:"Create color masks for red/blue/green"}),"\n",(0,s.jsx)(n.li,{children:"Combine CLIP detection with color verification"}),"\n",(0,s.jsx)(n.li,{children:"Reject detections if color doesn't match"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-kalman-filter-tracking-hard-60-minutes",children:"Exercise 3: Kalman Filter Tracking (Hard, 60 minutes)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Track object 3D positions over time with Kalman filtering."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement Kalman filter for 3D position tracking"}),"\n",(0,s.jsx)(n.li,{children:"Predict object position between detections"}),"\n",(0,s.jsx)(n.li,{children:"Handle temporary occlusions"}),"\n",(0,s.jsx)(n.li,{children:"Smooth noisy depth measurements"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"CLIP: Learning Transferable Visual Models"})," (Radford et al., 2021)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2103.00020",children:"https://arxiv.org/abs/2103.00020"})}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"PaliGemma: A versatile vision language model"})," (Google, 2024)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2407.07726",children:"https://arxiv.org/abs/2407.07726"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ROS 2 vision_opencv Package"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/ros-perception/vision_opencv",children:"https://github.com/ros-perception/vision_opencv"})}),"\n",(0,s.jsx)(n.li,{children:"cv_bridge for image conversion"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Intel RealSense ROS 2"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/IntelRealSense/realsense-ros",children:"https://github.com/IntelRealSense/realsense-ros"})}),"\n",(0,s.jsx)(n.li,{children:"Depth camera integration"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["Vision complete! In the final sub-chapter, ",(0,s.jsx)(n.a,{href:"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/capstone-project",children:"4.5 Capstone Project: Autonomous Humanoid"}),", you'll integrate voice, planning, and vision into a complete end-to-end VLA system."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"\u2705 CLIP enables zero-shot object recognition from text"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Depth sensors + camera intrinsics provide 3D localization"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Multimodal fusion grounds language in visual context"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Median filtering makes depth measurements robust"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Throttle vision processing to 2-5 Hz for real-time performance"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);