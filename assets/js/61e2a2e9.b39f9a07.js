"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[1294],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}},9414:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-4-vla/vision-integration","title":"4.4 Vision Integration & Object Detection","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/04-vision-integration.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/vision-integration","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/04-vision-integration.md","tags":[{"inline":true,"label":"Computer Vision","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/computer-vision"},{"inline":true,"label":"CLIP","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/clip"},{"inline":true,"label":"Object Detection","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/object-detection"},{"inline":true,"label":"Depth Sensing","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/depth-sensing"},{"inline":true,"label":"Multimodal Fusion","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/multimodal-fusion"}],"version":"current","sidebarPosition":4,"frontMatter":{"id":"vision-integration","title":"4.4 Vision Integration & Object Detection","sidebar_label":"Vision Integration","sidebar_position":4,"sidebar_custom_props":{"difficulty":"Intermediate","readingTime":"16 minutes","prerequisites":["Computer vision basics","ROS 2 camera topics","NumPy and OpenCV"]},"tags":["Computer Vision","CLIP","Object Detection","Depth Sensing","Multimodal Fusion"]},"sidebar":"tutorialSidebar","previous":{"title":"Cognitive Planning","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning"},"next":{"title":"Capstone Project","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/capstone-project"}}');var r=i(4848),t=i(8453);const o={id:"vision-integration",title:"4.4 Vision Integration & Object Detection",sidebar_label:"Vision Integration",sidebar_position:4,sidebar_custom_props:{difficulty:"Intermediate",readingTime:"16 minutes",prerequisites:["Computer vision basics","ROS 2 camera topics","NumPy and OpenCV"]},tags:["Computer Vision","CLIP","Object Detection","Depth Sensing","Multimodal Fusion"]},a="4.4 Vision Integration & Object Detection",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Vision Backbones in VLA Models",id:"vision-backbones-in-vla-models",level:2},{value:"Why Vision Matters for VLA",id:"why-vision-matters-for-vla",level:3},{value:"Vision Backbone Comparison",id:"vision-backbone-comparison",level:3},{value:"CLIP Architecture",id:"clip-architecture",level:3},{value:"ROS 2 Camera Integration",id:"ros-2-camera-integration",level:2},{value:"Camera Data Flow",id:"camera-data-flow",level:3},{value:"Complete Vision Node Implementation",id:"complete-vision-node-implementation",level:3},{value:"Testing the Vision Node",id:"testing-the-vision-node",level:3},{value:"Object Detection for Manipulation",id:"object-detection-for-manipulation",level:2},{value:"From Detection to 3D Localization",id:"from-detection-to-3d-localization",level:3},{value:"3D Localization Implementation",id:"3d-localization-implementation",level:3},{value:"Enhanced Vision Node with 3D Localization",id:"enhanced-vision-node-with-3d-localization",level:3},{value:"Multimodal Fusion",id:"multimodal-fusion",level:2},{value:"Combining Vision + Language Features",id:"combining-vision--language-features",level:3},{value:"Simple Fusion Implementation",id:"simple-fusion-implementation",level:3},{value:"Common Errors",id:"common-errors",level:2},{value:"Error 1: &quot;No objects detected&quot;",id:"error-1-no-objects-detected",level:3},{value:"Error 2: &quot;Noisy depth measurements&quot;",id:"error-2-noisy-depth-measurements",level:3},{value:"Error 3: &quot;High vision processing latency&quot;",id:"error-3-high-vision-processing-latency",level:3},{value:"Error 4: &quot;Camera calibration errors&quot;",id:"error-4-camera-calibration-errors",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Swap Vision Model (Easy, 30 minutes)",id:"exercise-1-swap-vision-model-easy-30-minutes",level:3},{value:"Exercise 2: Color-based Filtering (Medium, 45 minutes)",id:"exercise-2-color-based-filtering-medium-45-minutes",level:3},{value:"Exercise 3: Kalman Filter Tracking (Hard, 60 minutes)",id:"exercise-3-kalman-filter-tracking-hard-60-minutes",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"44-vision-integration--object-detection",children:"4.4 Vision Integration & Object Detection"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By completing this sub-chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate"})," vision backbones (CLIP, PaliGemma) into VLA pipelines for object recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Process"})," ROS 2 camera streams and convert them for vision model inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement"})," 3D object localization using depth sensors and camera intrinsics"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Combine"})," visual and language features through multimodal fusion architectures"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"vision-backbones-in-vla-models",children:"Vision Backbones in VLA Models"}),"\n",(0,r.jsx)(n.h3,{id:"why-vision-matters-for-vla",children:"Why Vision Matters for VLA"}),"\n",(0,r.jsx)(n.p,{children:'When you say "Pick up the red cup," the robot needs to:'}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"See"})," the environment through cameras"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Identify"}),' which object is the "red cup"']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Localize"})," the cup's 3D position for grasping"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Verify"})," the action succeeded after execution"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:['Vision enables robots to ground natural language commands in the physical world. Without vision, "pick up the cup" is just text\u2014with vision, the robot knows ',(0,r.jsx)(n.strong,{children:"which cup, where it is, and how to grasp it"}),"."]}),"\n",(0,r.jsx)(n.h3,{id:"vision-backbone-comparison",children:"Vision Backbone Comparison"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Type"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"Strengths"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"CLIP"})}),(0,r.jsx)(n.td,{children:"Vision-Language"}),(0,r.jsx)(n.td,{children:"400M"}),(0,r.jsx)(n.td,{children:"Zero-shot classification, fast inference"}),(0,r.jsx)(n.td,{children:"Object identification from text"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"PaliGemma"})}),(0,r.jsx)(n.td,{children:"Vision-Language"}),(0,r.jsx)(n.td,{children:"3B"}),(0,r.jsx)(n.td,{children:"Spatial reasoning, detailed captions"}),(0,r.jsx)(n.td,{children:"Scene understanding"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Grounded-SAM2"})}),(0,r.jsx)(n.td,{children:"Vision-Language-Segmentation"}),(0,r.jsx)(n.td,{children:"1.2B"}),(0,r.jsx)(n.td,{children:"Pixel-level segmentation, prompting"}),(0,r.jsx)(n.td,{children:"Precise object boundaries"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"DINOv2"})}),(0,r.jsx)(n.td,{children:"Vision-only"}),(0,r.jsx)(n.td,{children:"300M"}),(0,r.jsx)(n.td,{children:"Feature extraction, no labels needed"}),(0,r.jsx)(n.td,{children:"Visual embeddings"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation for Robotics"}),": Start with ",(0,r.jsx)(n.strong,{children:"CLIP"})," for object identification (fast, accurate) + ",(0,r.jsx)(n.strong,{children:"Grounded-SAM2"})," for segmentation when you need precise boundaries for grasping."]}),"\n",(0,r.jsx)(n.h3,{id:"clip-architecture",children:"CLIP Architecture"}),"\n",(0,r.jsx)(n.p,{children:"CLIP (Contrastive Language-Image Pre-training) learns to match images with text descriptions:"}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart LR\n    A[Image] --\x3e|Vision Encoder| B[Image Features 512D]\n    C[Text: 'red cup'] --\x3e|Text Encoder| D[Text Features 512D]\n    B --\x3e|Cosine Similarity| E[Match Score]\n    D --\x3e|Cosine Similarity| E\n    E --\x3e|> 0.7| F[Object Identified]\n    \n    style A fill:#E3F2FD\n    style C fill:#F3E5F5\n    style F fill:#E8F5E9"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision Encoder"})," (ViT or ResNet) converts image \u2192 512D vector"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Text Encoder"}),' (Transformer) converts "red cup" \u2192 512D vector']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Cosine similarity"})," measures how well image matches text"]}),"\n",(0,r.jsx)(n.li,{children:"High similarity (>0.7) = object identified"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Advantage"}),": Zero-shot recognition\u2014can identify objects never seen during training by comparing to text descriptions."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"ros-2-camera-integration",children:"ROS 2 Camera Integration"}),"\n",(0,r.jsx)(n.h3,{id:"camera-data-flow",children:"Camera Data Flow"}),"\n",(0,r.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Camera as Camera Driver\n    participant Bridge as cv_bridge\n    participant VisionNode as Vision Node\n    participant CLIP as CLIP Model\n    participant PlanNode as Planning Node\n    \n    Camera->>Bridge: sensor_msgs/Image (RGB)\n    Bridge->>VisionNode: cv2.Mat (numpy array)\n    VisionNode->>CLIP: Preprocessed tensor\n    CLIP->>VisionNode: Object detections\n    VisionNode->>PlanNode: ObjectDetection msg\n    PlanNode->>PlanNode: Update world model"}),"\n",(0,r.jsx)(n.h3,{id:"complete-vision-node-implementation",children:"Complete Vision Node Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# vision_node.py\n# ROS 2 node for CLIP-based object detection\n# Requirements: torch, torchvision, transformers, cv_bridge\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image as PILImage\nimport json\n\nclass VisionNode(Node):\n    """\n    Processes camera images and detects objects using CLIP.\n    \n    Subscribes:\n        /camera/color/image_raw (sensor_msgs/Image): RGB camera feed\n        /vision/query (std_msgs/String): Object to search for (e.g., "red cup")\n    \n    Publishes:\n        /vision/detections (std_msgs/String): JSON with detected objects\n    """\n    \n    def __init__(self):\n        super().__init__(\'vision_node\')\n        \n        # Parameters\n        self.declare_parameter(\'model_name\', \'openai/clip-vit-base-patch32\')\n        self.declare_parameter(\'confidence_threshold\', 0.25)\n        self.declare_parameter(\'device\', \'cuda\' if torch.cuda.is_available() else \'cpu\')\n        \n        model_name = self.get_parameter(\'model_name\').value\n        self.confidence_threshold = self.get_parameter(\'confidence_threshold\').value\n        device = self.get_parameter(\'device\').value\n        \n        # Load CLIP model\n        self.get_logger().info(f"Loading CLIP model: {model_name}")\n        self.model = CLIPModel.from_pretrained(model_name).to(device)\n        self.processor = CLIPProcessor.from_pretrained(model_name)\n        self.device = device\n        \n        # CV Bridge for ROS image conversion\n        self.bridge = CvBridge()\n        \n        # Current search query\n        self.search_query = ["object"]  # Default fallback\n        \n        # Subscribers\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/color/image_raw\',\n            self.image_callback,\n            10\n        )\n        \n        self.query_sub = self.create_subscription(\n            String,\n            \'/vision/query\',\n            self.query_callback,\n            10\n        )\n        \n        # Publishers\n        self.detection_pub = self.create_publisher(String, \'/vision/detections\', 10)\n        \n        self.get_logger().info(f"Vision Node ready on {device}")\n    \n    def query_callback(self, msg):\n        """Update search query from planning node"""\n        self.search_query = [msg.data]\n        self.get_logger().info(f"Updated search query: {msg.data}")\n    \n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        try:\n            # Convert ROS Image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=\'rgb8\')\n            \n            # Convert to PIL Image for CLIP\n            pil_image = PILImage.fromarray(cv_image)\n            \n            # Run detection\n            detections = self.detect_objects(pil_image, self.search_query)\n            \n            # Publish results\n            if detections:\n                detection_msg = String()\n                detection_msg.data = json.dumps(detections)\n                self.detection_pub.publish(detection_msg)\n                \n                self.get_logger().info(f"Detected: {detections}")\n            \n        except Exception as e:\n            self.get_logger().error(f"Image processing error: {e}")\n    \n    def detect_objects(self, image, text_queries):\n        """\n        Detect objects in image matching text queries.\n        \n        Args:\n            image: PIL Image\n            text_queries: List of text descriptions (e.g., ["red cup", "blue box"])\n        \n        Returns:\n            List of detections with confidence scores\n        """\n        # Preprocess inputs\n        inputs = self.processor(\n            text=text_queries,\n            images=image,\n            return_tensors="pt",\n            padding=True\n        ).to(self.device)\n        \n        # Run inference\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            logits_per_image = outputs.logits_per_image\n            probs = logits_per_image.softmax(dim=1)\n        \n        # Extract detections above threshold\n        detections = []\n        for i, query in enumerate(text_queries):\n            confidence = probs[0][i].item()\n            if confidence > self.confidence_threshold:\n                detections.append({\n                    "object": query,\n                    "confidence": round(confidence, 3)\n                })\n        \n        return detections\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"testing-the-vision-node",children:"Testing the Vision Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Start camera driver (or simulator)\nros2 run usb_cam usb_cam_node_exe\n\n# Terminal 2: Run vision node\npython vision_node.py\n\n# Terminal 3: Send search query\nros2 topic pub /vision/query std_msgs/String \"data: 'red cup'\" --once\n\n# Terminal 4: Monitor detections\nros2 topic echo /vision/detections\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "object": "red cup",\n  "confidence": 0.873\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"object-detection-for-manipulation",children:"Object Detection for Manipulation"}),"\n",(0,r.jsx)(n.h3,{id:"from-detection-to-3d-localization",children:"From Detection to 3D Localization"}),"\n",(0,r.jsxs)(n.p,{children:["CLIP tells us ",(0,r.jsx)(n.strong,{children:"what"})," is in the image, but not ",(0,r.jsx)(n.strong,{children:"where"})," in 3D space. For manipulation, we need (x, y, z) coordinates."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"CLIP"}),": Identify object in RGB image \u2192 get 2D bounding box"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Depth sensor"}),": Get depth value at object center"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera intrinsics"}),": Unproject 2D pixel + depth \u2192 3D point"]}),"\n"]}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart TD\n    A[RGB Image] --\x3e|CLIP| B[2D Bounding Box]\n    C[Depth Image] --\x3e|Lookup| D[Depth Value]\n    B --\x3e|Center Pixel u,v| E[2D Point]\n    D --\x3e|Depth Z| E\n    F[Camera Intrinsics K] --\x3e|Unproject| G[3D Point x,y,z]\n    E --\x3e|Combine| G\n    \n    style A fill:#E3F2FD\n    style C fill:#E3F2FD\n    style G fill:#E8F5E9"}),"\n",(0,r.jsx)(n.h3,{id:"3d-localization-implementation",children:"3D Localization Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# localization_helper.py\nimport numpy as np\n\nclass Camera3DLocalizer:\n    """Convert 2D image coordinates + depth to 3D world coordinates"""\n    \n    def __init__(self, camera_intrinsics):\n        """\n        Args:\n            camera_intrinsics: Dict with keys \'fx\', \'fy\', \'cx\', \'cy\'\n                fx, fy: Focal lengths in pixels\n                cx, cy: Principal point (image center)\n        """\n        self.fx = camera_intrinsics[\'fx\']\n        self.fy = camera_intrinsics[\'fy\']\n        self.cx = camera_intrinsics[\'cx\']\n        self.cy = camera_intrinsics[\'cy\']\n    \n    def unproject(self, u, v, depth):\n        """\n        Convert 2D pixel (u, v) + depth to 3D point (x, y, z).\n        \n        Args:\n            u: Pixel x-coordinate (column)\n            v: Pixel y-coordinate (row)\n            depth: Depth value in meters\n        \n        Returns:\n            np.array: [x, y, z] in camera frame (meters)\n        """\n        # Pinhole camera model: x = (u - cx) * Z / fx\n        x = (u - self.cx) * depth / self.fx\n        y = (v - self.cy) * depth / self.fy\n        z = depth\n        \n        return np.array([x, y, z])\n    \n    def project(self, point_3d):\n        """\n        Project 3D point to 2D pixel coordinates.\n        \n        Args:\n            point_3d: np.array [x, y, z] in camera frame\n        \n        Returns:\n            (u, v): Pixel coordinates\n        """\n        x, y, z = point_3d\n        u = int(self.fx * x / z + self.cx)\n        v = int(self.fy * y / z + self.cy)\n        return (u, v)\n\n# Example usage\nintrinsics = {\n    \'fx\': 615.0,  # RealSense D435 typical values\n    \'fy\': 615.0,\n    \'cx\': 320.0,\n    \'cy\': 240.0\n}\n\nlocalizer = Camera3DLocalizer(intrinsics)\n\n# Object detected at pixel (350, 200) with depth 0.8m\npixel_u, pixel_v = 350, 200\ndepth_m = 0.8\n\npoint_3d = localizer.unproject(pixel_u, pixel_v, depth_m)\nprint(f"3D position: x={point_3d[0]:.3f}, y={point_3d[1]:.3f}, z={point_3d[2]:.3f}")\n# Output: 3D position: x=0.039, y=-0.052, z=0.800\n'})}),"\n",(0,r.jsx)(n.h3,{id:"enhanced-vision-node-with-3d-localization",children:"Enhanced Vision Node with 3D Localization"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# vision_node_3d.py\nfrom sensor_msgs.msg import Image\nfrom geometry_msgs.msg import Point\nimport numpy as np\n\nclass VisionNode3D(VisionNode):\n    \"\"\"Extended vision node with 3D localization\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        \n        # Subscribe to depth\n        self.depth_sub = self.create_subscription(\n            Image,\n            '/camera/depth/image_raw',\n            self.depth_callback,\n            10\n        )\n        \n        # Publisher for 3D positions\n        self.position_pub = self.create_publisher(Point, '/vision/object_position', 10)\n        \n        # Cache latest depth image\n        self.latest_depth = None\n        \n        # Camera intrinsics (load from calibration file in practice)\n        self.localizer = Camera3DLocalizer({\n            'fx': 615.0, 'fy': 615.0, 'cx': 320.0, 'cy': 240.0\n        })\n    \n    def depth_callback(self, msg):\n        \"\"\"Cache depth image\"\"\"\n        self.latest_depth = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\n    \n    def get_object_3d_position(self, bbox, depth_image):\n        \"\"\"\n        Get 3D position of object from bounding box + depth.\n        \n        Args:\n            bbox: Dict with 'x_min', 'y_min', 'x_max', 'y_max'\n            depth_image: Depth image (numpy array)\n        \n        Returns:\n            np.array: [x, y, z] 3D position\n        \"\"\"\n        # Get bounding box center\n        center_u = int((bbox['x_min'] + bbox['x_max']) / 2)\n        center_v = int((bbox['y_min'] + bbox['y_max']) / 2)\n        \n        # Get median depth in central region (more robust than single pixel)\n        roi_size = 10\n        depth_roi = depth_image[\n            center_v-roi_size:center_v+roi_size,\n            center_u-roi_size:center_u+roi_size\n        ]\n        depth_m = np.median(depth_roi[depth_roi > 0])  # Ignore zero depths\n        \n        # Unproject to 3D\n        return self.localizer.unproject(center_u, center_v, depth_m)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"multimodal-fusion",children:"Multimodal Fusion"}),"\n",(0,r.jsx)(n.h3,{id:"combining-vision--language-features",children:"Combining Vision + Language Features"}),"\n",(0,r.jsx)(n.p,{children:"VLA models fuse visual and language features to enable grounding:"}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart TD\n    A[Image Features 512D] --\x3e|Projection| C[Shared Space 768D]\n    B[Text Features 512D] --\x3e|Projection| C\n    C --\x3e|Cross-Attention| D[Fused Features 768D]\n    D --\x3e|Action Decoder| E[Robot Actions]\n    \n    style A fill:#E3F2FD\n    style B fill:#F3E5F5\n    style D fill:#FFF3E0\n    style E fill:#E8F5E9"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cross-Attention Mechanism"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Query"}),': Language features ("pick up")']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Key/Value"}),": Vision features (object locations)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Grounded representation (language + visual context)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"simple-fusion-implementation",children:"Simple Fusion Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# multimodal_fusion.py\nimport torch\nimport torch.nn as nn\n\nclass SimpleVLFusion(nn.Module):\n    """Fuse vision and language features for VLA"""\n    \n    def __init__(self, vision_dim=512, language_dim=512, hidden_dim=768):\n        super().__init__()\n        \n        # Project to shared dimensionality\n        self.vision_proj = nn.Linear(vision_dim, hidden_dim)\n        self.language_proj = nn.Linear(language_dim, hidden_dim)\n        \n        # Cross-attention layer\n        self.cross_attention = nn.MultiheadAttention(hidden_dim, num_heads=8)\n        \n        # Output projection\n        self.output_proj = nn.Linear(hidden_dim, 256)\n    \n    def forward(self, vision_features, language_features):\n        """\n        Args:\n            vision_features: Tensor [batch, vision_dim]\n            language_features: Tensor [batch, language_dim]\n        \n        Returns:\n            Fused features [batch, 256]\n        """\n        # Project to shared space\n        vision_emb = self.vision_proj(vision_features).unsqueeze(0)  # [1, batch, hidden]\n        language_emb = self.language_proj(language_features).unsqueeze(0)\n        \n        # Cross-attend: language attends to vision\n        fused, _ = self.cross_attention(\n            query=language_emb,\n            key=vision_emb,\n            value=vision_emb\n        )\n        \n        # Project to output\n        output = self.output_proj(fused.squeeze(0))\n        return output\n\n# Example usage\nmodel = SimpleVLFusion()\n\n# Simulate CLIP features\nvision_feat = torch.randn(1, 512)  # From CLIP vision encoder\nlanguage_feat = torch.randn(1, 512)  # From CLIP text encoder\n\nfused = model(vision_feat, language_feat)\nprint(f"Fused features shape: {fused.shape}")  # [1, 256]\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"common-errors",children:"Common Errors"}),"\n",(0,r.jsx)(n.h3,{id:"error-1-no-objects-detected",children:'Error 1: "No objects detected"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"CLIP returns low confidence for all queries"}),"\n",(0,r.jsx)(n.li,{children:"Empty detection results"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Image too dark/bright"}),"\n",(0,r.jsx)(n.li,{children:"Object outside camera field of view"}),"\n",(0,r.jsx)(n.li,{children:'Text query too specific ("crimson cup" vs "red cup")'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Adjust confidence threshold"}),": Lower from 0.7 to 0.3"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Try multiple queries"}),': ["cup", "red object", "container"]']}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Check image preprocessing"}),": Ensure RGB not BGR"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Multi-query fallback\nqueries = ["red cup", "cup", "red object", "mug"]\nfor query in queries:\n    detections = detect_objects(image, [query])\n    if detections:\n        break\n'})}),"\n",(0,r.jsx)(n.h3,{id:"error-2-noisy-depth-measurements",children:'Error 2: "Noisy depth measurements"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"3D positions jump randomly"}),"\n",(0,r.jsx)(n.li,{children:"Depth values at object edges are incorrect"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Depth sensor noise (especially on reflective/transparent objects)"}),"\n",(0,r.jsx)(n.li,{children:"Using single pixel depth (not robust)"}),"\n",(0,r.jsx)(n.li,{children:"Object edges have invalid depth"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Solution: Median filtering in ROI\ndef get_robust_depth(depth_image, center_u, center_v, roi_size=15):\n    """Get robust depth using median filter"""\n    roi = depth_image[\n        center_v-roi_size:center_v+roi_size,\n        center_u-roi_size:center_u+roi_size\n    ]\n    # Filter out zeros and outliers\n    valid_depths = roi[(roi > 0.1) & (roi < 5.0)]\n    return np.median(valid_depths) if len(valid_depths) > 0 else None\n'})}),"\n",(0,r.jsx)(n.h3,{id:"error-3-high-vision-processing-latency",children:'Error 3: "High vision processing latency"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Camera callback queue fills up"}),"\n",(0,r.jsx)(n.li,{children:"Detections lag behind live feed"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Running CLIP on CPU (slow)"}),"\n",(0,r.jsx)(n.li,{children:"Processing every frame (30 FPS = 30 inferences/sec)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Solution: Throttle processing rate\nimport time\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__('vision_node')\n        self.last_process_time = 0\n        self.process_interval = 0.5  # Process every 500ms\n    \n    def image_callback(self, msg):\n        # Throttle to 2 Hz\n        now = time.time()\n        if now - self.last_process_time < self.process_interval:\n            return  # Skip this frame\n        \n        self.last_process_time = now\n        # Process image...\n"})}),"\n",(0,r.jsx)(n.h3,{id:"error-4-camera-calibration-errors",children:'Error 4: "Camera calibration errors"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"3D positions systematically offset"}),"\n",(0,r.jsx)(n.li,{children:"Grasp attempts miss object"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Wrong camera intrinsics (fx, fy, cx, cy)"}),"\n",(0,r.jsx)(n.li,{children:"Camera moved after calibration"}),"\n",(0,r.jsx)(n.li,{children:"Using default values instead of calibrated"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Calibrate camera using ROS 2 camera_calibration\nros2 run camera_calibration cameracalibrator --size 8x6 --square 0.025\n\n# Save intrinsics to YAML, load in code\nimport yaml\nwith open('camera_info.yaml') as f:\n    calib = yaml.safe_load(f)\n    intrinsics = {\n        'fx': calib['camera_matrix']['data'][0],\n        'fy': calib['camera_matrix']['data'][4],\n        'cx': calib['camera_matrix']['data'][2],\n        'cy': calib['camera_matrix']['data'][5]\n    }\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-swap-vision-model-easy-30-minutes",children:"Exercise 1: Swap Vision Model (Easy, 30 minutes)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Replace CLIP with a different vision-language model."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Try PaliGemma instead of CLIP"}),"\n",(0,r.jsx)(n.li,{children:"Compare detection accuracy on test images"}),"\n",(0,r.jsx)(n.li,{children:"Measure inference latency difference"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration\n\nmodel = PaliGemmaForConditionalGeneration.from_pretrained("google/paligemma-3b-pt-224")\nprocessor = PaliGemmaProcessor.from_pretrained("google/paligemma-3b-pt-224")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-color-based-filtering-medium-45-minutes",children:"Exercise 2: Color-based Filtering (Medium, 45 minutes)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),': Enhance CLIP with HSV color filtering for "red cup" vs "blue cup".']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Convert RGB image to HSV color space"}),"\n",(0,r.jsx)(n.li,{children:"Create color masks for red/blue/green"}),"\n",(0,r.jsx)(n.li,{children:"Combine CLIP detection with color verification"}),"\n",(0,r.jsx)(n.li,{children:"Reject detections if color doesn't match"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-kalman-filter-tracking-hard-60-minutes",children:"Exercise 3: Kalman Filter Tracking (Hard, 60 minutes)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Track object 3D positions over time with Kalman filtering."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Implement Kalman filter for 3D position tracking"}),"\n",(0,r.jsx)(n.li,{children:"Predict object position between detections"}),"\n",(0,r.jsx)(n.li,{children:"Handle temporary occlusions"}),"\n",(0,r.jsx)(n.li,{children:"Smooth noisy depth measurements"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"CLIP: Learning Transferable Visual Models"})," (Radford et al., 2021)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2103.00020",children:"https://arxiv.org/abs/2103.00020"})}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"PaliGemma: A versatile vision language model"})," (Google, 2024)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2407.07726",children:"https://arxiv.org/abs/2407.07726"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ROS 2 vision_opencv Package"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ros-perception/vision_opencv",children:"https://github.com/ros-perception/vision_opencv"})}),"\n",(0,r.jsx)(n.li,{children:"cv_bridge for image conversion"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Intel RealSense ROS 2"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/IntelRealSense/realsense-ros",children:"https://github.com/IntelRealSense/realsense-ros"})}),"\n",(0,r.jsx)(n.li,{children:"Depth camera integration"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.p,{children:["Vision complete! In the final sub-chapter, ",(0,r.jsx)(n.a,{href:"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/capstone-project",children:"4.5 Capstone Project: Autonomous Humanoid"}),", you'll integrate voice, planning, and vision into a complete end-to-end VLA system."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 CLIP enables zero-shot object recognition from text"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Depth sensors + camera intrinsics provide 3D localization"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Multimodal fusion grounds language in visual context"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Median filtering makes depth measurements robust"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Throttle vision processing to 2-5 Hz for real-time performance"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);