"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[2771],{8437:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter-4-vla/cognitive-planning","title":"4.3 Cognitive Planning with LLMs","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/03-cognitive-planning.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/cognitive-planning","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/03-cognitive-planning.md","tags":[{"inline":true,"label":"LLM Planning","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/llm-planning"},{"inline":true,"label":"Function Calling","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/function-calling"},{"inline":true,"label":"Pydantic Validation","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/pydantic-validation"},{"inline":true,"label":"Task Decomposition","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/task-decomposition"}],"version":"current","sidebarPosition":3,"frontMatter":{"id":"cognitive-planning","title":"4.3 Cognitive Planning with LLMs","sidebar_label":"Cognitive Planning","sidebar_position":3,"sidebar_custom_props":{"difficulty":"Intermediate","readingTime":"18 minutes","hasQuickStart":true,"prerequisites":["ROS 2 Actions","Python async/await","Basic LLM concepts"]},"tags":["LLM Planning","Function Calling","Pydantic Validation","Task Decomposition"]},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/voice-to-action"},"next":{"title":"Vision Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration"}}');var a=i(4848),s=i(8453);const o={id:"cognitive-planning",title:"4.3 Cognitive Planning with LLMs",sidebar_label:"Cognitive Planning",sidebar_position:3,sidebar_custom_props:{difficulty:"Intermediate",readingTime:"18 minutes",hasQuickStart:!0,prerequisites:["ROS 2 Actions","Python async/await","Basic LLM concepts"]},tags:["LLM Planning","Function Calling","Pydantic Validation","Task Decomposition"]},r="4.3 Cognitive Planning with LLMs",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Quick Start: Pick Up Cup Planning (15 Minutes)",id:"quick-start-pick-up-cup-planning-15-minutes",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Install Dependencies",id:"step-1-install-dependencies",level:3},{value:"Step 2: Define Action Schema",id:"step-2-define-action-schema",level:3},{value:"Step 3: Create LLM Function Calling",id:"step-3-create-llm-function-calling",level:3},{value:"Step 4: Test the Planner",id:"step-4-test-the-planner",level:3},{value:"Step 5: Integrate with ROS 2",id:"step-5-integrate-with-ros-2",level:3},{value:"LLM Planning Fundamentals",id:"llm-planning-fundamentals",level:2},{value:"Why LLMs for Robot Planning?",id:"why-llms-for-robot-planning",level:3},{value:"Limitations and Risks",id:"limitations-and-risks",level:3},{value:"Function Calling Pattern",id:"function-calling-pattern",level:2},{value:"What is Function Calling?",id:"what-is-function-calling",level:3},{value:"Implementation with OpenAI API",id:"implementation-with-openai-api",level:3},{value:"Anthropic Claude Function Calling",id:"anthropic-claude-function-calling",level:3},{value:"Schema Validation with Pydantic",id:"schema-validation-with-pydantic",level:2},{value:"Why Pydantic?",id:"why-pydantic",level:3},{value:"Advanced Pydantic Models",id:"advanced-pydantic-models",level:3},{value:"Custom Validators for Robot Safety",id:"custom-validators-for-robot-safety",level:3},{value:"Feedback Loops for Error Recovery",id:"feedback-loops-for-error-recovery",level:2},{value:"Why Feedback Loops?",id:"why-feedback-loops",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Safety Constraints &amp; Validation",id:"safety-constraints--validation",level:2},{value:"Pre-execution Validation",id:"pre-execution-validation",level:3},{value:"Common Errors",id:"common-errors",level:2},{value:"Error 1: &quot;Invalid JSON from LLM&quot;",id:"error-1-invalid-json-from-llm",level:3},{value:"Error 2: &quot;Impossible actions generated&quot;",id:"error-2-impossible-actions-generated",level:3},{value:"Error 3: &quot;LLM request timeout&quot;",id:"error-3-llm-request-timeout",level:3},{value:"Error 4: &quot;Infinite loop in action sequence&quot;",id:"error-4-infinite-loop-in-action-sequence",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Add New Action Type (Easy, 30 minutes)",id:"exercise-1-add-new-action-type-easy-30-minutes",level:3},{value:"Exercise 2: Multi-step Task Planning (Medium, 45 minutes)",id:"exercise-2-multi-step-task-planning-medium-45-minutes",level:3},{value:"Exercise 3: State Machine Integration (Hard, 60 minutes)",id:"exercise-3-state-machine-integration-hard-60-minutes",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Tutorials",id:"tutorials",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"43-cognitive-planning-with-llms",children:"4.3 Cognitive Planning with LLMs"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By completing this sub-chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Implement"})," LLM-based task planning using function calling patterns"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Validate"})," LLM outputs with Pydantic schemas for type safety and correctness"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Design"})," feedback loops that enable LLMs to recover from execution errors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Apply"})," safety constraints to prevent physically impossible or dangerous actions"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"quick-start-pick-up-cup-planning-15-minutes",children:"Quick Start: Pick Up Cup Planning (15 Minutes)"}),"\n",(0,a.jsx)(e.p,{children:'Transform "Pick up the cup" into executable robot actions using an LLM planner.'}),"\n",(0,a.jsx)(e.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"OpenAI API key (or Anthropic Claude API)"}),"\n",(0,a.jsx)(e.li,{children:"ROS 2 Humble installed"}),"\n",(0,a.jsxs)(e.li,{children:["Python 3.11+ with ",(0,a.jsx)(e.code,{children:"openai"})," and ",(0,a.jsx)(e.code,{children:"pydantic"})," packages"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"step-1-install-dependencies",children:"Step 1: Install Dependencies"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:'pip install openai pydantic\nexport OPENAI_API_KEY="your-api-key-here"  # Or use .env file\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-2-define-action-schema",children:"Step 2: Define Action Schema"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# robot_actions.py\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, List\nfrom enum import Enum\n\nclass ActionType(str, Enum):\n    """Valid robot action types"""\n    NAVIGATE = "navigate"\n    SCAN = "scan"\n    GRASP = "grasp"\n    PLACE = "place"\n    RELEASE = "release"\n\nclass RobotAction(BaseModel):\n    """Single robot action with parameters"""\n    action_type: ActionType\n    target: str = Field(..., description="Target object or location name")\n    position: tuple[float, float, float] | None = Field(None, description="3D coordinates (x, y, z) in meters")\n    confidence: float = Field(default=1.0, ge=0.0, le=1.0)\n    \n    class Config:\n        use_enum_values = True\n\nclass TaskPlan(BaseModel):\n    """Complete task plan as sequence of actions"""\n    task_description: str\n    actions: List[RobotAction] = Field(..., min_items=1, max_items=20)\n    estimated_duration: float = Field(..., description="Estimated duration in seconds", gt=0)\n'})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Expected"}),": Schemas define valid action types and enforce constraints (e.g., max 20 actions)."]}),"\n",(0,a.jsx)(e.h3,{id:"step-3-create-llm-function-calling",children:"Step 3: Create LLM Function Calling"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# llm_planner.py\nfrom openai import OpenAI\nimport json\nfrom robot_actions import TaskPlan, RobotAction, ActionType\n\nclient = OpenAI()  # Reads OPENAI_API_KEY from env\n\n# Define function schema for OpenAI\nFUNCTION_SCHEMA = {\n    "name": "create_robot_plan",\n    "description": "Generate a sequence of robot actions to accomplish a task",\n    "parameters": {\n        "type": "object",\n        "properties": {\n            "task_description": {\n                "type": "string",\n                "description": "High-level task description"\n            },\n            "actions": {\n                "type": "array",\n                "items": {\n                    "type": "object",\n                    "properties": {\n                        "action_type": {\n                            "type": "string",\n                            "enum": ["navigate", "scan", "grasp", "place", "release"]\n                        },\n                        "target": {"type": "string"},\n                        "position": {\n                            "type": "array",\n                            "items": {"type": "number"},\n                            "minItems": 3,\n                            "maxItems": 3\n                        }\n                    },\n                    "required": ["action_type", "target"]\n                }\n            },\n            "estimated_duration": {"type": "number"}\n        },\n        "required": ["task_description", "actions", "estimated_duration"]\n    }\n}\n\ndef plan_task(task_description: str) -> TaskPlan:\n    """Use LLM to generate task plan"""\n    response = client.chat.completions.create(\n        model="gpt-4",\n        messages=[\n            {"role": "system", "content": "You are a robot task planner. Generate executable action sequences."},\n            {"role": "user", "content": f"Plan how to: {task_description}"}\n        ],\n        functions=[FUNCTION_SCHEMA],\n        function_call={"name": "create_robot_plan"}\n    )\n    \n    # Extract function call arguments\n    function_call = response.choices[0].message.function_call\n    plan_data = json.loads(function_call.arguments)\n    \n    # Validate with Pydantic\n    plan = TaskPlan(**plan_data)\n    return plan\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-4-test-the-planner",children:"Step 4: Test the Planner"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# test_planner.py\nfrom llm_planner import plan_task\n\n# Test with simple command\nplan = plan_task("Pick up the cup from the table")\n\nprint(f"Task: {plan.task_description}")\nprint(f"Estimated Duration: {plan.estimated_duration}s")\nprint(f"\\nAction Sequence ({len(plan.actions)} steps):")\n\nfor i, action in enumerate(plan.actions, 1):\n    print(f"{i}. {action.action_type.upper()}: {action.target}")\n    if action.position:\n        print(f"   Position: {action.position}")\n'})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Expected Output"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Task: Pick up the cup from the table\nEstimated Duration: 15.0s\n\nAction Sequence (4 steps):\n1. NAVIGATE: table\n   Position: (1.5, 0.3, 0.0)\n2. SCAN: cup\n3. GRASP: cup\n   Position: (1.5, 0.3, 0.8)\n4. NAVIGATE: home_position\n   Position: (0.0, 0.0, 0.0)\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-5-integrate-with-ros-2",children:"Step 5: Integrate with ROS 2"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# planning_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom llm_planner import plan_task\nimport json\n\nclass CognitivePlanningNode(Node):\n    def __init__(self):\n        super().__init__(\'cognitive_planning_node\')\n        \n        # Subscribe to voice commands\n        self.subscription = self.create_subscription(\n            String,\n            \'/voice/command\',\n            self.command_callback,\n            10\n        )\n        \n        # Publish action plans\n        self.plan_pub = self.create_publisher(String, \'/robot/action_plan\', 10)\n        \n        self.get_logger().info("Cognitive Planning Node ready")\n    \n    def command_callback(self, msg):\n        """Generate plan from voice command"""\n        command = msg.data\n        self.get_logger().info(f"Planning task: {command}")\n        \n        try:\n            # Generate plan using LLM\n            plan = plan_task(command)\n            \n            # Publish as JSON\n            plan_msg = String()\n            plan_msg.data = plan.model_dump_json()\n            self.plan_pub.publish(plan_msg)\n            \n            self.get_logger().info(f"Plan generated: {len(plan.actions)} actions")\n            \n        except Exception as e:\n            self.get_logger().error(f"Planning failed: {e}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CognitivePlanningNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Result"}),": Voice commands now automatically generate validated action plans! \ud83c\udf89"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"llm-planning-fundamentals",children:"LLM Planning Fundamentals"}),"\n",(0,a.jsx)(e.h3,{id:"why-llms-for-robot-planning",children:"Why LLMs for Robot Planning?"}),"\n",(0,a.jsx)(e.p,{children:"Traditional robot planning uses algorithms like A* for path planning or PDDL for task planning. These approaches require:"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Explicit world models"}),": Complete knowledge of environment state"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Hand-coded rules"}),': "If object is graspable, approach from above"']}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain-specific languages"}),": PDDL, STRIPS, or custom DSLs"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Expert tuning"}),": Parameter adjustment for each robot/task"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"LLM-based planning"})," offers a paradigm shift:"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Zero-shot task understanding"}),": No hand-coded rules needed"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Natural language interface"}),': "Clean the room" instead of formal syntax']}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Common-sense reasoning"}),": Knows cups are graspable, tables are surfaces"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Flexible adaptation"}),": Handles novel tasks without retraining"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Example"}),":"]}),"\n",(0,a.jsx)(e.p,{children:"Traditional planner requires:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def plan_pick_up(object_name, location):\n    if not is_graspable(object_name):\n        raise ValueError(f"{object_name} cannot be grasped")\n    if not is_reachable(location):\n        raise ValueError(f"{location} is out of reach")\n    return [Navigate(location), Grasp(object_name)]\n'})}),"\n",(0,a.jsx)(e.p,{children:"LLM planner:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'plan = llm.plan("Pick up the cup")  # Figures out navigate \u2192 grasp automatically\n'})}),"\n",(0,a.jsx)(e.h3,{id:"limitations-and-risks",children:"Limitations and Risks"}),"\n",(0,a.jsxs)(e.p,{children:["\u26a0\ufe0f ",(0,a.jsx)(e.strong,{children:"Critical"}),": LLMs can generate physically impossible or unsafe plans. Always validate outputs."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Common LLM Planning Errors"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Hallucinated objects"}),": Plans actions for objects that don't exist"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Incorrect physics"}),': "Fly to the ceiling" (robot can\'t fly)']}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Missing dependencies"}),": Tries to grasp before navigating to object"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Infinite loops"}),': "Navigate to X, navigate back, navigate to X..."']}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Mitigation"}),": Use Pydantic validation + safety constraints (covered below)."]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"function-calling-pattern",children:"Function Calling Pattern"}),"\n",(0,a.jsx)(e.h3,{id:"what-is-function-calling",children:"What is Function Calling?"}),"\n",(0,a.jsx)(e.p,{children:'Function calling (also called "tool use") allows LLMs to generate structured outputs that match predefined schemas. Instead of free-form text, the LLM produces JSON that conforms to a function signature.'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Without Function Calling"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'response = llm.complete("Plan: Pick up cup")\n# Returns: "First, navigate to the table. Then scan for the cup..."\n# Problem: Unstructured text, hard to parse\n'})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"With Function Calling"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'response = llm.call_function("create_robot_plan", prompt)\n# Returns: {"actions": [{"action_type": "navigate", "target": "table"}, ...]}\n# Benefit: Structured JSON, type-safe\n'})}),"\n",(0,a.jsx)(e.h3,{id:"implementation-with-openai-api",children:"Implementation with OpenAI API"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# complete_llm_planner.py\nfrom openai import OpenAI\nfrom typing import List\nimport json\n\nclient = OpenAI()\n\n# Define multiple functions for different robot capabilities\nFUNCTIONS = [\n    {\n        "name": "create_navigation_plan",\n        "description": "Plan a navigation path to a target location",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "target_location": {"type": "string"},\n                "avoid_obstacles": {"type": "boolean"},\n                "max_speed": {"type": "number", "minimum": 0.1, "maximum": 2.0}\n            },\n            "required": ["target_location"]\n        }\n    },\n    {\n        "name": "create_manipulation_plan",\n        "description": "Plan object manipulation (grasp, place, release)",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "object_name": {"type": "string"},\n                "action": {"type": "string", "enum": ["grasp", "place", "release"]},\n                "target_location": {"type": "string"}\n            },\n            "required": ["object_name", "action"]\n        }\n    },\n    {\n        "name": "create_task_plan",\n        "description": "Plan a complete multi-step task",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "task_description": {"type": "string"},\n                "actions": {\n                    "type": "array",\n                    "items": {\n                        "type": "object",\n                        "properties": {\n                            "action_type": {"type": "string"},\n                            "parameters": {"type": "object"}\n                        }\n                    }\n                }\n            },\n            "required": ["task_description", "actions"]\n        }\n    }\n]\n\ndef plan_with_multi_functions(user_request: str):\n    """Let LLM choose which function to call based on request"""\n    response = client.chat.completions.create(\n        model="gpt-4",\n        messages=[\n            {"role": "system", "content": "You are a helpful robot assistant. Choose the appropriate planning function based on the user\'s request."},\n            {"role": "user", "content": user_request}\n        ],\n        functions=FUNCTIONS,\n        function_call="auto"  # Let LLM choose function\n    )\n    \n    message = response.choices[0].message\n    \n    if message.function_call:\n        function_name = message.function_call.name\n        arguments = json.loads(message.function_call.arguments)\n        return {\n            "function": function_name,\n            "arguments": arguments\n        }\n    else:\n        return {"error": "No function called", "content": message.content}\n\n# Test\nresult = plan_with_multi_functions("Navigate to the kitchen")\nprint(f"LLM chose function: {result[\'function\']}")\nprint(f"Arguments: {result[\'arguments\']}")\n'})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Expected"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"LLM chose function: create_navigation_plan\nArguments: {'target_location': 'kitchen', 'avoid_obstacles': True, 'max_speed': 1.0}\n"})}),"\n",(0,a.jsx)(e.h3,{id:"anthropic-claude-function-calling",children:"Anthropic Claude Function Calling"}),"\n",(0,a.jsx)(e.p,{children:"OpenAI and Anthropic use slightly different APIs. Here's the Claude equivalent:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# claude_planner.py\nfrom anthropic import Anthropic\n\nclient = Anthropic()  # Reads ANTHROPIC_API_KEY from env\n\nCLAUDE_TOOL = {\n    "name": "create_robot_plan",\n    "description": "Generate a sequence of robot actions",\n    "input_schema": {\n        "type": "object",\n        "properties": {\n            "actions": {\n                "type": "array",\n                "items": {\n                    "type": "object",\n                    "properties": {\n                        "action_type": {"type": "string"},\n                        "target": {"type": "string"}\n                    }\n                }\n            }\n        },\n        "required": ["actions"]\n    }\n}\n\ndef plan_with_claude(task: str):\n    response = client.messages.create(\n        model="claude-3-5-sonnet-20241022",\n        max_tokens=1024,\n        tools=[CLAUDE_TOOL],\n        messages=[{"role": "user", "content": f"Plan: {task}"}]\n    )\n    \n    # Extract tool use from response\n    for block in response.content:\n        if block.type == "tool_use":\n            return block.input\n    \n    return None\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"schema-validation-with-pydantic",children:"Schema Validation with Pydantic"}),"\n",(0,a.jsx)(e.h3,{id:"why-pydantic",children:"Why Pydantic?"}),"\n",(0,a.jsx)(e.p,{children:"LLMs sometimes generate invalid JSON:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Missing required fields"}),"\n",(0,a.jsxs)(e.li,{children:["Wrong data types (",(0,a.jsx)(e.code,{children:'"5.2"'})," instead of ",(0,a.jsx)(e.code,{children:"5.2"}),")"]}),"\n",(0,a.jsx)(e.li,{children:"Out-of-range values (negative durations, coordinates outside workspace)"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Pydantic"})," provides:"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Runtime type checking"}),": Ensures ",(0,a.jsx)(e.code,{children:"position"})," is ",(0,a.jsx)(e.code,{children:"tuple[float, float, float]"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Field validation"}),": Enforces ",(0,a.jsx)(e.code,{children:"confidence >= 0.0 and <= 1.0"})]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Automatic coercion"}),": Converts ",(0,a.jsx)(e.code,{children:'"5.2"'})," to ",(0,a.jsx)(e.code,{children:"5.2"})," when possible"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Clear error messages"}),': "estimated_duration must be greater than 0"']}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"advanced-pydantic-models",children:"Advanced Pydantic Models"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# advanced_schemas.py\nfrom pydantic import BaseModel, Field, field_validator, model_validator\nfrom typing import List, Literal\nimport numpy as np\n\nclass Position3D(BaseModel):\n    """3D position with workspace constraints"""\n    x: float = Field(..., ge=-5.0, le=5.0, description="X coordinate in meters")\n    y: float = Field(..., ge=-5.0, le=5.0, description="Y coordinate in meters")\n    z: float = Field(..., ge=0.0, le=2.5, description="Z coordinate in meters (height)")\n    \n    @field_validator(\'x\', \'y\', \'z\')\n    @classmethod\n    def round_precision(cls, v):\n        """Round to 3 decimal places"""\n        return round(v, 3)\n\nclass GraspAction(BaseModel):\n    """Grasp action with physics constraints"""\n    action_type: Literal["grasp"] = "grasp"\n    object_name: str = Field(..., min_length=1, max_length=50)\n    approach_vector: tuple[float, float, float] = Field(default=(0, 0, -1), description="Grasp approach direction")\n    gripper_width: float = Field(default=0.08, ge=0.0, le=0.15, description="Gripper opening in meters")\n    force: float = Field(default=10.0, ge=1.0, le=50.0, description="Grasp force in Newtons")\n    \n    @field_validator(\'approach_vector\')\n    @classmethod\n    def normalize_vector(cls, v):\n        """Ensure approach vector is normalized"""\n        norm = np.linalg.norm(v)\n        if norm == 0:\n            raise ValueError("Approach vector cannot be zero")\n        return tuple(np.array(v) / norm)\n\nclass NavigateAction(BaseModel):\n    """Navigation action with path constraints"""\n    action_type: Literal["navigate"] = "navigate"\n    target: Position3D\n    max_velocity: float = Field(default=1.0, ge=0.1, le=2.0, description="Max speed in m/s")\n    obstacle_clearance: float = Field(default=0.3, ge=0.1, le=1.0, description="Minimum clearance in meters")\n\nclass EnhancedTaskPlan(BaseModel):\n    """Task plan with cross-action validation"""\n    task_description: str\n    actions: List[GraspAction | NavigateAction]\n    total_distance: float | None = None\n    \n    @model_validator(mode=\'after\')\n    def validate_action_sequence(self):\n        """Ensure action sequence is physically valid"""\n        # Check: Can\'t grasp before navigating to object\n        for i, action in enumerate(self.actions):\n            if isinstance(action, GraspAction):\n                if i == 0:\n                    raise ValueError("Cannot grasp as first action (must navigate first)")\n                prev_action = self.actions[i-1]\n                if not isinstance(prev_action, NavigateAction):\n                    raise ValueError(f"Grasp must be preceded by navigation, got {type(prev_action).__name__}")\n        \n        return self\n\n# Test validation\ntry:\n    plan = EnhancedTaskPlan(\n        task_description="Pick up cup",\n        actions=[\n            GraspAction(object_name="cup")  # Invalid: grasp without navigation\n        ]\n    )\nexcept ValueError as e:\n    print(f"Validation error: {e}")\n    # Output: "Cannot grasp as first action (must navigate first)"\n'})}),"\n",(0,a.jsx)(e.h3,{id:"custom-validators-for-robot-safety",children:"Custom Validators for Robot Safety"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# safety_validators.py\nfrom pydantic import BaseModel, field_validator\n\n# Define safe workspace bounds (example for tabletop robot)\nSAFE_WORKSPACE = {\n    "x_min": -0.5, "x_max": 0.5,\n    "y_min": -0.3, "y_max": 0.3,\n    "z_min": 0.0, "z_max": 0.4\n}\n\nclass SafePosition(BaseModel):\n    x: float\n    y: float\n    z: float\n    \n    @field_validator(\'x\')\n    @classmethod\n    def validate_x(cls, v):\n        if not (SAFE_WORKSPACE["x_min"] <= v <= SAFE_WORKSPACE["x_max"]):\n            raise ValueError(f"X position {v} outside safe workspace")\n        return v\n    \n    @field_validator(\'y\')\n    @classmethod\n    def validate_y(cls, v):\n        if not (SAFE_WORKSPACE["y_min"] <= v <= SAFE_WORKSPACE["y_max"]):\n            raise ValueError(f"Y position {v} outside safe workspace")\n        return v\n    \n    @field_validator(\'z\')\n    @classmethod\n    def validate_z(cls, v):\n        if not (SAFE_WORKSPACE["z_min"] <= v <= SAFE_WORKSPACE["z_max"]):\n            raise ValueError(f"Z position {v} outside safe workspace")\n        return v\n\n# LLM might generate position outside workspace\ntry:\n    dangerous_pos = SafePosition(x=10.0, y=0.0, z=0.0)  # Far outside bounds\nexcept ValueError as e:\n    print(f"Safety violation prevented: {e}")\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"feedback-loops-for-error-recovery",children:"Feedback Loops for Error Recovery"}),"\n",(0,a.jsx)(e.h3,{id:"why-feedback-loops",children:"Why Feedback Loops?"}),"\n",(0,a.jsx)(e.p,{children:"Real-world robot execution fails frequently:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Object moved since planning"}),"\n",(0,a.jsx)(e.li,{children:"Grasp failed (object slipped)"}),"\n",(0,a.jsx)(e.li,{children:"Navigation blocked by unexpected obstacle"}),"\n",(0,a.jsx)(e.li,{children:"Sensor noise caused mis-detection"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Feedback loops"})," allow LLMs to adapt plans based on execution results."]}),"\n",(0,a.jsx)(e.h3,{id:"architecture",children:"Architecture"}),"\n",(0,a.jsx)(e.mermaid,{value:'sequenceDiagram\n    participant User\n    participant Planner as LLM Planner\n    participant Executor as Action Executor\n    participant Robot\n    \n    User->>Planner: "Pick up cup"\n    Planner->>Executor: Plan: [navigate, grasp, place]\n    Executor->>Robot: Execute action 1: navigate\n    Robot->>Executor: \u2713 Success\n    Executor->>Robot: Execute action 2: grasp\n    Robot->>Executor: \u2717 Failure (object slipped)\n    Executor->>Planner: Error: "Grasp failed, cup slipped"\n    Planner->>Planner: Replan considering error\n    Planner->>Executor: Updated plan: [approach_differently, reattempt_grasp]\n    Executor->>Robot: Execute revised plan\n    Robot->>Executor: \u2713 Success\n    Executor->>User: Task completed'}),"\n",(0,a.jsx)(e.h3,{id:"implementation",children:"Implementation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# feedback_loop.py\nfrom openai import OpenAI\nfrom robot_actions import TaskPlan, RobotAction\nimport json\n\nclient = OpenAI()\n\nclass FeedbackPlanner:\n    def __init__(self):\n        self.conversation_history = []\n        self.execution_log = []\n    \n    def plan(self, task: str) -> TaskPlan:\n        """Generate initial plan"""\n        self.conversation_history = [\n            {"role": "system", "content": "You are a robot task planner. Generate executable plans and adapt to feedback."},\n            {"role": "user", "content": f"Plan: {task}"}\n        ]\n        \n        response = client.chat.completions.create(\n            model="gpt-4",\n            messages=self.conversation_history,\n            functions=[FUNCTION_SCHEMA],\n            function_call={"name": "create_robot_plan"}\n        )\n        \n        # Parse plan\n        function_call = response.choices[0].message.function_call\n        plan = TaskPlan(**json.loads(function_call.arguments))\n        \n        # Save to history\n        self.conversation_history.append(response.choices[0].message)\n        \n        return plan\n    \n    def replan_on_error(self, failed_action: RobotAction, error_message: str) -> TaskPlan:\n        """Generate new plan based on execution error"""\n        # Add error feedback to conversation\n        feedback = f"Action \'{failed_action.action_type}\' on \'{failed_action.target}\' failed: {error_message}. Generate a recovery plan."\n        \n        self.conversation_history.append({"role": "user", "content": feedback})\n        \n        # Request new plan\n        response = client.chat.completions.create(\n            model="gpt-4",\n            messages=self.conversation_history,\n            functions=[FUNCTION_SCHEMA],\n            function_call={"name": "create_robot_plan"}\n        )\n        \n        # Parse recovery plan\n        function_call = response.choices[0].message.function_call\n        recovery_plan = TaskPlan(**json.loads(function_call.arguments))\n        \n        # Log recovery attempt\n        self.execution_log.append({\n            "failed_action": failed_action.model_dump(),\n            "error": error_message,\n            "recovery_plan": recovery_plan.model_dump()\n        })\n        \n        return recovery_plan\n\n# Usage example\nplanner = FeedbackPlanner()\n\n# Initial plan\nplan = planner.plan("Pick up the cup from the table")\nprint(f"Initial plan: {len(plan.actions)} actions")\n\n# Simulate grasp failure\nfailed_action = plan.actions[2]  # Assume action 2 is grasp\nrecovery = planner.replan_on_error(\n    failed_action,\n    "Grasp failed: cup slipped from gripper. Object appears wet."\n)\n\nprint(f"\\nRecovery plan: {len(recovery.actions)} actions")\nfor action in recovery.actions:\n    print(f"  - {action.action_type}: {action.target}")\n'})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Expected Recovery Plan"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Initial plan: 4 actions\n  - navigate: table\n  - scan: cup\n  - grasp: cup\n  - navigate: home\n\nRecovery plan: 3 actions\n  - adjust_gripper: increase_force\n  - dry_object: cup (using cloth)\n  - reattempt_grasp: cup\n"})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"safety-constraints--validation",children:"Safety Constraints & Validation"}),"\n",(0,a.jsx)(e.h3,{id:"pre-execution-validation",children:"Pre-execution Validation"}),"\n",(0,a.jsx)(e.p,{children:"Always validate LLM plans before execution:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# safety_validator.py\nfrom robot_actions import TaskPlan, RobotAction, ActionType\nfrom typing import List\n\nclass SafetyValidator:\n    """Validate task plans for safety and feasibility"""\n    \n    def __init__(self, workspace_bounds, max_actions=20, max_duration=300):\n        self.workspace_bounds = workspace_bounds\n        self.max_actions = max_actions\n        self.max_duration = max_duration\n    \n    def validate_plan(self, plan: TaskPlan) -> tuple[bool, List[str]]:\n        """\n        Validate plan for safety violations.\n        \n        Returns:\n            (is_valid, error_messages)\n        """\n        errors = []\n        \n        # Check 1: Plan length\n        if len(plan.actions) > self.max_actions:\n            errors.append(f"Plan too long: {len(plan.actions)} actions (max {self.max_actions})")\n        \n        # Check 2: Duration\n        if plan.estimated_duration > self.max_duration:\n            errors.append(f"Plan too slow: {plan.estimated_duration}s (max {self.max_duration}s)")\n        \n        # Check 3: Workspace bounds\n        for i, action in enumerate(plan.actions):\n            if action.position:\n                x, y, z = action.position\n                if not self._in_workspace(x, y, z):\n                    errors.append(f"Action {i}: Position {action.position} outside safe workspace")\n        \n        # Check 4: Action sequence validity\n        sequence_errors = self._validate_sequence(plan.actions)\n        errors.extend(sequence_errors)\n        \n        return (len(errors) == 0, errors)\n    \n    def _in_workspace(self, x, y, z):\n        """Check if position is within safe workspace"""\n        return (\n            self.workspace_bounds["x_min"] <= x <= self.workspace_bounds["x_max"] and\n            self.workspace_bounds["y_min"] <= y <= self.workspace_bounds["y_max"] and\n            self.workspace_bounds["z_min"] <= z <= self.workspace_bounds["z_max"]\n        )\n    \n    def _validate_sequence(self, actions: List[RobotAction]) -> List[str]:\n        """Validate action sequence logic"""\n        errors = []\n        \n        # Rule: Can\'t grasp without first navigating to object\n        for i, action in enumerate(actions):\n            if action.action_type == ActionType.GRASP:\n                if i == 0:\n                    errors.append("Cannot grasp as first action")\n                else:\n                    prev_action = actions[i-1]\n                    if prev_action.action_type not in [ActionType.NAVIGATE, ActionType.SCAN]:\n                        errors.append(f"Grasp must follow navigate/scan, got {prev_action.action_type}")\n        \n        # Rule: Can\'t place without first grasping\n        has_grasped = False\n        for i, action in enumerate(actions):\n            if action.action_type == ActionType.GRASP:\n                has_grasped = True\n            if action.action_type == ActionType.PLACE and not has_grasped:\n                errors.append(f"Action {i}: Cannot place without grasping first")\n        \n        # Rule: Detect infinite loops (same action repeated >3 times)\n        action_counts = {}\n        for action in actions:\n            key = (action.action_type, action.target)\n            action_counts[key] = action_counts.get(key, 0) + 1\n            if action_counts[key] > 3:\n                errors.append(f"Potential infinite loop: {action.action_type} on {action.target} repeated {action_counts[key]} times")\n        \n        return errors\n\n# Usage\nvalidator = SafetyValidator(\n    workspace_bounds={"x_min": -1.0, "x_max": 1.0, "y_min": -1.0, "y_max": 1.0, "z_min": 0.0, "z_max": 2.0}\n)\n\n# Validate a plan\nis_valid, errors = validator.validate_plan(plan)\nif not is_valid:\n    print("\u26a0\ufe0f  Plan validation failed:")\n    for error in errors:\n        print(f"  - {error}")\nelse:\n    print("\u2713 Plan is safe to execute")\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"common-errors",children:"Common Errors"}),"\n",(0,a.jsx)(e.h3,{id:"error-1-invalid-json-from-llm",children:'Error 1: "Invalid JSON from LLM"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"json.JSONDecodeError: Expecting ',' delimiter: line 5 column 3\n"})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"LLM generated malformed JSON (missing comma, trailing comma, unquoted keys)"}),"\n",(0,a.jsx)(e.li,{children:"Function calling not enforced (model returned text instead of function call)"}),"\n",(0,a.jsx)(e.li,{children:"Model doesn't support function calling (older GPT-3.5 models)"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Solution 1: Retry with explicit format instruction\ndef plan_with_retry(task, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            response = client.chat.completions.create(\n                model="gpt-4",\n                messages=[\n                    {"role": "system", "content": "Output ONLY valid JSON. No explanation."},\n                    {"role": "user", "content": task}\n                ],\n                functions=[FUNCTION_SCHEMA],\n                function_call={"name": "create_robot_plan"}\n            )\n            return json.loads(response.choices[0].message.function_call.arguments)\n        except json.JSONDecodeError as e:\n            print(f"Attempt {attempt+1} failed: {e}")\n            if attempt == max_retries - 1:\n                raise\n    return None\n\n# Solution 2: Use Pydantic\'s model_validate_json (more forgiving)\nfrom pydantic import ValidationError\n\ntry:\n    plan = TaskPlan.model_validate_json(llm_response)\nexcept ValidationError as e:\n    print(f"Validation error: {e}")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"error-2-impossible-actions-generated",children:'Error 2: "Impossible actions generated"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:'LLM plans "Fly to ceiling" (robot can\'t fly)'}),"\n",(0,a.jsx)(e.li,{children:"Plans grasp on non-existent object"}),"\n",(0,a.jsx)(e.li,{children:"Navigates outside workspace bounds"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"LLM lacks robot capability knowledge"}),"\n",(0,a.jsx)(e.li,{children:"No safety constraints in prompt"}),"\n",(0,a.jsx)(e.li,{children:"No validation layer"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Solution: Add capability constraints to system prompt\nSYSTEM_PROMPT = """You are a robot task planner with these constraints:\n\nCAPABILITIES:\n- Can navigate on ground (x, y), cannot fly (z must be 0 during navigation)\n- Can grasp objects weighing <2kg with gripper\n- Workspace bounds: x\u2208[-1,1]m, y\u2208[-1,1]m, z\u2208[0,2]m\n- Max reach: 0.8m from base\n\nKNOWN OBJECTS:\n- cup (0.5kg, graspable)\n- table (surface, not graspable)\n- box (1.2kg, graspable)\n\nGenerate plans that respect these constraints. If a task is impossible, return an error action."""\n\n# Then use SafetyValidator to catch any violations\n'})}),"\n",(0,a.jsx)(e.h3,{id:"error-3-llm-request-timeout",children:'Error 3: "LLM request timeout"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"openai.error.Timeout: Request timed out after 60 seconds\n"})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Complex task requires long thinking time"}),"\n",(0,a.jsx)(e.li,{children:"API service overloaded"}),"\n",(0,a.jsx)(e.li,{children:"Network latency"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Solution 1: Increase timeout\nclient = OpenAI(timeout=120.0)  # 2 minutes\n\n# Solution 2: Use streaming for progress updates\ndef plan_with_streaming(task):\n    stream = client.chat.completions.create(\n        model="gpt-4",\n        messages=[{"role": "user", "content": task}],\n        functions=[FUNCTION_SCHEMA],\n        function_call={"name": "create_robot_plan"},\n        stream=True\n    )\n    \n    function_args = ""\n    for chunk in stream:\n        if chunk.choices[0].delta.function_call:\n            function_args += chunk.choices[0].delta.function_call.arguments or ""\n            print(".", end="", flush=True)  # Progress indicator\n    \n    return json.loads(function_args)\n\n# Solution 3: Fallback to simpler model\ndef plan_with_fallback(task):\n    try:\n        return plan_with_model(task, model="gpt-4", timeout=30)\n    except Timeout:\n        print("GPT-4 timed out, falling back to GPT-3.5-turbo")\n        return plan_with_model(task, model="gpt-3.5-turbo", timeout=60)\n'})}),"\n",(0,a.jsx)(e.h3,{id:"error-4-infinite-loop-in-action-sequence",children:'Error 4: "Infinite loop in action sequence"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["Plan contains repeated actions: ",(0,a.jsx)(e.code,{children:"[navigate(A), navigate(B), navigate(A), navigate(B), ...]"})]}),"\n",(0,a.jsx)(e.li,{children:"Robot oscillates between positions"}),"\n",(0,a.jsx)(e.li,{children:"Plan never completes"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"LLM misunderstands task completion criteria"}),"\n",(0,a.jsx)(e.li,{children:"Feedback loop amplifies errors"}),"\n",(0,a.jsx)(e.li,{children:"No loop detection"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Solution: Detect and break loops in validator\ndef detect_loops(actions: List[RobotAction], max_repeats=2):\n    """Detect repeated action patterns"""\n    for i in range(len(actions) - max_repeats):\n        window = actions[i:i+max_repeats+1]\n        # Check if same action repeated\n        if all(a.action_type == window[0].action_type and a.target == window[0].target for a in window):\n            return True, f"Loop detected: {window[0].action_type} on {window[0].target} repeated {max_repeats+1} times"\n    return False, ""\n\n# Add to SafetyValidator\nhas_loop, loop_msg = detect_loops(plan.actions)\nif has_loop:\n    raise ValueError(f"Plan contains infinite loop: {loop_msg}")\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(e.h3,{id:"exercise-1-add-new-action-type-easy-30-minutes",children:"Exercise 1: Add New Action Type (Easy, 30 minutes)"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Goal"}),': Extend the planner to support a new "scan" action that inspects objects before grasping.']}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["Add ",(0,a.jsx)(e.code,{children:"SCAN"})," to ",(0,a.jsx)(e.code,{children:"ActionType"})," enum"]}),"\n",(0,a.jsxs)(e.li,{children:["Create ",(0,a.jsx)(e.code,{children:"ScanAction"})," Pydantic model with field ",(0,a.jsx)(e.code,{children:"scan_duration: float"})]}),"\n",(0,a.jsx)(e.li,{children:"Update function schema to include scan action"}),"\n",(0,a.jsx)(e.li,{children:"Test LLM generates scan before grasp"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Hints"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class ScanAction(BaseModel):\n    action_type: Literal["scan"] = "scan"\n    target: str\n    scan_duration: float = Field(default=2.0, ge=0.5, le=10.0)\n    scan_mode: Literal["visual", "depth", "both"] = "both"\n'})}),"\n",(0,a.jsx)(e.h3,{id:"exercise-2-multi-step-task-planning-medium-45-minutes",children:"Exercise 2: Multi-step Task Planning (Medium, 45 minutes)"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Goal"}),': Plan a complex task like "Clean the table" that requires multiple object manipulations.']}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Define task: Pick up all objects from table, move to storage bin"}),"\n",(0,a.jsx)(e.li,{children:"Generate plan with LLM (should include multiple grasp-place cycles)"}),"\n",(0,a.jsx)(e.li,{children:"Validate plan ensures objects are placed, not just grasped"}),"\n",(0,a.jsx)(e.li,{children:"Add constraint: Max 3 objects can be carried simultaneously"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Hints"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["Use ",(0,a.jsx)(e.code,{children:"model_validator"})," to track carried objects count"]}),"\n",(0,a.jsxs)(e.li,{children:["Require ",(0,a.jsx)(e.code,{children:"RELEASE"})," action before grasping 4th object"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-3-state-machine-integration-hard-60-minutes",children:"Exercise 3: State Machine Integration (Hard, 60 minutes)"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Goal"}),": Integrate LLM planner with a finite state machine for robust execution."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Define states: IDLE, PLANNING, EXECUTING, ERROR, REPLANNING"}),"\n",(0,a.jsxs)(e.li,{children:["Implement state machine that:","\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Transitions IDLE \u2192 PLANNING on voice command"}),"\n",(0,a.jsx)(e.li,{children:"Transitions PLANNING \u2192 EXECUTING when plan validated"}),"\n",(0,a.jsx)(e.li,{children:"Transitions EXECUTING \u2192 ERROR on action failure"}),"\n",(0,a.jsx)(e.li,{children:"Transitions ERROR \u2192 REPLANNING, then back to EXECUTING"}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.li,{children:"Add timeout: If stuck in REPLANNING >3 attempts, abort task"}),"\n",(0,a.jsx)(e.li,{children:"Log all state transitions"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Hints"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from enum import Enum\n\nclass State(Enum):\n    IDLE = 1\n    PLANNING = 2\n    EXECUTING = 3\n    ERROR = 4\n    REPLANNING = 5\n\nclass StateMachine:\n    def __init__(self):\n        self.state = State.IDLE\n        self.replan_attempts = 0\n    \n    def transition(self, new_state: State, reason: str):\n        print(f"State: {self.state.name} \u2192 {new_state.name} ({reason})")\n        self.state = new_state\n'})}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,a.jsx)(e.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Large Language Models as Zero-Shot Planners"})," (Huang et al., 2022)"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/2201.07207",children:"https://arxiv.org/abs/2201.07207"})}),"\n",(0,a.jsx)(e.li,{children:"Shows LLMs can plan complex tasks without fine-tuning"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"ProgPrompt: Program-Guided Prompt Engineering for Robot Planning"})," (Singh et al., 2023)"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Combines LLMs with structured programming for safer plans"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"documentation",children:"Documentation"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"OpenAI Function Calling Guide"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"https://platform.openai.com/docs/guides/function-calling"})}),"\n",(0,a.jsx)(e.li,{children:"Official documentation for function calling API"}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(e.li,{children:["\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Pydantic Documentation"})}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://docs.pydantic.dev/",children:"https://docs.pydantic.dev/"})}),"\n",(0,a.jsx)(e.li,{children:"Complete guide to data validation with Pydantic"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"tutorials",children:"Tutorials"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"LLM Planning Survey"})," (Valmeekam et al., 2023)","\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/2305.15771",children:"https://arxiv.org/abs/2305.15771"})}),"\n",(0,a.jsx)(e.li,{children:"Comprehensive analysis of LLM planning capabilities and limitations"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,a.jsxs)(e.p,{children:["You've mastered cognitive planning! In the next sub-chapter, ",(0,a.jsx)(e.a,{href:"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration",children:"4.4 Vision Integration & Object Detection"}),", you'll learn how to integrate computer vision so your robot can see the cup before trying to pick it up."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"\u2705 Function calling enables structured LLM outputs (type-safe JSON)"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Pydantic validation catches invalid plans before execution"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Feedback loops allow LLMs to recover from errors dynamically"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Safety validators prevent dangerous or impossible actions"}),"\n",(0,a.jsx)(e.li,{children:"\u2705 Always validate LLM outputs\u2014they can hallucinate impossible plans"}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>r});var t=i(6540);const a={},s=t.createContext(a);function o(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);