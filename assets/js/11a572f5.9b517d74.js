"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[2771],{8437:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-4-vla/cognitive-planning","title":"4.3 Cognitive Planning with LLMs","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/03-cognitive-planning.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/cognitive-planning","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/03-cognitive-planning.md","tags":[{"inline":true,"label":"LLM Planning","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/llm-planning"},{"inline":true,"label":"Function Calling","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/function-calling"},{"inline":true,"label":"Pydantic Validation","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/pydantic-validation"},{"inline":true,"label":"Task Decomposition","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/task-decomposition"}],"version":"current","sidebarPosition":3,"frontMatter":{"id":"cognitive-planning","title":"4.3 Cognitive Planning with LLMs","sidebar_label":"Cognitive Planning","sidebar_position":3,"sidebar_custom_props":{"difficulty":"Intermediate","readingTime":"18 minutes","hasQuickStart":true,"prerequisites":["ROS 2 Actions","Python async/await","Basic LLM concepts"]},"tags":["LLM Planning","Function Calling","Pydantic Validation","Task Decomposition"]},"sidebar":"tutorialSidebar","previous":{"title":"Voice-to-Action","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/voice-to-action"},"next":{"title":"Vision Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration"}}');var t=r(4848),a=r(8453);const s={id:"cognitive-planning",title:"4.3 Cognitive Planning with LLMs",sidebar_label:"Cognitive Planning",sidebar_position:3,sidebar_custom_props:{difficulty:"Intermediate",readingTime:"18 minutes",hasQuickStart:!0,prerequisites:["ROS 2 Actions","Python async/await","Basic LLM concepts"]},tags:["LLM Planning","Function Calling","Pydantic Validation","Task Decomposition"]},o="4.3 Cognitive Planning with LLMs",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Quick Start: Pick Up Cup Planning (15 Minutes)",id:"quick-start-pick-up-cup-planning-15-minutes",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Install Dependencies",id:"step-1-install-dependencies",level:3},{value:"Step 2: Define Action Schema",id:"step-2-define-action-schema",level:3},{value:"Step 3: Create LLM Function Calling",id:"step-3-create-llm-function-calling",level:3},{value:"Step 4: Test the Planner",id:"step-4-test-the-planner",level:3},{value:"Step 5: Integrate with ROS 2",id:"step-5-integrate-with-ros-2",level:3},{value:"LLM Planning Fundamentals",id:"llm-planning-fundamentals",level:2},{value:"Why LLMs for Robot Planning?",id:"why-llms-for-robot-planning",level:3},{value:"Limitations and Risks",id:"limitations-and-risks",level:3},{value:"Function Calling Pattern",id:"function-calling-pattern",level:2},{value:"What is Function Calling?",id:"what-is-function-calling",level:3},{value:"Implementation with OpenAI API",id:"implementation-with-openai-api",level:3},{value:"Anthropic Claude Function Calling",id:"anthropic-claude-function-calling",level:3},{value:"Schema Validation with Pydantic",id:"schema-validation-with-pydantic",level:2},{value:"Why Pydantic?",id:"why-pydantic",level:3},{value:"Advanced Pydantic Models",id:"advanced-pydantic-models",level:3},{value:"Custom Validators for Robot Safety",id:"custom-validators-for-robot-safety",level:3},{value:"Feedback Loops for Error Recovery",id:"feedback-loops-for-error-recovery",level:2},{value:"Why Feedback Loops?",id:"why-feedback-loops",level:3},{value:"Architecture",id:"architecture",level:3},{value:"Implementation",id:"implementation",level:3},{value:"Safety Constraints &amp; Validation",id:"safety-constraints--validation",level:2},{value:"Pre-execution Validation",id:"pre-execution-validation",level:3},{value:"Common Errors",id:"common-errors",level:2},{value:"Error 1: &quot;Invalid JSON from LLM&quot;",id:"error-1-invalid-json-from-llm",level:3},{value:"Error 2: &quot;Impossible actions generated&quot;",id:"error-2-impossible-actions-generated",level:3},{value:"Error 3: &quot;LLM request timeout&quot;",id:"error-3-llm-request-timeout",level:3},{value:"Error 4: &quot;Infinite loop in action sequence&quot;",id:"error-4-infinite-loop-in-action-sequence",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Add New Action Type (Easy, 30 minutes)",id:"exercise-1-add-new-action-type-easy-30-minutes",level:3},{value:"Exercise 2: Multi-step Task Planning (Medium, 45 minutes)",id:"exercise-2-multi-step-task-planning-medium-45-minutes",level:3},{value:"Exercise 3: State Machine Integration (Hard, 60 minutes)",id:"exercise-3-state-machine-integration-hard-60-minutes",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Tutorials",id:"tutorials",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"43-cognitive-planning-with-llms",children:"4.3 Cognitive Planning with LLMs"})}),"\n",(0,t.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(e.p,{children:"By completing this sub-chapter, you will be able to:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Implement"})," LLM-based task planning using function calling patterns"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Validate"})," LLM outputs with Pydantic schemas for type safety and correctness"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Design"})," feedback loops that enable LLMs to recover from execution errors"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Apply"})," safety constraints to prevent physically impossible or dangerous actions"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"quick-start-pick-up-cup-planning-15-minutes",children:"Quick Start: Pick Up Cup Planning (15 Minutes)"}),"\n",(0,t.jsx)(e.p,{children:'Transform "Pick up the cup" into executable robot actions using an LLM planner.'}),"\n",(0,t.jsx)(e.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"OpenAI API key (or Anthropic Claude API)"}),"\n",(0,t.jsx)(e.li,{children:"ROS 2 Humble installed"}),"\n",(0,t.jsxs)(e.li,{children:["Python 3.11+ with ",(0,t.jsx)(e.code,{children:"openai"})," and ",(0,t.jsx)(e.code,{children:"pydantic"})," packages"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"step-1-install-dependencies",children:"Step 1: Install Dependencies"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-bash",children:'pip install openai pydantic\r\nexport OPENAI_API_KEY="your-api-key-here"  # Or use .env file\n'})}),"\n",(0,t.jsx)(e.h3,{id:"step-2-define-action-schema",children:"Step 2: Define Action Schema"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# robot_actions.py\r\nfrom pydantic import BaseModel, Field\r\nfrom typing import Literal, List\r\nfrom enum import Enum\r\n\r\nclass ActionType(str, Enum):\r\n    """Valid robot action types"""\r\n    NAVIGATE = "navigate"\r\n    SCAN = "scan"\r\n    GRASP = "grasp"\r\n    PLACE = "place"\r\n    RELEASE = "release"\r\n\r\nclass RobotAction(BaseModel):\r\n    """Single robot action with parameters"""\r\n    action_type: ActionType\r\n    target: str = Field(..., description="Target object or location name")\r\n    position: tuple[float, float, float] | None = Field(None, description="3D coordinates (x, y, z) in meters")\r\n    confidence: float = Field(default=1.0, ge=0.0, le=1.0)\r\n    \r\n    class Config:\r\n        use_enum_values = True\r\n\r\nclass TaskPlan(BaseModel):\r\n    """Complete task plan as sequence of actions"""\r\n    task_description: str\r\n    actions: List[RobotAction] = Field(..., min_items=1, max_items=20)\r\n    estimated_duration: float = Field(..., description="Estimated duration in seconds", gt=0)\n'})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Expected"}),": Schemas define valid action types and enforce constraints (e.g., max 20 actions)."]}),"\n",(0,t.jsx)(e.h3,{id:"step-3-create-llm-function-calling",children:"Step 3: Create LLM Function Calling"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# llm_planner.py\r\nfrom openai import OpenAI\r\nimport json\r\nfrom robot_actions import TaskPlan, RobotAction, ActionType\r\n\r\nclient = OpenAI()  # Reads OPENAI_API_KEY from env\r\n\r\n# Define function schema for OpenAI\r\nFUNCTION_SCHEMA = {\r\n    "name": "create_robot_plan",\r\n    "description": "Generate a sequence of robot actions to accomplish a task",\r\n    "parameters": {\r\n        "type": "object",\r\n        "properties": {\r\n            "task_description": {\r\n                "type": "string",\r\n                "description": "High-level task description"\r\n            },\r\n            "actions": {\r\n                "type": "array",\r\n                "items": {\r\n                    "type": "object",\r\n                    "properties": {\r\n                        "action_type": {\r\n                            "type": "string",\r\n                            "enum": ["navigate", "scan", "grasp", "place", "release"]\r\n                        },\r\n                        "target": {"type": "string"},\r\n                        "position": {\r\n                            "type": "array",\r\n                            "items": {"type": "number"},\r\n                            "minItems": 3,\r\n                            "maxItems": 3\r\n                        }\r\n                    },\r\n                    "required": ["action_type", "target"]\r\n                }\r\n            },\r\n            "estimated_duration": {"type": "number"}\r\n        },\r\n        "required": ["task_description", "actions", "estimated_duration"]\r\n    }\r\n}\r\n\r\ndef plan_task(task_description: str) -> TaskPlan:\r\n    """Use LLM to generate task plan"""\r\n    response = client.chat.completions.create(\r\n        model="gpt-4",\r\n        messages=[\r\n            {"role": "system", "content": "You are a robot task planner. Generate executable action sequences."},\r\n            {"role": "user", "content": f"Plan how to: {task_description}"}\r\n        ],\r\n        functions=[FUNCTION_SCHEMA],\r\n        function_call={"name": "create_robot_plan"}\r\n    )\r\n    \r\n    # Extract function call arguments\r\n    function_call = response.choices[0].message.function_call\r\n    plan_data = json.loads(function_call.arguments)\r\n    \r\n    # Validate with Pydantic\r\n    plan = TaskPlan(**plan_data)\r\n    return plan\n'})}),"\n",(0,t.jsx)(e.h3,{id:"step-4-test-the-planner",children:"Step 4: Test the Planner"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# test_planner.py\r\nfrom llm_planner import plan_task\r\n\r\n# Test with simple command\r\nplan = plan_task("Pick up the cup from the table")\r\n\r\nprint(f"Task: {plan.task_description}")\r\nprint(f"Estimated Duration: {plan.estimated_duration}s")\r\nprint(f"\\nAction Sequence ({len(plan.actions)} steps):")\r\n\r\nfor i, action in enumerate(plan.actions, 1):\r\n    print(f"{i}. {action.action_type.upper()}: {action.target}")\r\n    if action.position:\r\n        print(f"   Position: {action.position}")\n'})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Expected Output"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Task: Pick up the cup from the table\r\nEstimated Duration: 15.0s\r\n\r\nAction Sequence (4 steps):\r\n1. NAVIGATE: table\r\n   Position: (1.5, 0.3, 0.0)\r\n2. SCAN: cup\r\n3. GRASP: cup\r\n   Position: (1.5, 0.3, 0.8)\r\n4. NAVIGATE: home_position\r\n   Position: (0.0, 0.0, 0.0)\n"})}),"\n",(0,t.jsx)(e.h3,{id:"step-5-integrate-with-ros-2",children:"Step 5: Integrate with ROS 2"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# planning_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom llm_planner import plan_task\r\nimport json\r\n\r\nclass CognitivePlanningNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'cognitive_planning_node\')\r\n        \r\n        # Subscribe to voice commands\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            \'/voice/command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        # Publish action plans\r\n        self.plan_pub = self.create_publisher(String, \'/robot/action_plan\', 10)\r\n        \r\n        self.get_logger().info("Cognitive Planning Node ready")\r\n    \r\n    def command_callback(self, msg):\r\n        """Generate plan from voice command"""\r\n        command = msg.data\r\n        self.get_logger().info(f"Planning task: {command}")\r\n        \r\n        try:\r\n            # Generate plan using LLM\r\n            plan = plan_task(command)\r\n            \r\n            # Publish as JSON\r\n            plan_msg = String()\r\n            plan_msg.data = plan.model_dump_json()\r\n            self.plan_pub.publish(plan_msg)\r\n            \r\n            self.get_logger().info(f"Plan generated: {len(plan.actions)} actions")\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f"Planning failed: {e}")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = CognitivePlanningNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Result"}),": Voice commands now automatically generate validated action plans! \ud83c\udf89"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"llm-planning-fundamentals",children:"LLM Planning Fundamentals"}),"\n",(0,t.jsx)(e.h3,{id:"why-llms-for-robot-planning",children:"Why LLMs for Robot Planning?"}),"\n",(0,t.jsx)(e.p,{children:"Traditional robot planning uses algorithms like A* for path planning or PDDL for task planning. These approaches require:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Explicit world models"}),": Complete knowledge of environment state"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hand-coded rules"}),': "If object is graspable, approach from above"']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Domain-specific languages"}),": PDDL, STRIPS, or custom DSLs"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Expert tuning"}),": Parameter adjustment for each robot/task"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"LLM-based planning"})," offers a paradigm shift:"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Zero-shot task understanding"}),": No hand-coded rules needed"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Natural language interface"}),': "Clean the room" instead of formal syntax']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Common-sense reasoning"}),": Knows cups are graspable, tables are surfaces"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Flexible adaptation"}),": Handles novel tasks without retraining"]}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),":"]}),"\n",(0,t.jsx)(e.p,{children:"Traditional planner requires:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'def plan_pick_up(object_name, location):\r\n    if not is_graspable(object_name):\r\n        raise ValueError(f"{object_name} cannot be grasped")\r\n    if not is_reachable(location):\r\n        raise ValueError(f"{location} is out of reach")\r\n    return [Navigate(location), Grasp(object_name)]\n'})}),"\n",(0,t.jsx)(e.p,{children:"LLM planner:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'plan = llm.plan("Pick up the cup")  # Figures out navigate \u2192 grasp automatically\n'})}),"\n",(0,t.jsx)(e.h3,{id:"limitations-and-risks",children:"Limitations and Risks"}),"\n",(0,t.jsxs)(e.p,{children:["\u26a0\ufe0f ",(0,t.jsx)(e.strong,{children:"Critical"}),": LLMs can generate physically impossible or unsafe plans. Always validate outputs."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Common LLM Planning Errors"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Hallucinated objects"}),": Plans actions for objects that don't exist"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Incorrect physics"}),': "Fly to the ceiling" (robot can\'t fly)']}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Missing dependencies"}),": Tries to grasp before navigating to object"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Infinite loops"}),': "Navigate to X, navigate back, navigate to X..."']}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Mitigation"}),": Use Pydantic validation + safety constraints (covered below)."]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"function-calling-pattern",children:"Function Calling Pattern"}),"\n",(0,t.jsx)(e.h3,{id:"what-is-function-calling",children:"What is Function Calling?"}),"\n",(0,t.jsx)(e.p,{children:'Function calling (also called "tool use") allows LLMs to generate structured outputs that match predefined schemas. Instead of free-form text, the LLM produces JSON that conforms to a function signature.'}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Without Function Calling"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'response = llm.complete("Plan: Pick up cup")\r\n# Returns: "First, navigate to the table. Then scan for the cup..."\r\n# Problem: Unstructured text, hard to parse\n'})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"With Function Calling"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'response = llm.call_function("create_robot_plan", prompt)\r\n# Returns: {"actions": [{"action_type": "navigate", "target": "table"}, ...]}\r\n# Benefit: Structured JSON, type-safe\n'})}),"\n",(0,t.jsx)(e.h3,{id:"implementation-with-openai-api",children:"Implementation with OpenAI API"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# complete_llm_planner.py\r\nfrom openai import OpenAI\r\nfrom typing import List\r\nimport json\r\n\r\nclient = OpenAI()\r\n\r\n# Define multiple functions for different robot capabilities\r\nFUNCTIONS = [\r\n    {\r\n        "name": "create_navigation_plan",\r\n        "description": "Plan a navigation path to a target location",\r\n        "parameters": {\r\n            "type": "object",\r\n            "properties": {\r\n                "target_location": {"type": "string"},\r\n                "avoid_obstacles": {"type": "boolean"},\r\n                "max_speed": {"type": "number", "minimum": 0.1, "maximum": 2.0}\r\n            },\r\n            "required": ["target_location"]\r\n        }\r\n    },\r\n    {\r\n        "name": "create_manipulation_plan",\r\n        "description": "Plan object manipulation (grasp, place, release)",\r\n        "parameters": {\r\n            "type": "object",\r\n            "properties": {\r\n                "object_name": {"type": "string"},\r\n                "action": {"type": "string", "enum": ["grasp", "place", "release"]},\r\n                "target_location": {"type": "string"}\r\n            },\r\n            "required": ["object_name", "action"]\r\n        }\r\n    },\r\n    {\r\n        "name": "create_task_plan",\r\n        "description": "Plan a complete multi-step task",\r\n        "parameters": {\r\n            "type": "object",\r\n            "properties": {\r\n                "task_description": {"type": "string"},\r\n                "actions": {\r\n                    "type": "array",\r\n                    "items": {\r\n                        "type": "object",\r\n                        "properties": {\r\n                            "action_type": {"type": "string"},\r\n                            "parameters": {"type": "object"}\r\n                        }\r\n                    }\r\n                }\r\n            },\r\n            "required": ["task_description", "actions"]\r\n        }\r\n    }\r\n]\r\n\r\ndef plan_with_multi_functions(user_request: str):\r\n    """Let LLM choose which function to call based on request"""\r\n    response = client.chat.completions.create(\r\n        model="gpt-4",\r\n        messages=[\r\n            {"role": "system", "content": "You are a helpful robot assistant. Choose the appropriate planning function based on the user\'s request."},\r\n            {"role": "user", "content": user_request}\r\n        ],\r\n        functions=FUNCTIONS,\r\n        function_call="auto"  # Let LLM choose function\r\n    )\r\n    \r\n    message = response.choices[0].message\r\n    \r\n    if message.function_call:\r\n        function_name = message.function_call.name\r\n        arguments = json.loads(message.function_call.arguments)\r\n        return {\r\n            "function": function_name,\r\n            "arguments": arguments\r\n        }\r\n    else:\r\n        return {"error": "No function called", "content": message.content}\r\n\r\n# Test\r\nresult = plan_with_multi_functions("Navigate to the kitchen")\r\nprint(f"LLM chose function: {result[\'function\']}")\r\nprint(f"Arguments: {result[\'arguments\']}")\n'})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Expected"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"LLM chose function: create_navigation_plan\r\nArguments: {'target_location': 'kitchen', 'avoid_obstacles': True, 'max_speed': 1.0}\n"})}),"\n",(0,t.jsx)(e.h3,{id:"anthropic-claude-function-calling",children:"Anthropic Claude Function Calling"}),"\n",(0,t.jsx)(e.p,{children:"OpenAI and Anthropic use slightly different APIs. Here's the Claude equivalent:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# claude_planner.py\r\nfrom anthropic import Anthropic\r\n\r\nclient = Anthropic()  # Reads ANTHROPIC_API_KEY from env\r\n\r\nCLAUDE_TOOL = {\r\n    "name": "create_robot_plan",\r\n    "description": "Generate a sequence of robot actions",\r\n    "input_schema": {\r\n        "type": "object",\r\n        "properties": {\r\n            "actions": {\r\n                "type": "array",\r\n                "items": {\r\n                    "type": "object",\r\n                    "properties": {\r\n                        "action_type": {"type": "string"},\r\n                        "target": {"type": "string"}\r\n                    }\r\n                }\r\n            }\r\n        },\r\n        "required": ["actions"]\r\n    }\r\n}\r\n\r\ndef plan_with_claude(task: str):\r\n    response = client.messages.create(\r\n        model="claude-3-5-sonnet-20241022",\r\n        max_tokens=1024,\r\n        tools=[CLAUDE_TOOL],\r\n        messages=[{"role": "user", "content": f"Plan: {task}"}]\r\n    )\r\n    \r\n    # Extract tool use from response\r\n    for block in response.content:\r\n        if block.type == "tool_use":\r\n            return block.input\r\n    \r\n    return None\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"schema-validation-with-pydantic",children:"Schema Validation with Pydantic"}),"\n",(0,t.jsx)(e.h3,{id:"why-pydantic",children:"Why Pydantic?"}),"\n",(0,t.jsx)(e.p,{children:"LLMs sometimes generate invalid JSON:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Missing required fields"}),"\n",(0,t.jsxs)(e.li,{children:["Wrong data types (",(0,t.jsx)(e.code,{children:'"5.2"'})," instead of ",(0,t.jsx)(e.code,{children:"5.2"}),")"]}),"\n",(0,t.jsx)(e.li,{children:"Out-of-range values (negative durations, coordinates outside workspace)"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Pydantic"})," provides:"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Runtime type checking"}),": Ensures ",(0,t.jsx)(e.code,{children:"position"})," is ",(0,t.jsx)(e.code,{children:"tuple[float, float, float]"})]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Field validation"}),": Enforces ",(0,t.jsx)(e.code,{children:"confidence >= 0.0 and <= 1.0"})]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Automatic coercion"}),": Converts ",(0,t.jsx)(e.code,{children:'"5.2"'})," to ",(0,t.jsx)(e.code,{children:"5.2"})," when possible"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Clear error messages"}),': "estimated_duration must be greater than 0"']}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"advanced-pydantic-models",children:"Advanced Pydantic Models"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# advanced_schemas.py\r\nfrom pydantic import BaseModel, Field, field_validator, model_validator\r\nfrom typing import List, Literal\r\nimport numpy as np\r\n\r\nclass Position3D(BaseModel):\r\n    """3D position with workspace constraints"""\r\n    x: float = Field(..., ge=-5.0, le=5.0, description="X coordinate in meters")\r\n    y: float = Field(..., ge=-5.0, le=5.0, description="Y coordinate in meters")\r\n    z: float = Field(..., ge=0.0, le=2.5, description="Z coordinate in meters (height)")\r\n    \r\n    @field_validator(\'x\', \'y\', \'z\')\r\n    @classmethod\r\n    def round_precision(cls, v):\r\n        """Round to 3 decimal places"""\r\n        return round(v, 3)\r\n\r\nclass GraspAction(BaseModel):\r\n    """Grasp action with physics constraints"""\r\n    action_type: Literal["grasp"] = "grasp"\r\n    object_name: str = Field(..., min_length=1, max_length=50)\r\n    approach_vector: tuple[float, float, float] = Field(default=(0, 0, -1), description="Grasp approach direction")\r\n    gripper_width: float = Field(default=0.08, ge=0.0, le=0.15, description="Gripper opening in meters")\r\n    force: float = Field(default=10.0, ge=1.0, le=50.0, description="Grasp force in Newtons")\r\n    \r\n    @field_validator(\'approach_vector\')\r\n    @classmethod\r\n    def normalize_vector(cls, v):\r\n        """Ensure approach vector is normalized"""\r\n        norm = np.linalg.norm(v)\r\n        if norm == 0:\r\n            raise ValueError("Approach vector cannot be zero")\r\n        return tuple(np.array(v) / norm)\r\n\r\nclass NavigateAction(BaseModel):\r\n    """Navigation action with path constraints"""\r\n    action_type: Literal["navigate"] = "navigate"\r\n    target: Position3D\r\n    max_velocity: float = Field(default=1.0, ge=0.1, le=2.0, description="Max speed in m/s")\r\n    obstacle_clearance: float = Field(default=0.3, ge=0.1, le=1.0, description="Minimum clearance in meters")\r\n\r\nclass EnhancedTaskPlan(BaseModel):\r\n    """Task plan with cross-action validation"""\r\n    task_description: str\r\n    actions: List[GraspAction | NavigateAction]\r\n    total_distance: float | None = None\r\n    \r\n    @model_validator(mode=\'after\')\r\n    def validate_action_sequence(self):\r\n        """Ensure action sequence is physically valid"""\r\n        # Check: Can\'t grasp before navigating to object\r\n        for i, action in enumerate(self.actions):\r\n            if isinstance(action, GraspAction):\r\n                if i == 0:\r\n                    raise ValueError("Cannot grasp as first action (must navigate first)")\r\n                prev_action = self.actions[i-1]\r\n                if not isinstance(prev_action, NavigateAction):\r\n                    raise ValueError(f"Grasp must be preceded by navigation, got {type(prev_action).__name__}")\r\n        \r\n        return self\r\n\r\n# Test validation\r\ntry:\r\n    plan = EnhancedTaskPlan(\r\n        task_description="Pick up cup",\r\n        actions=[\r\n            GraspAction(object_name="cup")  # Invalid: grasp without navigation\r\n        ]\r\n    )\r\nexcept ValueError as e:\r\n    print(f"Validation error: {e}")\r\n    # Output: "Cannot grasp as first action (must navigate first)"\n'})}),"\n",(0,t.jsx)(e.h3,{id:"custom-validators-for-robot-safety",children:"Custom Validators for Robot Safety"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# safety_validators.py\r\nfrom pydantic import BaseModel, field_validator\r\n\r\n# Define safe workspace bounds (example for tabletop robot)\r\nSAFE_WORKSPACE = {\r\n    "x_min": -0.5, "x_max": 0.5,\r\n    "y_min": -0.3, "y_max": 0.3,\r\n    "z_min": 0.0, "z_max": 0.4\r\n}\r\n\r\nclass SafePosition(BaseModel):\r\n    x: float\r\n    y: float\r\n    z: float\r\n    \r\n    @field_validator(\'x\')\r\n    @classmethod\r\n    def validate_x(cls, v):\r\n        if not (SAFE_WORKSPACE["x_min"] <= v <= SAFE_WORKSPACE["x_max"]):\r\n            raise ValueError(f"X position {v} outside safe workspace")\r\n        return v\r\n    \r\n    @field_validator(\'y\')\r\n    @classmethod\r\n    def validate_y(cls, v):\r\n        if not (SAFE_WORKSPACE["y_min"] <= v <= SAFE_WORKSPACE["y_max"]):\r\n            raise ValueError(f"Y position {v} outside safe workspace")\r\n        return v\r\n    \r\n    @field_validator(\'z\')\r\n    @classmethod\r\n    def validate_z(cls, v):\r\n        if not (SAFE_WORKSPACE["z_min"] <= v <= SAFE_WORKSPACE["z_max"]):\r\n            raise ValueError(f"Z position {v} outside safe workspace")\r\n        return v\r\n\r\n# LLM might generate position outside workspace\r\ntry:\r\n    dangerous_pos = SafePosition(x=10.0, y=0.0, z=0.0)  # Far outside bounds\r\nexcept ValueError as e:\r\n    print(f"Safety violation prevented: {e}")\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"feedback-loops-for-error-recovery",children:"Feedback Loops for Error Recovery"}),"\n",(0,t.jsx)(e.h3,{id:"why-feedback-loops",children:"Why Feedback Loops?"}),"\n",(0,t.jsx)(e.p,{children:"Real-world robot execution fails frequently:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Object moved since planning"}),"\n",(0,t.jsx)(e.li,{children:"Grasp failed (object slipped)"}),"\n",(0,t.jsx)(e.li,{children:"Navigation blocked by unexpected obstacle"}),"\n",(0,t.jsx)(e.li,{children:"Sensor noise caused mis-detection"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Feedback loops"})," allow LLMs to adapt plans based on execution results."]}),"\n",(0,t.jsx)(e.h3,{id:"architecture",children:"Architecture"}),"\n",(0,t.jsx)(e.mermaid,{value:'sequenceDiagram\r\n    participant User\r\n    participant Planner as LLM Planner\r\n    participant Executor as Action Executor\r\n    participant Robot\r\n    \r\n    User->>Planner: "Pick up cup"\r\n    Planner->>Executor: Plan: [navigate, grasp, place]\r\n    Executor->>Robot: Execute action 1: navigate\r\n    Robot->>Executor: \u2713 Success\r\n    Executor->>Robot: Execute action 2: grasp\r\n    Robot->>Executor: \u2717 Failure (object slipped)\r\n    Executor->>Planner: Error: "Grasp failed, cup slipped"\r\n    Planner->>Planner: Replan considering error\r\n    Planner->>Executor: Updated plan: [approach_differently, reattempt_grasp]\r\n    Executor->>Robot: Execute revised plan\r\n    Robot->>Executor: \u2713 Success\r\n    Executor->>User: Task completed'}),"\n",(0,t.jsx)(e.h3,{id:"implementation",children:"Implementation"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# feedback_loop.py\r\nfrom openai import OpenAI\r\nfrom robot_actions import TaskPlan, RobotAction\r\nimport json\r\n\r\nclient = OpenAI()\r\n\r\nclass FeedbackPlanner:\r\n    def __init__(self):\r\n        self.conversation_history = []\r\n        self.execution_log = []\r\n    \r\n    def plan(self, task: str) -> TaskPlan:\r\n        """Generate initial plan"""\r\n        self.conversation_history = [\r\n            {"role": "system", "content": "You are a robot task planner. Generate executable plans and adapt to feedback."},\r\n            {"role": "user", "content": f"Plan: {task}"}\r\n        ]\r\n        \r\n        response = client.chat.completions.create(\r\n            model="gpt-4",\r\n            messages=self.conversation_history,\r\n            functions=[FUNCTION_SCHEMA],\r\n            function_call={"name": "create_robot_plan"}\r\n        )\r\n        \r\n        # Parse plan\r\n        function_call = response.choices[0].message.function_call\r\n        plan = TaskPlan(**json.loads(function_call.arguments))\r\n        \r\n        # Save to history\r\n        self.conversation_history.append(response.choices[0].message)\r\n        \r\n        return plan\r\n    \r\n    def replan_on_error(self, failed_action: RobotAction, error_message: str) -> TaskPlan:\r\n        """Generate new plan based on execution error"""\r\n        # Add error feedback to conversation\r\n        feedback = f"Action \'{failed_action.action_type}\' on \'{failed_action.target}\' failed: {error_message}. Generate a recovery plan."\r\n        \r\n        self.conversation_history.append({"role": "user", "content": feedback})\r\n        \r\n        # Request new plan\r\n        response = client.chat.completions.create(\r\n            model="gpt-4",\r\n            messages=self.conversation_history,\r\n            functions=[FUNCTION_SCHEMA],\r\n            function_call={"name": "create_robot_plan"}\r\n        )\r\n        \r\n        # Parse recovery plan\r\n        function_call = response.choices[0].message.function_call\r\n        recovery_plan = TaskPlan(**json.loads(function_call.arguments))\r\n        \r\n        # Log recovery attempt\r\n        self.execution_log.append({\r\n            "failed_action": failed_action.model_dump(),\r\n            "error": error_message,\r\n            "recovery_plan": recovery_plan.model_dump()\r\n        })\r\n        \r\n        return recovery_plan\r\n\r\n# Usage example\r\nplanner = FeedbackPlanner()\r\n\r\n# Initial plan\r\nplan = planner.plan("Pick up the cup from the table")\r\nprint(f"Initial plan: {len(plan.actions)} actions")\r\n\r\n# Simulate grasp failure\r\nfailed_action = plan.actions[2]  # Assume action 2 is grasp\r\nrecovery = planner.replan_on_error(\r\n    failed_action,\r\n    "Grasp failed: cup slipped from gripper. Object appears wet."\r\n)\r\n\r\nprint(f"\\nRecovery plan: {len(recovery.actions)} actions")\r\nfor action in recovery.actions:\r\n    print(f"  - {action.action_type}: {action.target}")\n'})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Expected Recovery Plan"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"Initial plan: 4 actions\r\n  - navigate: table\r\n  - scan: cup\r\n  - grasp: cup\r\n  - navigate: home\r\n\r\nRecovery plan: 3 actions\r\n  - adjust_gripper: increase_force\r\n  - dry_object: cup (using cloth)\r\n  - reattempt_grasp: cup\n"})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"safety-constraints--validation",children:"Safety Constraints & Validation"}),"\n",(0,t.jsx)(e.h3,{id:"pre-execution-validation",children:"Pre-execution Validation"}),"\n",(0,t.jsx)(e.p,{children:"Always validate LLM plans before execution:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# safety_validator.py\r\nfrom robot_actions import TaskPlan, RobotAction, ActionType\r\nfrom typing import List\r\n\r\nclass SafetyValidator:\r\n    """Validate task plans for safety and feasibility"""\r\n    \r\n    def __init__(self, workspace_bounds, max_actions=20, max_duration=300):\r\n        self.workspace_bounds = workspace_bounds\r\n        self.max_actions = max_actions\r\n        self.max_duration = max_duration\r\n    \r\n    def validate_plan(self, plan: TaskPlan) -> tuple[bool, List[str]]:\r\n        """\r\n        Validate plan for safety violations.\r\n        \r\n        Returns:\r\n            (is_valid, error_messages)\r\n        """\r\n        errors = []\r\n        \r\n        # Check 1: Plan length\r\n        if len(plan.actions) > self.max_actions:\r\n            errors.append(f"Plan too long: {len(plan.actions)} actions (max {self.max_actions})")\r\n        \r\n        # Check 2: Duration\r\n        if plan.estimated_duration > self.max_duration:\r\n            errors.append(f"Plan too slow: {plan.estimated_duration}s (max {self.max_duration}s)")\r\n        \r\n        # Check 3: Workspace bounds\r\n        for i, action in enumerate(plan.actions):\r\n            if action.position:\r\n                x, y, z = action.position\r\n                if not self._in_workspace(x, y, z):\r\n                    errors.append(f"Action {i}: Position {action.position} outside safe workspace")\r\n        \r\n        # Check 4: Action sequence validity\r\n        sequence_errors = self._validate_sequence(plan.actions)\r\n        errors.extend(sequence_errors)\r\n        \r\n        return (len(errors) == 0, errors)\r\n    \r\n    def _in_workspace(self, x, y, z):\r\n        """Check if position is within safe workspace"""\r\n        return (\r\n            self.workspace_bounds["x_min"] <= x <= self.workspace_bounds["x_max"] and\r\n            self.workspace_bounds["y_min"] <= y <= self.workspace_bounds["y_max"] and\r\n            self.workspace_bounds["z_min"] <= z <= self.workspace_bounds["z_max"]\r\n        )\r\n    \r\n    def _validate_sequence(self, actions: List[RobotAction]) -> List[str]:\r\n        """Validate action sequence logic"""\r\n        errors = []\r\n        \r\n        # Rule: Can\'t grasp without first navigating to object\r\n        for i, action in enumerate(actions):\r\n            if action.action_type == ActionType.GRASP:\r\n                if i == 0:\r\n                    errors.append("Cannot grasp as first action")\r\n                else:\r\n                    prev_action = actions[i-1]\r\n                    if prev_action.action_type not in [ActionType.NAVIGATE, ActionType.SCAN]:\r\n                        errors.append(f"Grasp must follow navigate/scan, got {prev_action.action_type}")\r\n        \r\n        # Rule: Can\'t place without first grasping\r\n        has_grasped = False\r\n        for i, action in enumerate(actions):\r\n            if action.action_type == ActionType.GRASP:\r\n                has_grasped = True\r\n            if action.action_type == ActionType.PLACE and not has_grasped:\r\n                errors.append(f"Action {i}: Cannot place without grasping first")\r\n        \r\n        # Rule: Detect infinite loops (same action repeated >3 times)\r\n        action_counts = {}\r\n        for action in actions:\r\n            key = (action.action_type, action.target)\r\n            action_counts[key] = action_counts.get(key, 0) + 1\r\n            if action_counts[key] > 3:\r\n                errors.append(f"Potential infinite loop: {action.action_type} on {action.target} repeated {action_counts[key]} times")\r\n        \r\n        return errors\r\n\r\n# Usage\r\nvalidator = SafetyValidator(\r\n    workspace_bounds={"x_min": -1.0, "x_max": 1.0, "y_min": -1.0, "y_max": 1.0, "z_min": 0.0, "z_max": 2.0}\r\n)\r\n\r\n# Validate a plan\r\nis_valid, errors = validator.validate_plan(plan)\r\nif not is_valid:\r\n    print("\u26a0\ufe0f  Plan validation failed:")\r\n    for error in errors:\r\n        print(f"  - {error}")\r\nelse:\r\n    print("\u2713 Plan is safe to execute")\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"common-errors",children:"Common Errors"}),"\n",(0,t.jsx)(e.h3,{id:"error-1-invalid-json-from-llm",children:'Error 1: "Invalid JSON from LLM"'}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"json.JSONDecodeError: Expecting ',' delimiter: line 5 column 3\n"})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"LLM generated malformed JSON (missing comma, trailing comma, unquoted keys)"}),"\n",(0,t.jsx)(e.li,{children:"Function calling not enforced (model returned text instead of function call)"}),"\n",(0,t.jsx)(e.li,{children:"Model doesn't support function calling (older GPT-3.5 models)"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Solution 1: Retry with explicit format instruction\r\ndef plan_with_retry(task, max_retries=3):\r\n    for attempt in range(max_retries):\r\n        try:\r\n            response = client.chat.completions.create(\r\n                model="gpt-4",\r\n                messages=[\r\n                    {"role": "system", "content": "Output ONLY valid JSON. No explanation."},\r\n                    {"role": "user", "content": task}\r\n                ],\r\n                functions=[FUNCTION_SCHEMA],\r\n                function_call={"name": "create_robot_plan"}\r\n            )\r\n            return json.loads(response.choices[0].message.function_call.arguments)\r\n        except json.JSONDecodeError as e:\r\n            print(f"Attempt {attempt+1} failed: {e}")\r\n            if attempt == max_retries - 1:\r\n                raise\r\n    return None\r\n\r\n# Solution 2: Use Pydantic\'s model_validate_json (more forgiving)\r\nfrom pydantic import ValidationError\r\n\r\ntry:\r\n    plan = TaskPlan.model_validate_json(llm_response)\r\nexcept ValidationError as e:\r\n    print(f"Validation error: {e}")\n'})}),"\n",(0,t.jsx)(e.h3,{id:"error-2-impossible-actions-generated",children:'Error 2: "Impossible actions generated"'}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:'LLM plans "Fly to ceiling" (robot can\'t fly)'}),"\n",(0,t.jsx)(e.li,{children:"Plans grasp on non-existent object"}),"\n",(0,t.jsx)(e.li,{children:"Navigates outside workspace bounds"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"LLM lacks robot capability knowledge"}),"\n",(0,t.jsx)(e.li,{children:"No safety constraints in prompt"}),"\n",(0,t.jsx)(e.li,{children:"No validation layer"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Solution: Add capability constraints to system prompt\r\nSYSTEM_PROMPT = """You are a robot task planner with these constraints:\r\n\r\nCAPABILITIES:\r\n- Can navigate on ground (x, y), cannot fly (z must be 0 during navigation)\r\n- Can grasp objects weighing <2kg with gripper\r\n- Workspace bounds: x\u2208[-1,1]m, y\u2208[-1,1]m, z\u2208[0,2]m\r\n- Max reach: 0.8m from base\r\n\r\nKNOWN OBJECTS:\r\n- cup (0.5kg, graspable)\r\n- table (surface, not graspable)\r\n- box (1.2kg, graspable)\r\n\r\nGenerate plans that respect these constraints. If a task is impossible, return an error action."""\r\n\r\n# Then use SafetyValidator to catch any violations\n'})}),"\n",(0,t.jsx)(e.h3,{id:"error-3-llm-request-timeout",children:'Error 3: "LLM request timeout"'}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:"openai.error.Timeout: Request timed out after 60 seconds\n"})}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Complex task requires long thinking time"}),"\n",(0,t.jsx)(e.li,{children:"API service overloaded"}),"\n",(0,t.jsx)(e.li,{children:"Network latency"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Solution 1: Increase timeout\r\nclient = OpenAI(timeout=120.0)  # 2 minutes\r\n\r\n# Solution 2: Use streaming for progress updates\r\ndef plan_with_streaming(task):\r\n    stream = client.chat.completions.create(\r\n        model="gpt-4",\r\n        messages=[{"role": "user", "content": task}],\r\n        functions=[FUNCTION_SCHEMA],\r\n        function_call={"name": "create_robot_plan"},\r\n        stream=True\r\n    )\r\n    \r\n    function_args = ""\r\n    for chunk in stream:\r\n        if chunk.choices[0].delta.function_call:\r\n            function_args += chunk.choices[0].delta.function_call.arguments or ""\r\n            print(".", end="", flush=True)  # Progress indicator\r\n    \r\n    return json.loads(function_args)\r\n\r\n# Solution 3: Fallback to simpler model\r\ndef plan_with_fallback(task):\r\n    try:\r\n        return plan_with_model(task, model="gpt-4", timeout=30)\r\n    except Timeout:\r\n        print("GPT-4 timed out, falling back to GPT-3.5-turbo")\r\n        return plan_with_model(task, model="gpt-3.5-turbo", timeout=60)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"error-4-infinite-loop-in-action-sequence",children:'Error 4: "Infinite loop in action sequence"'}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Symptoms"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Plan contains repeated actions: ",(0,t.jsx)(e.code,{children:"[navigate(A), navigate(B), navigate(A), navigate(B), ...]"})]}),"\n",(0,t.jsx)(e.li,{children:"Robot oscillates between positions"}),"\n",(0,t.jsx)(e.li,{children:"Plan never completes"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Causes"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"LLM misunderstands task completion criteria"}),"\n",(0,t.jsx)(e.li,{children:"Feedback loop amplifies errors"}),"\n",(0,t.jsx)(e.li,{children:"No loop detection"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'# Solution: Detect and break loops in validator\r\ndef detect_loops(actions: List[RobotAction], max_repeats=2):\r\n    """Detect repeated action patterns"""\r\n    for i in range(len(actions) - max_repeats):\r\n        window = actions[i:i+max_repeats+1]\r\n        # Check if same action repeated\r\n        if all(a.action_type == window[0].action_type and a.target == window[0].target for a in window):\r\n            return True, f"Loop detected: {window[0].action_type} on {window[0].target} repeated {max_repeats+1} times"\r\n    return False, ""\r\n\r\n# Add to SafetyValidator\r\nhas_loop, loop_msg = detect_loops(plan.actions)\r\nif has_loop:\r\n    raise ValueError(f"Plan contains infinite loop: {loop_msg}")\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(e.h3,{id:"exercise-1-add-new-action-type-easy-30-minutes",children:"Exercise 1: Add New Action Type (Easy, 30 minutes)"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Goal"}),': Extend the planner to support a new "scan" action that inspects objects before grasping.']}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["Add ",(0,t.jsx)(e.code,{children:"SCAN"})," to ",(0,t.jsx)(e.code,{children:"ActionType"})," enum"]}),"\n",(0,t.jsxs)(e.li,{children:["Create ",(0,t.jsx)(e.code,{children:"ScanAction"})," Pydantic model with field ",(0,t.jsx)(e.code,{children:"scan_duration: float"})]}),"\n",(0,t.jsx)(e.li,{children:"Update function schema to include scan action"}),"\n",(0,t.jsx)(e.li,{children:"Test LLM generates scan before grasp"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Hints"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class ScanAction(BaseModel):\r\n    action_type: Literal["scan"] = "scan"\r\n    target: str\r\n    scan_duration: float = Field(default=2.0, ge=0.5, le=10.0)\r\n    scan_mode: Literal["visual", "depth", "both"] = "both"\n'})}),"\n",(0,t.jsx)(e.h3,{id:"exercise-2-multi-step-task-planning-medium-45-minutes",children:"Exercise 2: Multi-step Task Planning (Medium, 45 minutes)"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Goal"}),': Plan a complex task like "Clean the table" that requires multiple object manipulations.']}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Define task: Pick up all objects from table, move to storage bin"}),"\n",(0,t.jsx)(e.li,{children:"Generate plan with LLM (should include multiple grasp-place cycles)"}),"\n",(0,t.jsx)(e.li,{children:"Validate plan ensures objects are placed, not just grasped"}),"\n",(0,t.jsx)(e.li,{children:"Add constraint: Max 3 objects can be carried simultaneously"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Hints"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:["Use ",(0,t.jsx)(e.code,{children:"model_validator"})," to track carried objects count"]}),"\n",(0,t.jsxs)(e.li,{children:["Require ",(0,t.jsx)(e.code,{children:"RELEASE"})," action before grasping 4th object"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"exercise-3-state-machine-integration-hard-60-minutes",children:"Exercise 3: State Machine Integration (Hard, 60 minutes)"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Goal"}),": Integrate LLM planner with a finite state machine for robust execution."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsx)(e.li,{children:"Define states: IDLE, PLANNING, EXECUTING, ERROR, REPLANNING"}),"\n",(0,t.jsxs)(e.li,{children:["Implement state machine that:","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Transitions IDLE \u2192 PLANNING on voice command"}),"\n",(0,t.jsx)(e.li,{children:"Transitions PLANNING \u2192 EXECUTING when plan validated"}),"\n",(0,t.jsx)(e.li,{children:"Transitions EXECUTING \u2192 ERROR on action failure"}),"\n",(0,t.jsx)(e.li,{children:"Transitions ERROR \u2192 REPLANNING, then back to EXECUTING"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.li,{children:"Add timeout: If stuck in REPLANNING >3 attempts, abort task"}),"\n",(0,t.jsx)(e.li,{children:"Log all state transitions"}),"\n"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Hints"}),":"]}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from enum import Enum\r\n\r\nclass State(Enum):\r\n    IDLE = 1\r\n    PLANNING = 2\r\n    EXECUTING = 3\r\n    ERROR = 4\r\n    REPLANNING = 5\r\n\r\nclass StateMachine:\r\n    def __init__(self):\r\n        self.state = State.IDLE\r\n        self.replan_attempts = 0\r\n    \r\n    def transition(self, new_state: State, reason: str):\r\n        print(f"State: {self.state.name} \u2192 {new_state.name} ({reason})")\r\n        self.state = new_state\n'})}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsx)(e.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Large Language Models as Zero-Shot Planners"})," (Huang et al., 2022)"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/2201.07207",children:"https://arxiv.org/abs/2201.07207"})}),"\n",(0,t.jsx)(e.li,{children:"Shows LLMs can plan complex tasks without fine-tuning"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"ProgPrompt: Program-Guided Prompt Engineering for Robot Planning"})," (Singh et al., 2023)"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Combines LLMs with structured programming for safer plans"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"documentation",children:"Documentation"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"OpenAI Function Calling Guide"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://platform.openai.com/docs/guides/function-calling",children:"https://platform.openai.com/docs/guides/function-calling"})}),"\n",(0,t.jsx)(e.li,{children:"Official documentation for function calling API"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(e.li,{children:["\n",(0,t.jsx)(e.p,{children:(0,t.jsx)(e.strong,{children:"Pydantic Documentation"})}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://docs.pydantic.dev/",children:"https://docs.pydantic.dev/"})}),"\n",(0,t.jsx)(e.li,{children:"Complete guide to data validation with Pydantic"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"tutorials",children:"Tutorials"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"LLM Planning Survey"})," (Valmeekam et al., 2023)","\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:(0,t.jsx)(e.a,{href:"https://arxiv.org/abs/2305.15771",children:"https://arxiv.org/abs/2305.15771"})}),"\n",(0,t.jsx)(e.li,{children:"Comprehensive analysis of LLM planning capabilities and limitations"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(e.hr,{}),"\n",(0,t.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(e.p,{children:["You've mastered cognitive planning! In the next sub-chapter, ",(0,t.jsx)(e.a,{href:"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration",children:"4.4 Vision Integration & Object Detection"}),", you'll learn how to integrate computer vision so your robot can see the cup before trying to pick it up."]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"\u2705 Function calling enables structured LLM outputs (type-safe JSON)"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Pydantic validation catches invalid plans before execution"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Feedback loops allow LLMs to recover from errors dynamically"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Safety validators prevent dangerous or impossible actions"}),"\n",(0,t.jsx)(e.li,{children:"\u2705 Always validate LLM outputs\u2014they can hallucinate impossible plans"}),"\n"]})]})}function p(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>s,x:()=>o});var i=r(6540);const t={},a=i.createContext(t);function s(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);