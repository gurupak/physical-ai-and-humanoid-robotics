"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[7808],{6027:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"chapter-4-vla/capstone-project","title":"4.5 Capstone Project: Autonomous Humanoid","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/05-capstone-project.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/capstone-project","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/05-capstone-project.md","tags":[{"inline":true,"label":"Capstone","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/capstone"},{"inline":true,"label":"Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/integration"},{"inline":true,"label":"End-to-End VLA","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/end-to-end-vla"},{"inline":true,"label":"Project Deliverables","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/project-deliverables"}],"version":"current","sidebarPosition":5,"frontMatter":{"id":"capstone-project","title":"4.5 Capstone Project: Autonomous Humanoid","sidebar_label":"Capstone Project","sidebar_position":5,"sidebar_custom_props":{"difficulty":"Advanced","readingTime":"25 minutes","projectTime":"4-6 hours"},"tags":["Capstone","Integration","End-to-End VLA","Project Deliverables"]},"sidebar":"tutorialSidebar","previous":{"title":"Vision Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration"}}');var t=r(4848),i=r(8453);const o={id:"capstone-project",title:"4.5 Capstone Project: Autonomous Humanoid",sidebar_label:"Capstone Project",sidebar_position:5,sidebar_custom_props:{difficulty:"Advanced",readingTime:"25 minutes",projectTime:"4-6 hours"},tags:["Capstone","Integration","End-to-End VLA","Project Deliverables"]},a="4.5 Capstone Project: Autonomous Humanoid",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"Goal",id:"goal",level:3},{value:"Components",id:"components",level:3},{value:"Timeline",id:"timeline",level:3},{value:"Phase 1: System Architecture Design",id:"phase-1-system-architecture-design",level:2},{value:"Define Your Task",id:"define-your-task",level:3},{value:"System Architecture",id:"system-architecture",level:3},{value:"File Structure",id:"file-structure",level:3},{value:"Phase 2: Integration Strategy",id:"phase-2-integration-strategy",level:2},{value:"State Machine Design",id:"state-machine-design",level:3},{value:"Orchestrator Node Implementation",id:"orchestrator-node-implementation",level:3},{value:"Phase 3: Fine-tuning VLA on Custom Task",id:"phase-3-fine-tuning-vla-on-custom-task",level:2},{value:"Why Fine-tune?",id:"why-fine-tune",level:3},{value:"Data Collection",id:"data-collection",level:3},{value:"Fine-tuning Script",id:"fine-tuning-script",level:3},{value:"Phase 4: Deployment &amp; Testing",id:"phase-4-deployment--testing",level:2},{value:"Complete Launch File",id:"complete-launch-file",level:3},{value:"Evaluation Harness",id:"evaluation-harness",level:3},{value:"Phase 5: Troubleshooting &amp; Optimization",id:"phase-5-troubleshooting--optimization",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Common Integration Issues",id:"common-integration-issues",level:3},{value:"Issue 1: &quot;System hangs in SCANNING state&quot;",id:"issue-1-system-hangs-in-scanning-state",level:4},{value:"Issue 2: &quot;High failure rate on manipulation&quot;",id:"issue-2-high-failure-rate-on-manipulation",level:4},{value:"Issue 3: &quot;Latency budget exceeded&quot;",id:"issue-3-latency-budget-exceeded",level:4},{value:"Project Deliverables",id:"project-deliverables",level:2},{value:"Required Submissions",id:"required-submissions",level:3},{value:"Grading Rubric",id:"grading-rubric",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Add Object Sorting (Easy, 2 hours)",id:"exercise-1-add-object-sorting-easy-2-hours",level:3},{value:"Exercise 2: Multi-Robot Coordination (Medium, 4 hours)",id:"exercise-2-multi-robot-coordination-medium-4-hours",level:3},{value:"Exercise 3: Real Hardware Deployment (Hard, 8+ hours)",id:"exercise-3-real-hardware-deployment-hard-8-hours",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Case Studies",id:"case-studies",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"What&#39;s Next?",id:"whats-next",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"45-capstone-project-autonomous-humanoid",children:"4.5 Capstone Project: Autonomous Humanoid"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By completing this capstone project, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate"})," voice, cognitive planning, vision, navigation, and manipulation into a complete VLA system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fine-tune"})," pre-trained VLA models on custom robot tasks using demonstration data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy"})," end-to-end autonomous systems to simulated environments (Gazebo/Isaac Sim)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluate"})," system performance using quantitative metrics (success rate, latency, accuracy)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Troubleshoot"})," complex integration issues across multiple subsystems"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,t.jsx)(n.h3,{id:"goal",children:"Goal"}),"\n",(0,t.jsx)(n.p,{children:"Build a complete autonomous robot that receives a voice command, plans a task, navigates to objects, identifies them using computer vision, and manipulates them to accomplish the goal."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Example Task"}),': "Clean the room"']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Robot hears command via Whisper"}),"\n",(0,t.jsx)(n.li,{children:"LLM plans: [navigate to objects, identify items, grasp each, navigate to bin, place]"}),"\n",(0,t.jsx)(n.li,{children:"Vision identifies objects (cup, book, toy)"}),"\n",(0,t.jsx)(n.li,{children:"Robot executes manipulation sequence"}),"\n",(0,t.jsx)(n.li,{children:'Reports completion: "Task complete. 3 objects cleaned."'}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,t.jsx)(n.mermaid,{value:"flowchart TD\r\n    A[Voice Input] --\x3e|Whisper| B[Text Command]\r\n    B --\x3e|LLM Planner| C[Task Plan]\r\n    D[Camera Feed] --\x3e|CLIP Vision| E[Object Detections]\r\n    C --\x3e|Action Sequence| F[Orchestrator Node]\r\n    E --\x3e|3D Positions| F\r\n    F --\x3e|Navigate| G[Nav2 Stack]\r\n    F --\x3e|Grasp| H[MoveIt2]\r\n    G --\x3e|Feedback| F\r\n    H --\x3e|Feedback| F\r\n    F --\x3e|Status| I[User Feedback]\r\n    \r\n    style A fill:#E3F2FD\r\n    style D fill:#E3F2FD\r\n    style C fill:#FFF3E0\r\n    style F fill:#FCE4EC\r\n    style I fill:#E8F5E9"}),"\n",(0,t.jsx)(n.h3,{id:"timeline",children:"Timeline"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Week 1 (4-6 hours)"}),": System architecture design, component integration"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Week 2 (6-8 hours)"}),": Testing, debugging, performance optimization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Week 3 (2-3 hours)"}),": Documentation, video recording, final submission"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Total"}),": 12-17 hours"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"phase-1-system-architecture-design",children:"Phase 1: System Architecture Design"}),"\n",(0,t.jsx)(n.h3,{id:"define-your-task",children:"Define Your Task"}),"\n",(0,t.jsx)(n.p,{children:"Choose a concrete task for your robot:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Easy Tasks"})," (Recommended for beginners):"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Single Object Pickup"}),': "Pick up the cup"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Sorting"}),': "Move red objects to box A, blue objects to box B"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Table Clearing"}),': "Remove all objects from the table"']}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Medium Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Room Cleaning"}),': "Clean the room" (multiple object types, navigation required)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fetch and Deliver"}),': "Bring me the book from the shelf"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object Arrangement"}),': "Arrange objects in a line"']}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hard Tasks"})," (Advanced students):"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multi-room Navigation"}),': "Find and bring the cup from the kitchen"']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Contextual Manipulation"}),': "Put away the dirty dishes" (requires understanding)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Collaborative Task"}),': "Help me set the table" (interactive)']}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(n.p,{children:"Create a system architecture diagram showing:"}),"\n",(0,t.jsx)(n.mermaid,{value:"graph TB\r\n    subgraph Input Layer\r\n        A1[Microphone]\r\n        A2[RGB Camera]\r\n        A3[Depth Camera]\r\n    end\r\n    \r\n    subgraph Processing Layer\r\n        B1[Voice Command Node]\r\n        B2[Vision Node]\r\n        B3[Planning Node]\r\n        B4[Localization Node]\r\n    end\r\n    \r\n    subgraph Execution Layer\r\n        C1[Orchestrator Node]\r\n        C2[Navigation Controller]\r\n        C3[Manipulation Controller]\r\n    end\r\n    \r\n    subgraph Simulation\r\n        D1[Gazebo/Isaac Sim]\r\n        D2[Robot Model]\r\n    end\r\n    \r\n    A1 --\x3e B1\r\n    A2 --\x3e B2\r\n    A3 --\x3e B2\r\n    B1 --\x3e|/voice/command| B3\r\n    B2 --\x3e|/vision/detections| C1\r\n    B3 --\x3e|/robot/action_plan| C1\r\n    B4 --\x3e|/robot/pose| C1\r\n    C1 --\x3e C2\r\n    C1 --\x3e C3\r\n    C2 --\x3e D1\r\n    C3 --\x3e D1\r\n    D1 --\x3e D2\r\n    \r\n    style C1 fill:#FCE4EC\r\n    style D1 fill:#E8F5E9"}),"\n",(0,t.jsx)(n.h3,{id:"file-structure",children:"File Structure"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"vla_capstone/\r\n\u251c\u2500\u2500 launch/\r\n\u2502   \u251c\u2500\u2500 full_system.launch.py       # Main launch file\r\n\u2502   \u251c\u2500\u2500 simulation.launch.py        # Gazebo/Isaac Sim setup\r\n\u2502   \u2514\u2500\u2500 sensors.launch.py           # Camera, microphone\r\n\u251c\u2500\u2500 config/\r\n\u2502   \u251c\u2500\u2500 robot_config.yaml           # Robot parameters\r\n\u2502   \u251c\u2500\u2500 camera_intrinsics.yaml      # Calibration data\r\n\u2502   \u2514\u2500\u2500 task_definitions.yaml       # Task-specific configs\r\n\u251c\u2500\u2500 src/\r\n\u2502   \u251c\u2500\u2500 orchestrator_node.py        # Main coordination logic\r\n\u2502   \u251c\u2500\u2500 voice_command_node.py       # From sub-chapter 4.2\r\n\u2502   \u251c\u2500\u2500 planning_node.py            # From sub-chapter 4.3\r\n\u2502   \u251c\u2500\u2500 vision_node.py              # From sub-chapter 4.4\r\n\u2502   \u251c\u2500\u2500 navigation_client.py        # Nav2 action client\r\n\u2502   \u2514\u2500\u2500 manipulation_client.py      # MoveIt2 action client\r\n\u251c\u2500\u2500 tests/\r\n\u2502   \u251c\u2500\u2500 test_integration.py         # End-to-end tests\r\n\u2502   \u2514\u2500\u2500 test_individual_nodes.py    # Unit tests\r\n\u251c\u2500\u2500 docs/\r\n\u2502   \u251c\u2500\u2500 architecture.md\r\n\u2502   \u2514\u2500\u2500 user_guide.md\r\n\u2514\u2500\u2500 README.md\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"phase-2-integration-strategy",children:"Phase 2: Integration Strategy"}),"\n",(0,t.jsx)(n.h3,{id:"state-machine-design",children:"State Machine Design"}),"\n",(0,t.jsx)(n.p,{children:"The orchestrator node coordinates all components using a state machine:"}),"\n",(0,t.jsx)(n.mermaid,{value:"stateDiagram-v2\r\n    [*] --\x3e Idle\r\n    Idle --\x3e Listening: Start system\r\n    Listening --\x3e Planning: Voice command received\r\n    Planning --\x3e Scanning: Plan generated\r\n    Scanning --\x3e Navigating: Objects detected\r\n    Navigating --\x3e Manipulating: Reached target\r\n    Manipulating --\x3e Verifying: Action executed\r\n    Verifying --\x3e Scanning: More objects\r\n    Verifying --\x3e Reporting: Task complete\r\n    Reporting --\x3e Idle: Report sent\r\n    \r\n    Planning --\x3e Error: Plan invalid\r\n    Navigating --\x3e Error: Navigation failed\r\n    Manipulating --\x3e Error: Grasp failed\r\n    Error --\x3e Replanning: Retry\r\n    Replanning --\x3e Planning: New plan\r\n    Replanning --\x3e Idle: Max retries exceeded"}),"\n",(0,t.jsx)(n.h3,{id:"orchestrator-node-implementation",children:"Orchestrator Node Implementation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# orchestrator_node.py\r\n# Main coordinator for VLA system\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom rclpy.action import ActionClient\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import Point\r\nfrom enum import Enum\r\nimport json\r\n\r\nclass SystemState(Enum):\r\n    IDLE = 1\r\n    LISTENING = 2\r\n    PLANNING = 3\r\n    SCANNING = 4\r\n    NAVIGATING = 5\r\n    MANIPULATING = 6\r\n    VERIFYING = 7\r\n    REPORTING = 8\r\n    ERROR = 9\r\n\r\nclass OrchestratorNode(Node):\r\n    """\r\n    Main orchestrator coordinating voice, planning, vision, navigation, manipulation.\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'orchestrator_node\')\r\n        \r\n        # State machine\r\n        self.state = SystemState.IDLE\r\n        self.current_plan = None\r\n        self.current_action_index = 0\r\n        self.detected_objects = []\r\n        self.max_retries = 3\r\n        self.retry_count = 0\r\n        \r\n        # Subscribers\r\n        self.voice_sub = self.create_subscription(\r\n            String, \'/voice/command\', self.voice_callback, 10\r\n        )\r\n        self.plan_sub = self.create_subscription(\r\n            String, \'/robot/action_plan\', self.plan_callback, 10\r\n        )\r\n        self.vision_sub = self.create_subscription(\r\n            String, \'/vision/detections\', self.vision_callback, 10\r\n        )\r\n        \r\n        # Publishers\r\n        self.status_pub = self.create_publisher(String, \'/system/status\', 10)\r\n        self.query_pub = self.create_publisher(String, \'/vision/query\', 10)\r\n        \r\n        # Action clients (navigation, manipulation)\r\n        # self.nav_client = ActionClient(self, NavigateToPose, \'/navigate_to_pose\')\r\n        # self.grasp_client = ActionClient(self, GraspObject, \'/grasp_object\')\r\n        \r\n        # Timer for state machine execution\r\n        self.timer = self.create_timer(0.1, self.execute_state_machine)\r\n        \r\n        self.get_logger().info("Orchestrator ready. Awaiting voice command...")\r\n        self.transition_state(SystemState.LISTENING)\r\n    \r\n    def transition_state(self, new_state: SystemState):\r\n        """Transition to new state with logging"""\r\n        self.get_logger().info(f"State: {self.state.name} \u2192 {new_state.name}")\r\n        self.state = new_state\r\n        self.publish_status(f"State: {new_state.name}")\r\n    \r\n    def publish_status(self, message: str):\r\n        """Publish system status"""\r\n        msg = String()\r\n        msg.data = message\r\n        self.status_pub.publish(msg)\r\n    \r\n    def voice_callback(self, msg):\r\n        """Handle incoming voice command"""\r\n        if self.state == SystemState.LISTENING:\r\n            self.get_logger().info(f"Voice command: {msg.data}")\r\n            self.transition_state(SystemState.PLANNING)\r\n            # Planning node will receive this and generate plan\r\n    \r\n    def plan_callback(self, msg):\r\n        """Handle action plan from LLM planner"""\r\n        if self.state == SystemState.PLANNING:\r\n            try:\r\n                self.current_plan = json.loads(msg.data)\r\n                self.current_action_index = 0\r\n                self.get_logger().info(f"Plan received: {len(self.current_plan[\'actions\'])} actions")\r\n                self.transition_state(SystemState.SCANNING)\r\n            except json.JSONDecodeError as e:\r\n                self.get_logger().error(f"Invalid plan JSON: {e}")\r\n                self.transition_state(SystemState.ERROR)\r\n    \r\n    def vision_callback(self, msg):\r\n        """Handle object detections from vision"""\r\n        if self.state == SystemState.SCANNING:\r\n            try:\r\n                self.detected_objects = json.loads(msg.data)\r\n                self.get_logger().info(f"Detected {len(self.detected_objects)} objects")\r\n                if self.detected_objects:\r\n                    self.transition_state(SystemState.NAVIGATING)\r\n                else:\r\n                    self.get_logger().warn("No objects detected, retrying...")\r\n            except json.JSONDecodeError as e:\r\n                self.get_logger().error(f"Invalid detection JSON: {e}")\r\n    \r\n    def execute_state_machine(self):\r\n        """Main state machine execution loop"""\r\n        \r\n        if self.state == SystemState.IDLE:\r\n            pass  # Waiting for start signal\r\n        \r\n        elif self.state == SystemState.LISTENING:\r\n            pass  # Waiting for voice command\r\n        \r\n        elif self.state == SystemState.PLANNING:\r\n            pass  # Waiting for plan from LLM\r\n        \r\n        elif self.state == SystemState.SCANNING:\r\n            # Request vision to find target object\r\n            if self.current_plan and self.current_action_index < len(self.current_plan[\'actions\']):\r\n                action = self.current_plan[\'actions\'][self.current_action_index]\r\n                if action[\'action_type\'] == \'grasp\':\r\n                    # Query vision for this object\r\n                    query_msg = String()\r\n                    query_msg.data = action[\'target\']\r\n                    self.query_pub.publish(query_msg)\r\n        \r\n        elif self.state == SystemState.NAVIGATING:\r\n            # Send navigation goal\r\n            self.execute_navigation()\r\n        \r\n        elif self.state == SystemState.MANIPULATING:\r\n            # Send grasp goal\r\n            self.execute_manipulation()\r\n        \r\n        elif self.state == SystemState.VERIFYING:\r\n            # Check if task complete\r\n            self.current_action_index += 1\r\n            if self.current_action_index >= len(self.current_plan[\'actions\']):\r\n                self.transition_state(SystemState.REPORTING)\r\n            else:\r\n                self.transition_state(SystemState.SCANNING)\r\n        \r\n        elif self.state == SystemState.REPORTING:\r\n            self.publish_status("Task complete!")\r\n            self.transition_state(SystemState.IDLE)\r\n        \r\n        elif self.state == SystemState.ERROR:\r\n            self.retry_count += 1\r\n            if self.retry_count < self.max_retries:\r\n                self.get_logger().warn(f"Error occurred, retrying ({self.retry_count}/{self.max_retries})")\r\n                self.transition_state(SystemState.LISTENING)\r\n            else:\r\n                self.get_logger().error("Max retries exceeded, aborting task")\r\n                self.transition_state(SystemState.IDLE)\r\n    \r\n    def execute_navigation(self):\r\n        """Execute navigation action"""\r\n        # Simplified: In practice, use Nav2 action client\r\n        self.get_logger().info("Navigating to target...")\r\n        # Simulate navigation completion\r\n        self.transition_state(SystemState.MANIPULATING)\r\n    \r\n    def execute_manipulation(self):\r\n        """Execute manipulation action"""\r\n        # Simplified: In practice, use MoveIt2 action client\r\n        self.get_logger().info("Grasping object...")\r\n        # Simulate grasp completion\r\n        self.transition_state(SystemState.VERIFYING)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = OrchestratorNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"phase-3-fine-tuning-vla-on-custom-task",children:"Phase 3: Fine-tuning VLA on Custom Task"}),"\n",(0,t.jsx)(n.h3,{id:"why-fine-tune",children:"Why Fine-tune?"}),"\n",(0,t.jsx)(n.p,{children:"Pre-trained VLA models (like GR00T N1.5) are trained on diverse tasks but may not optimize for your specific robot/environment. Fine-tuning adapts the model to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Your robot's kinematics"})," (humanoid vs arm vs mobile manipulator)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Your objects"})," (specific cups, boxes, tools in your lab)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Your task success criteria"})," (precision requirements, speed constraints)"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"data-collection",children:"Data Collection"}),"\n",(0,t.jsx)(n.p,{children:"Collect 50-100 demonstrations of your task:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# collect_demonstrations.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, JointState\r\nimport h5py\r\nimport numpy as np\r\n\r\nclass DemonstrationCollector(Node):\r\n    \"\"\"Collect VLA training data: images + actions\"\"\"\r\n    \r\n    def __init__(self, output_file='demonstrations.h5'):\r\n        super().__init__('demo_collector')\r\n        \r\n        # Create HDF5 file\r\n        self.h5file = h5py.File(output_file, 'w')\r\n        self.demo_count = 0\r\n        self.current_demo = []\r\n        \r\n        # Subscribe to sensors and actions\r\n        self.image_sub = self.create_subscription(\r\n            Image, '/camera/color/image_raw', self.image_callback, 10\r\n        )\r\n        self.joint_sub = self.create_subscription(\r\n            JointState, '/joint_states', self.joint_callback, 10\r\n        )\r\n        \r\n        self.get_logger().info(\"Collecting demonstrations. Press SPACE to start/stop recording.\")\r\n    \r\n    def save_demonstration(self):\r\n        \"\"\"Save collected demo to HDF5\"\"\"\r\n        group = self.h5file.create_group(f'demo_{self.demo_count}')\r\n        \r\n        # Save images, actions, language commands\r\n        images = np.array([frame['image'] for frame in self.current_demo])\r\n        actions = np.array([frame['action'] for frame in self.current_demo])\r\n        \r\n        group.create_dataset('images', data=images)\r\n        group.create_dataset('actions', data=actions)\r\n        group.attrs['language_command'] = \"Pick up the cup\"\r\n        \r\n        self.demo_count += 1\r\n        self.current_demo = []\r\n        self.get_logger().info(f\"Saved demonstration {self.demo_count}\")\n"})}),"\n",(0,t.jsx)(n.h3,{id:"fine-tuning-script",children:"Fine-tuning Script"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# finetune_vla.py\r\nimport torch\r\nfrom transformers import AutoModelForVision2Seq, TrainingArguments, Trainer\r\nfrom datasets import load_dataset\r\n\r\n# Load pre-trained VLA model\r\nmodel = AutoModelForVision2Seq.from_pretrained("nvidia/gr00t-n1.5")\r\n\r\n# Load your demonstrations\r\ndataset = load_dataset("hdf5", data_files="demonstrations.h5")\r\n\r\n# Training configuration\r\ntraining_args = TrainingArguments(\r\n    output_dir="./vla_finetuned",\r\n    num_train_epochs=10,\r\n    per_device_train_batch_size=4,\r\n    learning_rate=1e-5,\r\n    save_steps=100,\r\n    logging_steps=10\r\n)\r\n\r\n# Fine-tune\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=dataset[\'train\']\r\n)\r\n\r\ntrainer.train()\r\nmodel.save_pretrained("./vla_finetuned_final")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"phase-4-deployment--testing",children:"Phase 4: Deployment & Testing"}),"\n",(0,t.jsx)(n.h3,{id:"complete-launch-file",children:"Complete Launch File"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# full_system.launch.py\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Simulation\r\n        Node(\r\n            package='gazebo_ros',\r\n            executable='spawn_entity.py',\r\n            arguments=['-entity', 'robot', '-file', 'robot.urdf']\r\n        ),\r\n        \r\n        # Sensors\r\n        Node(\r\n            package='usb_cam',\r\n            executable='usb_cam_node_exe',\r\n            name='camera'\r\n        ),\r\n        \r\n        # VLA Components\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='voice_command_node',\r\n            name='voice'\r\n        ),\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='planning_node',\r\n            name='planning'\r\n        ),\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='vision_node',\r\n            name='vision'\r\n        ),\r\n        Node(\r\n            package='vla_capstone',\r\n            executable='orchestrator_node',\r\n            name='orchestrator'\r\n        ),\r\n    ])\n"})}),"\n",(0,t.jsx)(n.h3,{id:"evaluation-harness",children:"Evaluation Harness"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# evaluate_system.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nimport time\r\nimport json\r\n\r\nclass EvaluationHarness(Node):\r\n    """Automated evaluation of VLA system"""\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'evaluator\')\r\n        \r\n        self.test_cases = [\r\n            {"command": "Pick up the red cup", "expected_objects": ["red cup"], "timeout": 60},\r\n            {"command": "Clean the table", "expected_objects": ["cup", "book", "toy"], "timeout": 120},\r\n        ]\r\n        \r\n        self.results = []\r\n    \r\n    def run_evaluation(self):\r\n        """Run all test cases and collect metrics"""\r\n        for i, test in enumerate(self.test_cases):\r\n            self.get_logger().info(f"Test {i+1}/{len(self.test_cases)}: {test[\'command\']}")\r\n            \r\n            start_time = time.time()\r\n            success = self.execute_test(test)\r\n            duration = time.time() - start_time\r\n            \r\n            self.results.append({\r\n                "test": test[\'command\'],\r\n                "success": success,\r\n                "duration": duration,\r\n                "timeout": test[\'timeout\']\r\n            })\r\n        \r\n        self.print_summary()\r\n    \r\n    def print_summary(self):\r\n        """Print evaluation results"""\r\n        total = len(self.results)\r\n        successful = sum(1 for r in self.results if r[\'success\'])\r\n        \r\n        print("\\n=== Evaluation Results ===")\r\n        print(f"Success Rate: {successful}/{total} ({100*successful/total:.1f}%)")\r\n        print(f"Avg Duration: {sum(r[\'duration\'] for r in self.results)/total:.1f}s")\r\n        \r\n        for r in self.results:\r\n            status = "\u2713" if r[\'success\'] else "\u2717"\r\n            print(f"{status} {r[\'test\']}: {r[\'duration\']:.1f}s")\n'})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"phase-5-troubleshooting--optimization",children:"Phase 5: Troubleshooting & Optimization"}),"\n",(0,t.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,t.jsx)(n.p,{children:"Track these metrics:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Metric"}),(0,t.jsx)(n.th,{children:"Target"}),(0,t.jsx)(n.th,{children:"Measurement"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Task Success Rate"})}),(0,t.jsx)(n.td,{children:">70%"}),(0,t.jsx)(n.td,{children:"Tasks completed / Tasks attempted"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Voice Recognition Accuracy"})}),(0,t.jsx)(n.td,{children:">90%"}),(0,t.jsx)(n.td,{children:"Correct transcriptions / Total commands"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Object Detection Precision"})}),(0,t.jsx)(n.td,{children:">80%"}),(0,t.jsx)(n.td,{children:"True positives / (True positives + False positives)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Navigation Success"})}),(0,t.jsx)(n.td,{children:">85%"}),(0,t.jsx)(n.td,{children:"Successful navigations / Navigation attempts"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Grasp Success"})}),(0,t.jsx)(n.td,{children:">75%"}),(0,t.jsx)(n.td,{children:"Successful grasps / Grasp attempts"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"End-to-End Latency"})}),(0,t.jsx)(n.td,{children:"<90s"}),(0,t.jsx)(n.td,{children:"Time from voice command to task completion"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"common-integration-issues",children:"Common Integration Issues"}),"\n",(0,t.jsx)(n.h4,{id:"issue-1-system-hangs-in-scanning-state",children:'Issue 1: "System hangs in SCANNING state"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptoms"}),": Orchestrator stuck waiting for vision detections"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Debug"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Check if vision node is publishing\r\nros2 topic hz /vision/detections\r\n\r\n# Check camera feed\r\nros2 run rqt_image_view rqt_image_view /camera/color/image_raw\r\n\r\n# Check vision query\r\nros2 topic echo /vision/query\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ensure vision node receives camera images"}),"\n",(0,t.jsx)(n.li,{children:"Verify CLIP model loaded correctly"}),"\n",(0,t.jsx)(n.li,{children:"Check confidence threshold (lower to 0.2 for testing)"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"issue-2-high-failure-rate-on-manipulation",children:'Issue 2: "High failure rate on manipulation"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptoms"}),": Grasp success <50%"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Improve 3D localization"}),": Use larger ROI for depth median filtering"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Add pre-grasp alignment"}),": Rotate gripper to match object orientation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement visual servoing"}),": Close-loop control using camera feedback"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"issue-3-latency-budget-exceeded",children:'Issue 3: "Latency budget exceeded"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptoms"}),": Tasks take >2 minutes for simple commands"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Optimization"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Profile each component\r\nimport time\r\n\r\nclass ProfilingOrchestrator(OrchestratorNode):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.timings = {}\r\n    \r\n    def transition_state(self, new_state):\r\n        if hasattr(self, 'state_start_time'):\r\n            duration = time.time() - self.state_start_time\r\n            self.timings[self.state.name] = duration\r\n            self.get_logger().info(f\"{self.state.name}: {duration:.2f}s\")\r\n        \r\n        self.state_start_time = time.time()\r\n        super().transition_state(new_state)\n"})}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"project-deliverables",children:"Project Deliverables"}),"\n",(0,t.jsx)(n.h3,{id:"required-submissions",children:"Required Submissions"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Demonstration Video"})," (2-3 minutes)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Show complete task execution from voice command to completion"}),"\n",(0,t.jsx)(n.li,{children:"Include at least 2 successful runs and 1 failure with recovery"}),"\n",(0,t.jsx)(n.li,{children:'Narrate key decision points ("Here the LLM planned 4 actions...", "Vision detected object at 0.8m...")'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Code Repository"})," (GitHub)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"All source code with README"}),"\n",(0,t.jsx)(n.li,{children:"Launch files and configuration"}),"\n",(0,t.jsx)(n.li,{children:"Installation instructions"}),"\n",(0,t.jsx)(n.li,{children:"Example commands to run system"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Performance Report"})," (2-4 pages)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Task success rate across 10 trials"}),"\n",(0,t.jsx)(n.li,{children:"Breakdown by subsystem (voice, planning, vision, navigation, manipulation)"}),"\n",(0,t.jsx)(n.li,{children:"Latency analysis"}),"\n",(0,t.jsx)(n.li,{children:"Failure mode analysis with examples"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Lessons Learned"})," (1 page)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Top 3 technical challenges and how you solved them"}),"\n",(0,t.jsx)(n.li,{children:"Unexpected failure modes discovered"}),"\n",(0,t.jsx)(n.li,{children:"Suggestions for future improvements"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Category"}),(0,t.jsx)(n.th,{children:"Weight"}),(0,t.jsx)(n.th,{children:"Criteria"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Code Quality"})}),(0,t.jsx)(n.td,{children:"30%"}),(0,t.jsx)(n.td,{children:"Clean, documented, follows ROS 2 conventions"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Integration"})}),(0,t.jsx)(n.td,{children:"25%"}),(0,t.jsx)(n.td,{children:"All components work together, state machine robust"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Performance"})}),(0,t.jsx)(n.td,{children:"25%"}),(0,t.jsx)(n.td,{children:"Meets >70% success rate, <90s latency"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Documentation"})}),(0,t.jsx)(n.td,{children:"20%"}),(0,t.jsx)(n.td,{children:"Clear README, architecture diagram, performance report"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Total"})}),(0,t.jsx)(n.td,{children:"100%"}),(0,t.jsx)(n.td,{})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Bonus Points"})," (+10% each):"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Deploy to real hardware (not just simulation)"}),"\n",(0,t.jsx)(n.li,{children:"Implement fine-tuning of VLA model"}),"\n",(0,t.jsx)(n.li,{children:"Add safety monitoring (collision avoidance, emergency stop)"}),"\n",(0,t.jsx)(n.li,{children:"Multi-task support (switch between tasks dynamically)"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-add-object-sorting-easy-2-hours",children:"Exercise 1: Add Object Sorting (Easy, 2 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Extend system to sort objects by color."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:'Define task: "Sort objects by color"'}),"\n",(0,t.jsx)(n.li,{children:"Modify vision node to detect object colors"}),"\n",(0,t.jsx)(n.li,{children:"Update LLM prompt with sorting logic"}),"\n",(0,t.jsx)(n.li,{children:"Test with 3 red and 3 blue objects"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-multi-robot-coordination-medium-4-hours",children:"Exercise 2: Multi-Robot Coordination (Medium, 4 hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Coordinate 2 robots on same task."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Launch 2 orchestrator instances"}),"\n",(0,t.jsx)(n.li,{children:"Implement task allocation (robot 1 gets objects 1-3, robot 2 gets 4-6)"}),"\n",(0,t.jsx)(n.li,{children:"Add collision avoidance between robots"}),"\n",(0,t.jsx)(n.li,{children:"Measure speedup vs single robot"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-real-hardware-deployment-hard-8-hours",children:"Exercise 3: Real Hardware Deployment (Hard, 8+ hours)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Deploy to physical robot."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Calibrate cameras on real robot"}),"\n",(0,t.jsx)(n.li,{children:"Handle real-world noise (lighting, occlusion, sensor drift)"}),"\n",(0,t.jsx)(n.li,{children:"Implement safety monitoring (force limits, workspace bounds)"}),"\n",(0,t.jsx)(n.li,{children:"Compare simulation vs real-world success rates"}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"})," (Google, 2023)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"https://arxiv.org/abs/2307.15818"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"GR00T: Generalist Robot Foundation Model"})," (NVIDIA, 2024)"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multimodal VLA architecture details"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"ROS 2 Best Practices"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html",children:"https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html"})}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"MoveIt2 Integration Guide"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.a,{href:"https://moveit.ros.org/",children:"https://moveit.ros.org/"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"case-studies",children:"Case Studies"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Physical Intelligence \u03c00: Dishwasher Task"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Real-world VLA deployment lessons"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Boston Dynamics Spot + VLA"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Vision-language control of quadruped robots"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsxs)(n.p,{children:["\ud83c\udf89 ",(0,t.jsx)(n.strong,{children:"Congratulations!"})," You've completed the VLA module and built a complete autonomous robot system."]}),"\n",(0,t.jsx)(n.p,{children:"You now have hands-on experience with:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"\u2705 Voice-controlled robotics using Whisper"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 LLM-based cognitive planning with function calling"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Computer vision integration for object detection"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 End-to-end VLA pipeline from language to action"}),"\n",(0,t.jsx)(n.li,{children:"\u2705 Real-world deployment and troubleshooting"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"whats-next",children:"What's Next?"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Extend your capstone"}),": Add new capabilities (tool use, collaborative tasks)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deploy to hardware"}),": Apply for lab access to physical robots"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Contribute to open source"}),": Share your VLA implementations with the community"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explore research"}),": Read latest VLA papers, propose improvements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Career opportunities"}),": Physical AI engineers are in high demand (Tesla, Boston Dynamics, NVIDIA, Figure AI)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Keep building intelligent robots!"})," \ud83e\udd16"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var s=r(6540);const t={},i=s.createContext(t);function o(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);