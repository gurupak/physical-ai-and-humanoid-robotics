"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[7808],{6027:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"chapter-4-vla/capstone-project","title":"4.5 Capstone Project: Autonomous Humanoid","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/05-capstone-project.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/capstone-project","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/05-capstone-project.md","tags":[{"inline":true,"label":"Capstone","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/capstone"},{"inline":true,"label":"Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/integration"},{"inline":true,"label":"End-to-End VLA","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/end-to-end-vla"},{"inline":true,"label":"Project Deliverables","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/project-deliverables"}],"version":"current","sidebarPosition":5,"frontMatter":{"id":"capstone-project","title":"4.5 Capstone Project: Autonomous Humanoid","sidebar_label":"Capstone Project","sidebar_position":5,"sidebar_custom_props":{"difficulty":"Advanced","readingTime":"25 minutes","projectTime":"4-6 hours"},"tags":["Capstone","Integration","End-to-End VLA","Project Deliverables"]},"sidebar":"tutorialSidebar","previous":{"title":"Vision Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/vision-integration"}}');var i=s(4848),r=s(8453);const o={id:"capstone-project",title:"4.5 Capstone Project: Autonomous Humanoid",sidebar_label:"Capstone Project",sidebar_position:5,sidebar_custom_props:{difficulty:"Advanced",readingTime:"25 minutes",projectTime:"4-6 hours"},tags:["Capstone","Integration","End-to-End VLA","Project Deliverables"]},a="4.5 Capstone Project: Autonomous Humanoid",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Project Overview",id:"project-overview",level:2},{value:"Goal",id:"goal",level:3},{value:"Components",id:"components",level:3},{value:"Timeline",id:"timeline",level:3},{value:"Phase 1: System Architecture Design",id:"phase-1-system-architecture-design",level:2},{value:"Define Your Task",id:"define-your-task",level:3},{value:"System Architecture",id:"system-architecture",level:3},{value:"File Structure",id:"file-structure",level:3},{value:"Phase 2: Integration Strategy",id:"phase-2-integration-strategy",level:2},{value:"State Machine Design",id:"state-machine-design",level:3},{value:"Orchestrator Node Implementation",id:"orchestrator-node-implementation",level:3},{value:"Phase 3: Fine-tuning VLA on Custom Task",id:"phase-3-fine-tuning-vla-on-custom-task",level:2},{value:"Why Fine-tune?",id:"why-fine-tune",level:3},{value:"Data Collection",id:"data-collection",level:3},{value:"Fine-tuning Script",id:"fine-tuning-script",level:3},{value:"Phase 4: Deployment &amp; Testing",id:"phase-4-deployment--testing",level:2},{value:"Complete Launch File",id:"complete-launch-file",level:3},{value:"Evaluation Harness",id:"evaluation-harness",level:3},{value:"Phase 5: Troubleshooting &amp; Optimization",id:"phase-5-troubleshooting--optimization",level:2},{value:"Performance Metrics",id:"performance-metrics",level:3},{value:"Common Integration Issues",id:"common-integration-issues",level:3},{value:"Issue 1: &quot;System hangs in SCANNING state&quot;",id:"issue-1-system-hangs-in-scanning-state",level:4},{value:"Issue 2: &quot;High failure rate on manipulation&quot;",id:"issue-2-high-failure-rate-on-manipulation",level:4},{value:"Issue 3: &quot;Latency budget exceeded&quot;",id:"issue-3-latency-budget-exceeded",level:4},{value:"Project Deliverables",id:"project-deliverables",level:2},{value:"Required Submissions",id:"required-submissions",level:3},{value:"Grading Rubric",id:"grading-rubric",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Add Object Sorting (Easy, 2 hours)",id:"exercise-1-add-object-sorting-easy-2-hours",level:3},{value:"Exercise 2: Multi-Robot Coordination (Medium, 4 hours)",id:"exercise-2-multi-robot-coordination-medium-4-hours",level:3},{value:"Exercise 3: Real Hardware Deployment (Hard, 8+ hours)",id:"exercise-3-real-hardware-deployment-hard-8-hours",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Case Studies",id:"case-studies",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"What&#39;s Next?",id:"whats-next",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"45-capstone-project-autonomous-humanoid",children:"4.5 Capstone Project: Autonomous Humanoid"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By completing this capstone project, you will be able to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate"})," voice, cognitive planning, vision, navigation, and manipulation into a complete VLA system"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fine-tune"})," pre-trained VLA models on custom robot tasks using demonstration data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deploy"})," end-to-end autonomous systems to simulated environments (Gazebo/Isaac Sim)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Evaluate"})," system performance using quantitative metrics (success rate, latency, accuracy)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Troubleshoot"})," complex integration issues across multiple subsystems"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.h3,{id:"goal",children:"Goal"}),"\n",(0,i.jsx)(n.p,{children:"Build a complete autonomous robot that receives a voice command, plans a task, navigates to objects, identifies them using computer vision, and manipulates them to accomplish the goal."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example Task"}),': "Clean the room"']}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Robot hears command via Whisper"}),"\n",(0,i.jsx)(n.li,{children:"LLM plans: [navigate to objects, identify items, grasp each, navigate to bin, place]"}),"\n",(0,i.jsx)(n.li,{children:"Vision identifies objects (cup, book, toy)"}),"\n",(0,i.jsx)(n.li,{children:"Robot executes manipulation sequence"}),"\n",(0,i.jsx)(n.li,{children:'Reports completion: "Task complete. 3 objects cleaned."'}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"components",children:"Components"}),"\n",(0,i.jsx)(n.mermaid,{value:"flowchart TD\n    A[Voice Input] --\x3e|Whisper| B[Text Command]\n    B --\x3e|LLM Planner| C[Task Plan]\n    D[Camera Feed] --\x3e|CLIP Vision| E[Object Detections]\n    C --\x3e|Action Sequence| F[Orchestrator Node]\n    E --\x3e|3D Positions| F\n    F --\x3e|Navigate| G[Nav2 Stack]\n    F --\x3e|Grasp| H[MoveIt2]\n    G --\x3e|Feedback| F\n    H --\x3e|Feedback| F\n    F --\x3e|Status| I[User Feedback]\n    \n    style A fill:#E3F2FD\n    style D fill:#E3F2FD\n    style C fill:#FFF3E0\n    style F fill:#FCE4EC\n    style I fill:#E8F5E9"}),"\n",(0,i.jsx)(n.h3,{id:"timeline",children:"Timeline"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Week 1 (4-6 hours)"}),": System architecture design, component integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Week 2 (6-8 hours)"}),": Testing, debugging, performance optimization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Week 3 (2-3 hours)"}),": Documentation, video recording, final submission"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Total"}),": 12-17 hours"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"phase-1-system-architecture-design",children:"Phase 1: System Architecture Design"}),"\n",(0,i.jsx)(n.h3,{id:"define-your-task",children:"Define Your Task"}),"\n",(0,i.jsx)(n.p,{children:"Choose a concrete task for your robot:"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Easy Tasks"})," (Recommended for beginners):"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Single Object Pickup"}),': "Pick up the cup"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Sorting"}),': "Move red objects to box A, blue objects to box B"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Table Clearing"}),': "Remove all objects from the table"']}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Medium Tasks"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Room Cleaning"}),': "Clean the room" (multiple object types, navigation required)']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Fetch and Deliver"}),': "Bring me the book from the shelf"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Arrangement"}),': "Arrange objects in a line"']}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hard Tasks"})," (Advanced students):"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-room Navigation"}),': "Find and bring the cup from the kitchen"']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Contextual Manipulation"}),': "Put away the dirty dishes" (requires understanding)']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Collaborative Task"}),': "Help me set the table" (interactive)']}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,i.jsx)(n.p,{children:"Create a system architecture diagram showing:"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph TB\n    subgraph Input Layer\n        A1[Microphone]\n        A2[RGB Camera]\n        A3[Depth Camera]\n    end\n    \n    subgraph Processing Layer\n        B1[Voice Command Node]\n        B2[Vision Node]\n        B3[Planning Node]\n        B4[Localization Node]\n    end\n    \n    subgraph Execution Layer\n        C1[Orchestrator Node]\n        C2[Navigation Controller]\n        C3[Manipulation Controller]\n    end\n    \n    subgraph Simulation\n        D1[Gazebo/Isaac Sim]\n        D2[Robot Model]\n    end\n    \n    A1 --\x3e B1\n    A2 --\x3e B2\n    A3 --\x3e B2\n    B1 --\x3e|/voice/command| B3\n    B2 --\x3e|/vision/detections| C1\n    B3 --\x3e|/robot/action_plan| C1\n    B4 --\x3e|/robot/pose| C1\n    C1 --\x3e C2\n    C1 --\x3e C3\n    C2 --\x3e D1\n    C3 --\x3e D1\n    D1 --\x3e D2\n    \n    style C1 fill:#FCE4EC\n    style D1 fill:#E8F5E9"}),"\n",(0,i.jsx)(n.h3,{id:"file-structure",children:"File Structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"vla_capstone/\n\u251c\u2500\u2500 launch/\n\u2502   \u251c\u2500\u2500 full_system.launch.py       # Main launch file\n\u2502   \u251c\u2500\u2500 simulation.launch.py        # Gazebo/Isaac Sim setup\n\u2502   \u2514\u2500\u2500 sensors.launch.py           # Camera, microphone\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 robot_config.yaml           # Robot parameters\n\u2502   \u251c\u2500\u2500 camera_intrinsics.yaml      # Calibration data\n\u2502   \u2514\u2500\u2500 task_definitions.yaml       # Task-specific configs\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 orchestrator_node.py        # Main coordination logic\n\u2502   \u251c\u2500\u2500 voice_command_node.py       # From sub-chapter 4.2\n\u2502   \u251c\u2500\u2500 planning_node.py            # From sub-chapter 4.3\n\u2502   \u251c\u2500\u2500 vision_node.py              # From sub-chapter 4.4\n\u2502   \u251c\u2500\u2500 navigation_client.py        # Nav2 action client\n\u2502   \u2514\u2500\u2500 manipulation_client.py      # MoveIt2 action client\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 test_integration.py         # End-to-end tests\n\u2502   \u2514\u2500\u2500 test_individual_nodes.py    # Unit tests\n\u251c\u2500\u2500 docs/\n\u2502   \u251c\u2500\u2500 architecture.md\n\u2502   \u2514\u2500\u2500 user_guide.md\n\u2514\u2500\u2500 README.md\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"phase-2-integration-strategy",children:"Phase 2: Integration Strategy"}),"\n",(0,i.jsx)(n.h3,{id:"state-machine-design",children:"State Machine Design"}),"\n",(0,i.jsx)(n.p,{children:"The orchestrator node coordinates all components using a state machine:"}),"\n",(0,i.jsx)(n.mermaid,{value:"stateDiagram-v2\n    [*] --\x3e Idle\n    Idle --\x3e Listening: Start system\n    Listening --\x3e Planning: Voice command received\n    Planning --\x3e Scanning: Plan generated\n    Scanning --\x3e Navigating: Objects detected\n    Navigating --\x3e Manipulating: Reached target\n    Manipulating --\x3e Verifying: Action executed\n    Verifying --\x3e Scanning: More objects\n    Verifying --\x3e Reporting: Task complete\n    Reporting --\x3e Idle: Report sent\n    \n    Planning --\x3e Error: Plan invalid\n    Navigating --\x3e Error: Navigation failed\n    Manipulating --\x3e Error: Grasp failed\n    Error --\x3e Replanning: Retry\n    Replanning --\x3e Planning: New plan\n    Replanning --\x3e Idle: Max retries exceeded"}),"\n",(0,i.jsx)(n.h3,{id:"orchestrator-node-implementation",children:"Orchestrator Node Implementation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# orchestrator_node.py\n# Main coordinator for VLA system\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Point\nfrom enum import Enum\nimport json\n\nclass SystemState(Enum):\n    IDLE = 1\n    LISTENING = 2\n    PLANNING = 3\n    SCANNING = 4\n    NAVIGATING = 5\n    MANIPULATING = 6\n    VERIFYING = 7\n    REPORTING = 8\n    ERROR = 9\n\nclass OrchestratorNode(Node):\n    """\n    Main orchestrator coordinating voice, planning, vision, navigation, manipulation.\n    """\n    \n    def __init__(self):\n        super().__init__(\'orchestrator_node\')\n        \n        # State machine\n        self.state = SystemState.IDLE\n        self.current_plan = None\n        self.current_action_index = 0\n        self.detected_objects = []\n        self.max_retries = 3\n        self.retry_count = 0\n        \n        # Subscribers\n        self.voice_sub = self.create_subscription(\n            String, \'/voice/command\', self.voice_callback, 10\n        )\n        self.plan_sub = self.create_subscription(\n            String, \'/robot/action_plan\', self.plan_callback, 10\n        )\n        self.vision_sub = self.create_subscription(\n            String, \'/vision/detections\', self.vision_callback, 10\n        )\n        \n        # Publishers\n        self.status_pub = self.create_publisher(String, \'/system/status\', 10)\n        self.query_pub = self.create_publisher(String, \'/vision/query\', 10)\n        \n        # Action clients (navigation, manipulation)\n        # self.nav_client = ActionClient(self, NavigateToPose, \'/navigate_to_pose\')\n        # self.grasp_client = ActionClient(self, GraspObject, \'/grasp_object\')\n        \n        # Timer for state machine execution\n        self.timer = self.create_timer(0.1, self.execute_state_machine)\n        \n        self.get_logger().info("Orchestrator ready. Awaiting voice command...")\n        self.transition_state(SystemState.LISTENING)\n    \n    def transition_state(self, new_state: SystemState):\n        """Transition to new state with logging"""\n        self.get_logger().info(f"State: {self.state.name} \u2192 {new_state.name}")\n        self.state = new_state\n        self.publish_status(f"State: {new_state.name}")\n    \n    def publish_status(self, message: str):\n        """Publish system status"""\n        msg = String()\n        msg.data = message\n        self.status_pub.publish(msg)\n    \n    def voice_callback(self, msg):\n        """Handle incoming voice command"""\n        if self.state == SystemState.LISTENING:\n            self.get_logger().info(f"Voice command: {msg.data}")\n            self.transition_state(SystemState.PLANNING)\n            # Planning node will receive this and generate plan\n    \n    def plan_callback(self, msg):\n        """Handle action plan from LLM planner"""\n        if self.state == SystemState.PLANNING:\n            try:\n                self.current_plan = json.loads(msg.data)\n                self.current_action_index = 0\n                self.get_logger().info(f"Plan received: {len(self.current_plan[\'actions\'])} actions")\n                self.transition_state(SystemState.SCANNING)\n            except json.JSONDecodeError as e:\n                self.get_logger().error(f"Invalid plan JSON: {e}")\n                self.transition_state(SystemState.ERROR)\n    \n    def vision_callback(self, msg):\n        """Handle object detections from vision"""\n        if self.state == SystemState.SCANNING:\n            try:\n                self.detected_objects = json.loads(msg.data)\n                self.get_logger().info(f"Detected {len(self.detected_objects)} objects")\n                if self.detected_objects:\n                    self.transition_state(SystemState.NAVIGATING)\n                else:\n                    self.get_logger().warn("No objects detected, retrying...")\n            except json.JSONDecodeError as e:\n                self.get_logger().error(f"Invalid detection JSON: {e}")\n    \n    def execute_state_machine(self):\n        """Main state machine execution loop"""\n        \n        if self.state == SystemState.IDLE:\n            pass  # Waiting for start signal\n        \n        elif self.state == SystemState.LISTENING:\n            pass  # Waiting for voice command\n        \n        elif self.state == SystemState.PLANNING:\n            pass  # Waiting for plan from LLM\n        \n        elif self.state == SystemState.SCANNING:\n            # Request vision to find target object\n            if self.current_plan and self.current_action_index < len(self.current_plan[\'actions\']):\n                action = self.current_plan[\'actions\'][self.current_action_index]\n                if action[\'action_type\'] == \'grasp\':\n                    # Query vision for this object\n                    query_msg = String()\n                    query_msg.data = action[\'target\']\n                    self.query_pub.publish(query_msg)\n        \n        elif self.state == SystemState.NAVIGATING:\n            # Send navigation goal\n            self.execute_navigation()\n        \n        elif self.state == SystemState.MANIPULATING:\n            # Send grasp goal\n            self.execute_manipulation()\n        \n        elif self.state == SystemState.VERIFYING:\n            # Check if task complete\n            self.current_action_index += 1\n            if self.current_action_index >= len(self.current_plan[\'actions\']):\n                self.transition_state(SystemState.REPORTING)\n            else:\n                self.transition_state(SystemState.SCANNING)\n        \n        elif self.state == SystemState.REPORTING:\n            self.publish_status("Task complete!")\n            self.transition_state(SystemState.IDLE)\n        \n        elif self.state == SystemState.ERROR:\n            self.retry_count += 1\n            if self.retry_count < self.max_retries:\n                self.get_logger().warn(f"Error occurred, retrying ({self.retry_count}/{self.max_retries})")\n                self.transition_state(SystemState.LISTENING)\n            else:\n                self.get_logger().error("Max retries exceeded, aborting task")\n                self.transition_state(SystemState.IDLE)\n    \n    def execute_navigation(self):\n        """Execute navigation action"""\n        # Simplified: In practice, use Nav2 action client\n        self.get_logger().info("Navigating to target...")\n        # Simulate navigation completion\n        self.transition_state(SystemState.MANIPULATING)\n    \n    def execute_manipulation(self):\n        """Execute manipulation action"""\n        # Simplified: In practice, use MoveIt2 action client\n        self.get_logger().info("Grasping object...")\n        # Simulate grasp completion\n        self.transition_state(SystemState.VERIFYING)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = OrchestratorNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"phase-3-fine-tuning-vla-on-custom-task",children:"Phase 3: Fine-tuning VLA on Custom Task"}),"\n",(0,i.jsx)(n.h3,{id:"why-fine-tune",children:"Why Fine-tune?"}),"\n",(0,i.jsx)(n.p,{children:"Pre-trained VLA models (like GR00T N1.5) are trained on diverse tasks but may not optimize for your specific robot/environment. Fine-tuning adapts the model to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Your robot's kinematics"})," (humanoid vs arm vs mobile manipulator)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Your objects"})," (specific cups, boxes, tools in your lab)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Your task success criteria"})," (precision requirements, speed constraints)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-collection",children:"Data Collection"}),"\n",(0,i.jsx)(n.p,{children:"Collect 50-100 demonstrations of your task:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# collect_demonstrations.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, JointState\nimport h5py\nimport numpy as np\n\nclass DemonstrationCollector(Node):\n    \"\"\"Collect VLA training data: images + actions\"\"\"\n    \n    def __init__(self, output_file='demonstrations.h5'):\n        super().__init__('demo_collector')\n        \n        # Create HDF5 file\n        self.h5file = h5py.File(output_file, 'w')\n        self.demo_count = 0\n        self.current_demo = []\n        \n        # Subscribe to sensors and actions\n        self.image_sub = self.create_subscription(\n            Image, '/camera/color/image_raw', self.image_callback, 10\n        )\n        self.joint_sub = self.create_subscription(\n            JointState, '/joint_states', self.joint_callback, 10\n        )\n        \n        self.get_logger().info(\"Collecting demonstrations. Press SPACE to start/stop recording.\")\n    \n    def save_demonstration(self):\n        \"\"\"Save collected demo to HDF5\"\"\"\n        group = self.h5file.create_group(f'demo_{self.demo_count}')\n        \n        # Save images, actions, language commands\n        images = np.array([frame['image'] for frame in self.current_demo])\n        actions = np.array([frame['action'] for frame in self.current_demo])\n        \n        group.create_dataset('images', data=images)\n        group.create_dataset('actions', data=actions)\n        group.attrs['language_command'] = \"Pick up the cup\"\n        \n        self.demo_count += 1\n        self.current_demo = []\n        self.get_logger().info(f\"Saved demonstration {self.demo_count}\")\n"})}),"\n",(0,i.jsx)(n.h3,{id:"fine-tuning-script",children:"Fine-tuning Script"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# finetune_vla.py\nimport torch\nfrom transformers import AutoModelForVision2Seq, TrainingArguments, Trainer\nfrom datasets import load_dataset\n\n# Load pre-trained VLA model\nmodel = AutoModelForVision2Seq.from_pretrained("nvidia/gr00t-n1.5")\n\n# Load your demonstrations\ndataset = load_dataset("hdf5", data_files="demonstrations.h5")\n\n# Training configuration\ntraining_args = TrainingArguments(\n    output_dir="./vla_finetuned",\n    num_train_epochs=10,\n    per_device_train_batch_size=4,\n    learning_rate=1e-5,\n    save_steps=100,\n    logging_steps=10\n)\n\n# Fine-tune\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\'train\']\n)\n\ntrainer.train()\nmodel.save_pretrained("./vla_finetuned_final")\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"phase-4-deployment--testing",children:"Phase 4: Deployment & Testing"}),"\n",(0,i.jsx)(n.h3,{id:"complete-launch-file",children:"Complete Launch File"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# full_system.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Simulation\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=['-entity', 'robot', '-file', 'robot.urdf']\n        ),\n        \n        # Sensors\n        Node(\n            package='usb_cam',\n            executable='usb_cam_node_exe',\n            name='camera'\n        ),\n        \n        # VLA Components\n        Node(\n            package='vla_capstone',\n            executable='voice_command_node',\n            name='voice'\n        ),\n        Node(\n            package='vla_capstone',\n            executable='planning_node',\n            name='planning'\n        ),\n        Node(\n            package='vla_capstone',\n            executable='vision_node',\n            name='vision'\n        ),\n        Node(\n            package='vla_capstone',\n            executable='orchestrator_node',\n            name='orchestrator'\n        ),\n    ])\n"})}),"\n",(0,i.jsx)(n.h3,{id:"evaluation-harness",children:"Evaluation Harness"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# evaluate_system.py\nimport rclpy\nfrom rclpy.node import Node\nimport time\nimport json\n\nclass EvaluationHarness(Node):\n    """Automated evaluation of VLA system"""\n    \n    def __init__(self):\n        super().__init__(\'evaluator\')\n        \n        self.test_cases = [\n            {"command": "Pick up the red cup", "expected_objects": ["red cup"], "timeout": 60},\n            {"command": "Clean the table", "expected_objects": ["cup", "book", "toy"], "timeout": 120},\n        ]\n        \n        self.results = []\n    \n    def run_evaluation(self):\n        """Run all test cases and collect metrics"""\n        for i, test in enumerate(self.test_cases):\n            self.get_logger().info(f"Test {i+1}/{len(self.test_cases)}: {test[\'command\']}")\n            \n            start_time = time.time()\n            success = self.execute_test(test)\n            duration = time.time() - start_time\n            \n            self.results.append({\n                "test": test[\'command\'],\n                "success": success,\n                "duration": duration,\n                "timeout": test[\'timeout\']\n            })\n        \n        self.print_summary()\n    \n    def print_summary(self):\n        """Print evaluation results"""\n        total = len(self.results)\n        successful = sum(1 for r in self.results if r[\'success\'])\n        \n        print("\\n=== Evaluation Results ===")\n        print(f"Success Rate: {successful}/{total} ({100*successful/total:.1f}%)")\n        print(f"Avg Duration: {sum(r[\'duration\'] for r in self.results)/total:.1f}s")\n        \n        for r in self.results:\n            status = "\u2713" if r[\'success\'] else "\u2717"\n            print(f"{status} {r[\'test\']}: {r[\'duration\']:.1f}s")\n'})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"phase-5-troubleshooting--optimization",children:"Phase 5: Troubleshooting & Optimization"}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics",children:"Performance Metrics"}),"\n",(0,i.jsx)(n.p,{children:"Track these metrics:"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Metric"}),(0,i.jsx)(n.th,{children:"Target"}),(0,i.jsx)(n.th,{children:"Measurement"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Task Success Rate"})}),(0,i.jsx)(n.td,{children:">70%"}),(0,i.jsx)(n.td,{children:"Tasks completed / Tasks attempted"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Voice Recognition Accuracy"})}),(0,i.jsx)(n.td,{children:">90%"}),(0,i.jsx)(n.td,{children:"Correct transcriptions / Total commands"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Object Detection Precision"})}),(0,i.jsx)(n.td,{children:">80%"}),(0,i.jsx)(n.td,{children:"True positives / (True positives + False positives)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Navigation Success"})}),(0,i.jsx)(n.td,{children:">85%"}),(0,i.jsx)(n.td,{children:"Successful navigations / Navigation attempts"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Grasp Success"})}),(0,i.jsx)(n.td,{children:">75%"}),(0,i.jsx)(n.td,{children:"Successful grasps / Grasp attempts"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"End-to-End Latency"})}),(0,i.jsx)(n.td,{children:"<90s"}),(0,i.jsx)(n.td,{children:"Time from voice command to task completion"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"common-integration-issues",children:"Common Integration Issues"}),"\n",(0,i.jsx)(n.h4,{id:"issue-1-system-hangs-in-scanning-state",children:'Issue 1: "System hangs in SCANNING state"'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": Orchestrator stuck waiting for vision detections"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Debug"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check if vision node is publishing\nros2 topic hz /vision/detections\n\n# Check camera feed\nros2 run rqt_image_view rqt_image_view /camera/color/image_raw\n\n# Check vision query\nros2 topic echo /vision/query\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ensure vision node receives camera images"}),"\n",(0,i.jsx)(n.li,{children:"Verify CLIP model loaded correctly"}),"\n",(0,i.jsx)(n.li,{children:"Check confidence threshold (lower to 0.2 for testing)"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"issue-2-high-failure-rate-on-manipulation",children:'Issue 2: "High failure rate on manipulation"'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": Grasp success <50%"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Improve 3D localization"}),": Use larger ROI for depth median filtering"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add pre-grasp alignment"}),": Rotate gripper to match object orientation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Implement visual servoing"}),": Close-loop control using camera feedback"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"issue-3-latency-budget-exceeded",children:'Issue 3: "Latency budget exceeded"'}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": Tasks take >2 minutes for simple commands"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Optimization"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# Profile each component\nimport time\n\nclass ProfilingOrchestrator(OrchestratorNode):\n    def __init__(self):\n        super().__init__()\n        self.timings = {}\n    \n    def transition_state(self, new_state):\n        if hasattr(self, 'state_start_time'):\n            duration = time.time() - self.state_start_time\n            self.timings[self.state.name] = duration\n            self.get_logger().info(f\"{self.state.name}: {duration:.2f}s\")\n        \n        self.state_start_time = time.time()\n        super().transition_state(new_state)\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"project-deliverables",children:"Project Deliverables"}),"\n",(0,i.jsx)(n.h3,{id:"required-submissions",children:"Required Submissions"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Demonstration Video"})," (2-3 minutes)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Show complete task execution from voice command to completion"}),"\n",(0,i.jsx)(n.li,{children:"Include at least 2 successful runs and 1 failure with recovery"}),"\n",(0,i.jsx)(n.li,{children:'Narrate key decision points ("Here the LLM planned 4 actions...", "Vision detected object at 0.8m...")'}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Code Repository"})," (GitHub)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"All source code with README"}),"\n",(0,i.jsx)(n.li,{children:"Launch files and configuration"}),"\n",(0,i.jsx)(n.li,{children:"Installation instructions"}),"\n",(0,i.jsx)(n.li,{children:"Example commands to run system"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Performance Report"})," (2-4 pages)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Task success rate across 10 trials"}),"\n",(0,i.jsx)(n.li,{children:"Breakdown by subsystem (voice, planning, vision, navigation, manipulation)"}),"\n",(0,i.jsx)(n.li,{children:"Latency analysis"}),"\n",(0,i.jsx)(n.li,{children:"Failure mode analysis with examples"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Lessons Learned"})," (1 page)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Top 3 technical challenges and how you solved them"}),"\n",(0,i.jsx)(n.li,{children:"Unexpected failure modes discovered"}),"\n",(0,i.jsx)(n.li,{children:"Suggestions for future improvements"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"grading-rubric",children:"Grading Rubric"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Category"}),(0,i.jsx)(n.th,{children:"Weight"}),(0,i.jsx)(n.th,{children:"Criteria"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Code Quality"})}),(0,i.jsx)(n.td,{children:"30%"}),(0,i.jsx)(n.td,{children:"Clean, documented, follows ROS 2 conventions"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Integration"})}),(0,i.jsx)(n.td,{children:"25%"}),(0,i.jsx)(n.td,{children:"All components work together, state machine robust"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Performance"})}),(0,i.jsx)(n.td,{children:"25%"}),(0,i.jsx)(n.td,{children:"Meets >70% success rate, <90s latency"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Documentation"})}),(0,i.jsx)(n.td,{children:"20%"}),(0,i.jsx)(n.td,{children:"Clear README, architecture diagram, performance report"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Total"})}),(0,i.jsx)(n.td,{children:"100%"}),(0,i.jsx)(n.td,{})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Bonus Points"})," (+10% each):"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Deploy to real hardware (not just simulation)"}),"\n",(0,i.jsx)(n.li,{children:"Implement fine-tuning of VLA model"}),"\n",(0,i.jsx)(n.li,{children:"Add safety monitoring (collision avoidance, emergency stop)"}),"\n",(0,i.jsx)(n.li,{children:"Multi-task support (switch between tasks dynamically)"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-add-object-sorting-easy-2-hours",children:"Exercise 1: Add Object Sorting (Easy, 2 hours)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Goal"}),": Extend system to sort objects by color."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'Define task: "Sort objects by color"'}),"\n",(0,i.jsx)(n.li,{children:"Modify vision node to detect object colors"}),"\n",(0,i.jsx)(n.li,{children:"Update LLM prompt with sorting logic"}),"\n",(0,i.jsx)(n.li,{children:"Test with 3 red and 3 blue objects"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-multi-robot-coordination-medium-4-hours",children:"Exercise 2: Multi-Robot Coordination (Medium, 4 hours)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Goal"}),": Coordinate 2 robots on same task."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Launch 2 orchestrator instances"}),"\n",(0,i.jsx)(n.li,{children:"Implement task allocation (robot 1 gets objects 1-3, robot 2 gets 4-6)"}),"\n",(0,i.jsx)(n.li,{children:"Add collision avoidance between robots"}),"\n",(0,i.jsx)(n.li,{children:"Measure speedup vs single robot"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-real-hardware-deployment-hard-8-hours",children:"Exercise 3: Real Hardware Deployment (Hard, 8+ hours)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Goal"}),": Deploy to physical robot."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Calibrate cameras on real robot"}),"\n",(0,i.jsx)(n.li,{children:"Handle real-world noise (lighting, occlusion, sensor drift)"}),"\n",(0,i.jsx)(n.li,{children:"Implement safety monitoring (force limits, workspace bounds)"}),"\n",(0,i.jsx)(n.li,{children:"Compare simulation vs real-world success rates"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"})," (Google, 2023)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2307.15818",children:"https://arxiv.org/abs/2307.15818"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"GR00T: Generalist Robot Foundation Model"})," (NVIDIA, 2024)"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multimodal VLA architecture details"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"ROS 2 Best Practices"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html",children:"https://docs.ros.org/en/humble/The-ROS2-Project/Contributing/Developer-Guide.html"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"MoveIt2 Integration Guide"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://moveit.ros.org/",children:"https://moveit.ros.org/"})}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"case-studies",children:"Case Studies"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Physical Intelligence \u03c00: Dishwasher Task"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Real-world VLA deployment lessons"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Boston Dynamics Spot + VLA"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Vision-language control of quadruped robots"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udf89 ",(0,i.jsx)(n.strong,{children:"Congratulations!"})," You've completed the VLA module and built a complete autonomous robot system."]}),"\n",(0,i.jsx)(n.p,{children:"You now have hands-on experience with:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Voice-controlled robotics using Whisper"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 LLM-based cognitive planning with function calling"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Computer vision integration for object detection"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 End-to-end VLA pipeline from language to action"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Real-world deployment and troubleshooting"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"whats-next",children:"What's Next?"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Extend your capstone"}),": Add new capabilities (tool use, collaborative tasks)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Deploy to hardware"}),": Apply for lab access to physical robots"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Contribute to open source"}),": Share your VLA implementations with the community"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explore research"}),": Read latest VLA papers, propose improvements"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Career opportunities"}),": Physical AI engineers are in high demand (Tesla, Boston Dynamics, NVIDIA, Figure AI)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Keep building intelligent robots!"})," \ud83e\udd16"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var t=s(6540);const i={},r=t.createContext(i);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);