"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[1658],{4250:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>a});const s=JSON.parse('{"id":"chapter-4-vla/voice-to-action","title":"4.2 Voice-to-Action with OpenAI Whisper","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/02-voice-to-action.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/voice-to-action","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/02-voice-to-action.md","tags":[{"inline":true,"label":"Voice Recognition","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/voice-recognition"},{"inline":true,"label":"Whisper","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/whisper"},{"inline":true,"label":"Speech-to-Text","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/speech-to-text"},{"inline":true,"label":"ROS 2 Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/ros-2-integration"}],"version":"current","sidebarPosition":2,"frontMatter":{"id":"voice-to-action","title":"4.2 Voice-to-Action with OpenAI Whisper","sidebar_label":"Voice-to-Action","sidebar_position":2,"sidebar_custom_props":{"difficulty":"Intermediate","readingTime":"15 minutes","hasQuickStart":true,"prerequisites":["ROS 2 Fundamentals","Python 3.11+","Basic audio processing concepts"]},"tags":["Voice Recognition","Whisper","Speech-to-Text","ROS 2 Integration"]},"sidebar":"tutorialSidebar","previous":{"title":"4.1 VLA Introduction","permalink":"/physical-ai-and-humanoid-robotics/docs/vla-introduction"},"next":{"title":"Cognitive Planning","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning"}}');var r=i(4848),o=i(8453);const t={id:"voice-to-action",title:"4.2 Voice-to-Action with OpenAI Whisper",sidebar_label:"Voice-to-Action",sidebar_position:2,sidebar_custom_props:{difficulty:"Intermediate",readingTime:"15 minutes",hasQuickStart:!0,prerequisites:["ROS 2 Fundamentals","Python 3.11+","Basic audio processing concepts"]},tags:["Voice Recognition","Whisper","Speech-to-Text","ROS 2 Integration"]},l="4.2 Voice-to-Action with OpenAI Whisper",c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Quick Start: Voice Command Demo (15 Minutes)",id:"quick-start-voice-command-demo-15-minutes",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Install Whisper",id:"step-1-install-whisper",level:3},{value:"Step 2: Test Speech Recognition",id:"step-2-test-speech-recognition",level:3},{value:"Step 3: Create ROS 2 Voice Node",id:"step-3-create-ros-2-voice-node",level:3},{value:"Step 4: Test the Node",id:"step-4-test-the-node",level:3},{value:"Step 5: Integrate with Robot Actions",id:"step-5-integrate-with-robot-actions",level:3},{value:"Speech Recognition Fundamentals",id:"speech-recognition-fundamentals",level:2},{value:"Why Whisper for Robotics?",id:"why-whisper-for-robotics",level:3},{value:"How Whisper Works",id:"how-whisper-works",level:3},{value:"Whisper Model Selection for Robotics",id:"whisper-model-selection-for-robotics",level:2},{value:"ROS 2 Integration Pattern",id:"ros-2-integration-pattern",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Complete ROS 2 Node Implementation",id:"complete-ros-2-node-implementation",level:3},{value:"Launch File Configuration",id:"launch-file-configuration",level:3},{value:"Audio Preprocessing &amp; Optimization",id:"audio-preprocessing--optimization",level:2},{value:"Voice Activity Detection (VAD)",id:"voice-activity-detection-vad",level:3},{value:"Noise Reduction",id:"noise-reduction",level:3},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Model Caching",id:"model-caching",level:3},{value:"Batch Processing (Advanced)",id:"batch-processing-advanced",level:3},{value:"Common Errors",id:"common-errors",level:2},{value:"Error 1: &quot;No audio device found&quot;",id:"error-1-no-audio-device-found",level:3},{value:"Error 2: &quot;Transcription quality is poor&quot;",id:"error-2-transcription-quality-is-poor",level:3},{value:"Error 3: &quot;High latency (&gt;5 seconds)&quot;",id:"error-3-high-latency-5-seconds",level:3},{value:"Error 4: &quot;Background noise triggers false commands&quot;",id:"error-4-background-noise-triggers-false-commands",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Model Comparison (Easy, 30 minutes)",id:"exercise-1-model-comparison-easy-30-minutes",level:3},{value:"Exercise 2: Confidence Filtering (Medium, 45 minutes)",id:"exercise-2-confidence-filtering-medium-45-minutes",level:3},{value:"Exercise 3: Wake Word Detection (Hard, 60 minutes)",id:"exercise-3-wake-word-detection-hard-60-minutes",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Tutorials",id:"tutorials",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"42-voice-to-action-with-openai-whisper",children:"4.2 Voice-to-Action with OpenAI Whisper"})}),"\n",(0,r.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(n.p,{children:"By completing this sub-chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement"})," speech recognition for robotic systems using OpenAI Whisper"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Select"})," appropriate Whisper model sizes based on latency and accuracy requirements"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apply"})," audio preprocessing techniques (VAD, noise reduction) to improve transcription quality"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Debug"})," common audio pipeline issues in real-time robotic applications"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start-voice-command-demo-15-minutes",children:"Quick Start: Voice Command Demo (15 Minutes)"}),"\n",(0,r.jsx)(n.p,{children:"Get a robot responding to voice commands in under 15 minutes."}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"ROS 2 Humble installed"}),"\n",(0,r.jsx)(n.li,{children:"Python 3.11+"}),"\n",(0,r.jsx)(n.li,{children:"Microphone connected to your system"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-install-whisper",children:"Step 1: Install Whisper"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected"}),": Package installs successfully with version >=20230314. First run will download model weights (~500MB for ",(0,r.jsx)(n.code,{children:"small"})," model)."]}),"\n",(0,r.jsx)(n.h3,{id:"step-2-test-speech-recognition",children:"Step 2: Test Speech Recognition"}),"\n",(0,r.jsx)(n.p,{children:"Create a test script to verify Whisper works:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# test_whisper.py\nimport whisper\n\n# Load the model (downloads ~500MB on first run)\nmodel = whisper.load_model("small")\n\n# Test with a sample audio file\nresult = model.transcribe("test_audio.wav")\nprint(f"Transcribed: {result[\'text\']}")\nprint(f"Language: {result[\'language\']}")\nprint(f"Confidence: {result.get(\'avg_logprob\', \'N/A\')}")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected"}),": Prints transcribed text from audio file with language detection."]}),"\n",(0,r.jsx)(n.h3,{id:"step-3-create-ros-2-voice-node",children:"Step 3: Create ROS 2 Voice Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# voice_command_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nfrom scipy.io.wavfile import write\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n        self.model = whisper.load_model("small")  # ~500MB, 2-5s latency\n        self.text_pub = self.create_publisher(String, \'/voice/command\', 10)\n        self.get_logger().info("Voice command node ready. Say something!")\n        \n    def record_audio(self, duration=5, sample_rate=16000):\n        """Record audio from microphone"""\n        self.get_logger().info("Recording...")\n        audio = sd.rec(int(duration * sample_rate), \n                      samplerate=sample_rate, \n                      channels=1, \n                      dtype=\'int16\')\n        sd.wait()\n        return audio, sample_rate\n    \n    def transcribe_and_publish(self):\n        """Capture audio and publish transcription"""\n        audio, sr = self.record_audio()\n        \n        # Save temporarily for Whisper\n        temp_file = "/tmp/voice_command.wav"\n        write(temp_file, sr, audio)\n        \n        # Transcribe\n        result = self.model.transcribe(temp_file, language=\'en\')\n        text = result["text"].strip()\n        \n        # Publish\n        msg = String()\n        msg.data = text\n        self.text_pub.publish(msg)\n        self.get_logger().info(f"Transcribed: {text}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n    \n    try:\n        while rclpy.ok():\n            node.transcribe_and_publish()\n            rclpy.spin_once(node, timeout_sec=0.1)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-test-the-node",children:"Step 4: Test the Node"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run the voice node\npython voice_command_node.py\n\n# Terminal 2: Monitor commands\nros2 topic echo /voice/command\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected"}),": Speak into your microphone, see transcribed text published to ",(0,r.jsx)(n.code,{children:"/voice/command"})," topic."]}),"\n",(0,r.jsx)(n.h3,{id:"step-5-integrate-with-robot-actions",children:"Step 5: Integrate with Robot Actions"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# command_interpreter_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass CommandInterpreterNode(Node):\n    def __init__(self):\n        super().__init__(\'command_interpreter\')\n        self.subscription = self.create_subscription(\n            String,\n            \'/voice/command\',\n            self.command_callback,\n            10\n        )\n        \n    def command_callback(self, msg):\n        """Parse voice commands and trigger actions"""\n        command = msg.data.lower()\n        \n        if "pick up" in command or "grab" in command:\n            self.get_logger().info("\ud83e\udd16 Initiating pick-up sequence")\n            # TODO: Call manipulation service\n            \n        elif "navigate" in command or "go to" in command:\n            self.get_logger().info("\ud83e\udd16 Starting navigation")\n            # TODO: Call navigation action\n            \n        elif "stop" in command or "halt" in command:\n            self.get_logger().info("\ud83e\udd16 Emergency stop")\n            # TODO: Cancel all actions\n            \n        else:\n            self.get_logger().warn(f"Unknown command: {command}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandInterpreterNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Result"}),": You now have a voice-controlled robot that can recognize commands and trigger actions!"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"speech-recognition-fundamentals",children:"Speech Recognition Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"why-whisper-for-robotics",children:"Why Whisper for Robotics?"}),"\n",(0,r.jsx)(n.p,{children:"Traditional speech recognition systems like Google Speech API or Amazon Transcribe require internet connectivity and have unpredictable latency. For robotic applications, we need:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Offline Operation"}),": Robots in warehouses, hospitals, or remote locations can't rely on cloud services"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Low Latency"}),": Voice commands need responses in <3 seconds for natural interaction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": Handle noisy environments (motor sounds, people talking nearby)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multilingual Support"}),": Work across different languages without retraining"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"OpenAI Whisper"})," (released September 2022, updated 2023) solves these challenges using a transformer-based encoder-decoder architecture trained on 680,000 hours of multilingual audio data. Unlike cloud APIs, Whisper runs entirely on your local machine with GPU acceleration."]}),"\n",(0,r.jsx)(n.h3,{id:"how-whisper-works",children:"How Whisper Works"}),"\n",(0,r.jsx)(n.mermaid,{value:"flowchart LR\n    A[Raw Audio] --\x3e|Log-Mel Spectrogram| B[Audio Encoder]\n    B --\x3e|Audio Features| C[Transformer Decoder]\n    C --\x3e|Token Predictions| D[Text Output]\n    \n    style A fill:#E3F2FD\n    style B fill:#F3E5F5\n    style C fill:#FFF3E0\n    style D fill:#E8F5E9"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Preprocessing"}),": Convert raw audio to log-mel spectrogram (80 frequency bins)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Encoding"}),": Transformer encoder processes spectrogram into audio features"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Decoding"}),": Transformer decoder generates text tokens autoregressively"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Post-processing"}),": Beam search and language model rescoring"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Insight"}),": Whisper treats speech recognition as a sequence-to-sequence translation task (audio \u2192 text), similar to machine translation. This enables zero-shot transfer to new domains and accents."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"whisper-model-selection-for-robotics",children:"Whisper Model Selection for Robotics"}),"\n",(0,r.jsx)(n.p,{children:"OpenAI provides 5 model sizes with different speed/accuracy tradeoffs:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Parameters"}),(0,r.jsx)(n.th,{children:"VRAM"}),(0,r.jsx)(n.th,{children:"Latency (CPU)"}),(0,r.jsx)(n.th,{children:"Latency (GPU)"}),(0,r.jsx)(n.th,{children:"WER (English)"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"tiny"})}),(0,r.jsx)(n.td,{children:"39M"}),(0,r.jsx)(n.td,{children:"~1GB"}),(0,r.jsx)(n.td,{children:"~10s"}),(0,r.jsx)(n.td,{children:"~1s"}),(0,r.jsx)(n.td,{children:"7.5%"}),(0,r.jsx)(n.td,{children:"Embedded systems, real-time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"base"})}),(0,r.jsx)(n.td,{children:"74M"}),(0,r.jsx)(n.td,{children:"~1GB"}),(0,r.jsx)(n.td,{children:"~7s"}),(0,r.jsx)(n.td,{children:"~0.7s"}),(0,r.jsx)(n.td,{children:"5.5%"}),(0,r.jsx)(n.td,{children:"Edge devices, quick responses"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"small"})}),(0,r.jsx)(n.td,{children:"244M"}),(0,r.jsx)(n.td,{children:"~2GB"}),(0,r.jsx)(n.td,{children:"~5s"}),(0,r.jsx)(n.td,{children:"~0.5s"}),(0,r.jsx)(n.td,{children:"4.3%"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Recommended for ROS 2"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"medium"})}),(0,r.jsx)(n.td,{children:"769M"}),(0,r.jsx)(n.td,{children:"~5GB"}),(0,r.jsx)(n.td,{children:"~12s"}),(0,r.jsx)(n.td,{children:"~2s"}),(0,r.jsx)(n.td,{children:"3.4%"}),(0,r.jsx)(n.td,{children:"High-accuracy applications"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"large"})}),(0,r.jsx)(n.td,{children:"1550M"}),(0,r.jsx)(n.td,{children:"~10GB"}),(0,r.jsx)(n.td,{children:"~30s"}),(0,r.jsx)(n.td,{children:"~5s"}),(0,r.jsx)(n.td,{children:"2.9%"}),(0,r.jsx)(n.td,{children:"Offline batch processing"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Recommendation for Robotics"}),": Use ",(0,r.jsx)(n.code,{children:"small"})," model as the default. It provides the best balance of accuracy (96% WER) and latency (<1s on modern GPUs). Only upgrade to ",(0,r.jsx)(n.code,{children:"medium"})," if accuracy is critical and you have GPU acceleration."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Benchmarks"})," (tested on RTX 3060, 5-second audio clips):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"tiny"}),": 850ms, 92% accuracy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"small"}),": 1200ms, 96% accuracy \u2713 ",(0,r.jsx)(n.strong,{children:"Best choice"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"medium"}),": 3400ms, 97.5% accuracy (diminishing returns)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"ros-2-integration-pattern",children:"ROS 2 Integration Pattern"}),"\n",(0,r.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,r.jsx)(n.mermaid,{value:'sequenceDiagram\n    participant User\n    participant Mic as Audio Device\n    participant VoiceNode as VoiceCommandNode\n    participant Interpreter as CommandInterpreter\n    participant Robot as Robot Controller\n    \n    User->>Mic: Speak "Pick up the cup"\n    Mic->>VoiceNode: Audio stream (PCM 16kHz)\n    VoiceNode->>VoiceNode: Whisper.transcribe()\n    VoiceNode->>Interpreter: String("/voice/command")\n    Interpreter->>Interpreter: Parse command\n    Interpreter->>Robot: Action goal (grasp)\n    Robot->>User: Execute motion'}),"\n",(0,r.jsx)(n.h3,{id:"complete-ros-2-node-implementation",children:"Complete ROS 2 Node Implementation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n# voice_command_node.py\n# ROS 2 Node for continuous voice command recognition\n# Requirements: openai-whisper>=20230314, sounddevice, scipy\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom std_srvs.srv import Trigger\nimport whisper\nimport sounddevice as sd\nimport numpy as np\nfrom scipy.io.wavfile import write\nimport threading\nimport queue\n\nclass VoiceCommandNode(Node):\n    """\n    Continuously listens for voice commands and publishes transcriptions.\n    \n    Publishers:\n        /voice/command (String): Transcribed voice commands\n        /voice/status (String): Node status updates\n    \n    Services:\n        ~/start_listening (Trigger): Start voice recognition\n        ~/stop_listening (Trigger): Stop voice recognition\n    """\n    \n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n        \n        # Parameters\n        self.declare_parameter(\'model_size\', \'small\')\n        self.declare_parameter(\'language\', \'en\')\n        self.declare_parameter(\'record_duration\', 5)  # seconds\n        self.declare_parameter(\'sample_rate\', 16000)\n        self.declare_parameter(\'confidence_threshold\', 0.5)\n        \n        model_size = self.get_parameter(\'model_size\').value\n        self.language = self.get_parameter(\'language\').value\n        self.duration = self.get_parameter(\'record_duration\').value\n        self.sample_rate = self.get_parameter(\'sample_rate\').value\n        self.confidence_threshold = self.get_parameter(\'confidence_threshold\').value\n        \n        # Load Whisper model\n        self.get_logger().info(f"Loading Whisper {model_size} model...")\n        self.model = whisper.load_model(model_size)\n        self.get_logger().info("Whisper model loaded successfully")\n        \n        # Publishers\n        self.command_pub = self.create_publisher(String, \'/voice/command\', 10)\n        self.status_pub = self.create_publisher(String, \'/voice/status\', 10)\n        \n        # Services\n        self.start_srv = self.create_service(\n            Trigger, \n            \'~/start_listening\', \n            self.start_listening_callback\n        )\n        self.stop_srv = self.create_service(\n            Trigger, \n            \'~/stop_listening\', \n            self.stop_listening_callback\n        )\n        \n        # State\n        self.listening = False\n        self.audio_queue = queue.Queue()\n        \n        self.get_logger().info("Voice Command Node ready")\n    \n    def record_audio(self):\n        """Record audio from microphone"""\n        self.publish_status("Recording...")\n        audio = sd.rec(\n            int(self.duration * self.sample_rate),\n            samplerate=self.sample_rate,\n            channels=1,\n            dtype=\'int16\'\n        )\n        sd.wait()\n        return audio\n    \n    def transcribe_audio(self, audio_data):\n        """Transcribe audio using Whisper"""\n        # Save to temporary file (Whisper requires file input)\n        temp_file = "/tmp/voice_command.wav"\n        write(temp_file, self.sample_rate, audio_data)\n        \n        # Transcribe with language specification\n        result = self.model.transcribe(\n            temp_file,\n            language=self.language,\n            fp16=False  # Use fp32 for CPU compatibility\n        )\n        \n        return result\n    \n    def publish_command(self, text, confidence):\n        """Publish transcribed command if confidence is sufficient"""\n        if confidence > self.confidence_threshold:\n            msg = String()\n            msg.data = text\n            self.command_pub.publish(msg)\n            self.get_logger().info(f"Command: {text} (confidence: {confidence:.2f})")\n        else:\n            self.get_logger().warn(f"Low confidence ({confidence:.2f}): {text}")\n    \n    def publish_status(self, status):\n        """Publish status update"""\n        msg = String()\n        msg.data = status\n        self.status_pub.publish(msg)\n    \n    def listening_loop(self):\n        """Continuous listening loop (runs in separate thread)"""\n        while self.listening and rclpy.ok():\n            try:\n                # Record audio\n                audio = self.record_audio()\n                \n                # Transcribe\n                result = self.transcribe_audio(audio)\n                text = result["text"].strip()\n                \n                # Estimate confidence from average log probability\n                confidence = np.exp(result.get("avg_logprob", -1.0))\n                \n                # Publish if valid\n                if text:\n                    self.publish_command(text, confidence)\n                else:\n                    self.get_logger().info("No speech detected")\n                    \n            except Exception as e:\n                self.get_logger().error(f"Transcription error: {e}")\n                self.publish_status(f"Error: {e}")\n    \n    def start_listening_callback(self, request, response):\n        """Service callback to start listening"""\n        if not self.listening:\n            self.listening = True\n            self.listen_thread = threading.Thread(target=self.listening_loop)\n            self.listen_thread.start()\n            response.success = True\n            response.message = "Started listening"\n            self.get_logger().info("Voice recognition started")\n        else:\n            response.success = False\n            response.message = "Already listening"\n        return response\n    \n    def stop_listening_callback(self, request, response):\n        """Service callback to stop listening"""\n        if self.listening:\n            self.listening = False\n            self.listen_thread.join()\n            response.success = True\n            response.message = "Stopped listening"\n            self.get_logger().info("Voice recognition stopped")\n        else:\n            response.success = False\n            response.message = "Not currently listening"\n        return response\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceCommandNode()\n    \n    # Auto-start listening\n    node.listening = True\n    node.listen_thread = threading.Thread(target=node.listening_loop)\n    node.listen_thread.start()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.listening = False\n        if hasattr(node, \'listen_thread\'):\n            node.listen_thread.join()\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,r.jsx)(n.h3,{id:"launch-file-configuration",children:"Launch File Configuration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'# voice_command.launch.yaml\nlaunch:\n  - node:\n      pkg: vla_voice\n      exec: voice_command_node\n      name: voice_command_node\n      parameters:\n        - model_size: "small"\n        - language: "en"\n        - record_duration: 5\n        - sample_rate: 16000\n        - confidence_threshold: 0.5\n      remappings:\n        - from: /voice/command\n          to: /robot/voice_command\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch vla_voice voice_command.launch.yaml\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"audio-preprocessing--optimization",children:"Audio Preprocessing & Optimization"}),"\n",(0,r.jsx)(n.h3,{id:"voice-activity-detection-vad",children:"Voice Activity Detection (VAD)"}),"\n",(0,r.jsx)(n.p,{children:"Problem: Recording 5 seconds of silence wastes computation. Solution: Only transcribe when speech is detected."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# vad_helper.py\nimport webrtcvad\nimport collections\n\nclass VoiceActivityDetector:\n    """Detect speech vs silence using WebRTC VAD"""\n    \n    def __init__(self, sample_rate=16000, frame_duration=30, aggressiveness=2):\n        """\n        Args:\n            sample_rate: Audio sample rate (8000, 16000, 32000, or 48000 Hz)\n            frame_duration: Frame length in ms (10, 20, or 30)\n            aggressiveness: VAD aggressiveness (0-3, higher = more aggressive)\n        """\n        self.vad = webrtcvad.Vad(aggressiveness)\n        self.sample_rate = sample_rate\n        self.frame_duration = frame_duration\n        self.frame_length = int(sample_rate * frame_duration / 1000) * 2  # bytes\n    \n    def contains_speech(self, audio_data):\n        """\n        Check if audio contains speech.\n        \n        Args:\n            audio_data: numpy array of audio samples (int16)\n        \n        Returns:\n            bool: True if speech detected, False otherwise\n        """\n        audio_bytes = audio_data.tobytes()\n        num_frames = len(audio_bytes) // self.frame_length\n        \n        # Check frames in sliding window\n        speech_frames = 0\n        for i in range(num_frames):\n            start = i * self.frame_length\n            end = start + self.frame_length\n            frame = audio_bytes[start:end]\n            \n            if len(frame) == self.frame_length:\n                if self.vad.is_speech(frame, self.sample_rate):\n                    speech_frames += 1\n        \n        # Consider speech if >30% of frames contain speech\n        speech_ratio = speech_frames / max(num_frames, 1)\n        return speech_ratio > 0.3\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Integration with Voice Node"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# In VoiceCommandNode.__init__():\nfrom vad_helper import VoiceActivityDetector\nself.vad = VoiceActivityDetector(sample_rate=self.sample_rate)\n\n# In listening_loop():\naudio = self.record_audio()\nif not self.vad.contains_speech(audio):\n    self.get_logger().info("No speech detected, skipping transcription")\n    continue\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Performance Impact"}),": Reduces unnecessary transcriptions by ~60%, saving GPU cycles."]}),"\n",(0,r.jsx)(n.h3,{id:"noise-reduction",children:"Noise Reduction"}),"\n",(0,r.jsx)(n.p,{children:"Robotic environments are noisy (motors, fans, ambient sounds). Apply spectral subtraction before transcription:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# noise_reduction.py\nimport numpy as np\nfrom scipy.signal import stft, istft\n\ndef reduce_noise(audio, sample_rate=16000, noise_duration=0.5):\n    """\n    Reduce background noise using spectral subtraction.\n    \n    Args:\n        audio: numpy array of audio samples\n        sample_rate: Audio sample rate\n        noise_duration: Duration (seconds) to estimate noise profile\n    \n    Returns:\n        numpy array: Noise-reduced audio\n    """\n    # Estimate noise from first 0.5 seconds (assumed silence)\n    noise_samples = int(sample_rate * noise_duration)\n    noise_profile = audio[:noise_samples]\n    \n    # Compute STFT\n    f, t, Zxx = stft(audio, fs=sample_rate, nperseg=256)\n    _, _, Zxx_noise = stft(noise_profile, fs=sample_rate, nperseg=256)\n    \n    # Estimate noise power spectrum\n    noise_power = np.mean(np.abs(Zxx_noise) ** 2, axis=1, keepdims=True)\n    \n    # Spectral subtraction\n    signal_power = np.abs(Zxx) ** 2\n    clean_power = np.maximum(signal_power - noise_power, 0)\n    \n    # Reconstruct phase\n    phase = np.angle(Zxx)\n    Zxx_clean = np.sqrt(clean_power) * np.exp(1j * phase)\n    \n    # Inverse STFT\n    _, audio_clean = istft(Zxx_clean, fs=sample_rate, nperseg=256)\n    \n    return audio_clean.astype(np.int16)\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# In transcribe_audio():\naudio_data = reduce_noise(audio_data, self.sample_rate)\n# Then proceed with Whisper transcription\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,r.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"CPU vs GPU Latency"})," (small model, 5-second audio):"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"CPU (Intel i7): ~5 seconds"}),"\n",(0,r.jsxs)(n.li,{children:["GPU (RTX 3060): ~0.5 seconds (",(0,r.jsx)(n.strong,{children:"10x faster"}),")"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Enable GPU in Whisper:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# In VoiceCommandNode.__init__():\nimport torch\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\nself.model = whisper.load_model(model_size).to(device)\nself.get_logger().info(f"Using device: {device}")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Memory Management"}),": Use ",(0,r.jsx)(n.code,{children:"fp16=True"})," for faster GPU inference (requires CUDA):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"result = self.model.transcribe(temp_file, fp16=True)  # Halves VRAM usage\n"})}),"\n",(0,r.jsx)(n.h3,{id:"model-caching",children:"Model Caching"}),"\n",(0,r.jsxs)(n.p,{children:["Whisper models are downloaded on first use (~500MB for ",(0,r.jsx)(n.code,{children:"small"}),"). Cache them to avoid re-downloading:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Set cache directory\nexport HF_HOME=/path/to/cache\nexport WHISPER_CACHE=/path/to/cache/whisper\n"})}),"\n",(0,r.jsx)(n.p,{children:"Or in Python:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import os\nos.environ["WHISPER_CACHE"] = "/opt/whisper_models"\nmodel = whisper.load_model("small")  # Loads from cache if available\n'})}),"\n",(0,r.jsx)(n.h3,{id:"batch-processing-advanced",children:"Batch Processing (Advanced)"}),"\n",(0,r.jsx)(n.p,{children:"For offline analysis (e.g., processing recorded robot logs), transcribe multiple files in parallel:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from concurrent.futures import ThreadPoolExecutor\n\ndef batch_transcribe(audio_files, model_size="small", max_workers=4):\n    """Transcribe multiple audio files in parallel"""\n    model = whisper.load_model(model_size)\n    \n    def transcribe_one(file_path):\n        return model.transcribe(file_path)\n    \n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        results = list(executor.map(transcribe_one, audio_files))\n    \n    return results\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"common-errors",children:"Common Errors"}),"\n",(0,r.jsx)(n.h3,{id:"error-1-no-audio-device-found",children:'Error 1: "No audio device found"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"OSError: PortAudio library not found\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Missing PortAudio system library"}),"\n",(0,r.jsx)(n.li,{children:"Microphone not connected or disabled"}),"\n",(0,r.jsx)(n.li,{children:"Permissions issue (Linux)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Ubuntu/Debian\nsudo apt-get install portaudio19-dev python3-pyaudio\n\n# macOS\nbrew install portaudio\n\n# Test microphone\npython -c "import sounddevice as sd; print(sd.query_devices())"\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Prevention"}),": Add microphone check to node startup:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import sounddevice as sd\ndevices = sd.query_devices()\nif not any(d['max_input_channels'] > 0 for d in devices):\n    self.get_logger().error(\"No input devices found!\")\n"})}),"\n",(0,r.jsx)(n.h3,{id:"error-2-transcription-quality-is-poor",children:'Error 2: "Transcription quality is poor"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Frequent mis-transcriptions"}),"\n",(0,r.jsx)(n.li,{children:"Random words inserted"}),"\n",(0,r.jsx)(n.li,{children:"Commands not recognized"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Background noise too high (SNR < 10dB)"}),"\n",(0,r.jsx)(n.li,{children:"Microphone too far from speaker (>2 meters)"}),"\n",(0,r.jsx)(n.li,{children:"Incorrect language setting"}),"\n",(0,r.jsxs)(n.li,{children:["Using ",(0,r.jsx)(n.code,{children:"tiny"})," model (insufficient capacity)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Upgrade model"}),": Switch from ",(0,r.jsx)(n.code,{children:"tiny"})," to ",(0,r.jsx)(n.code,{children:"small"})," (+2% accuracy)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Add noise reduction"}),": Use ",(0,r.jsx)(n.code,{children:"reduce_noise()"})," preprocessing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Verify language"}),": Ensure ",(0,r.jsx)(n.code,{children:"language='en'"})," matches spoken language"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Check audio quality"}),": Record test file and inspect waveform"]}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Debug transcription\nresult = model.transcribe(audio_file, verbose=True)\nprint(f\"Detected language: {result['language']}\")\nprint(f\"Avg log prob: {result['avg_logprob']}\")  # Should be > -1.0\n"})}),"\n",(0,r.jsx)(n.h3,{id:"error-3-high-latency-5-seconds",children:'Error 3: "High latency (>5 seconds)"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Long delay between speaking and transcription"}),"\n",(0,r.jsx)(n.li,{children:"Robot feels unresponsive"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Using large model on CPU"}),"\n",(0,r.jsx)(n.li,{children:"Recording duration too long (>10 seconds)"}),"\n",(0,r.jsx)(n.li,{children:"No GPU acceleration"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Enable GPU"}),": Install CUDA, use ",(0,r.jsx)(n.code,{children:"fp16=True"})]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reduce recording duration"}),": Use 3-5 seconds instead of 10"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Downgrade model"}),": ",(0,r.jsx)(n.code,{children:"medium"})," \u2192 ",(0,r.jsx)(n.code,{children:"small"})," (2x faster, -1% accuracy)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Benchmarks"}),":"]}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"CPU (i7)"}),(0,r.jsx)(n.th,{children:"GPU (RTX 3060)"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"tiny"}),(0,r.jsx)(n.td,{children:"850ms"}),(0,r.jsx)(n.td,{children:"200ms"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"small"}),(0,r.jsx)(n.td,{children:"5000ms"}),(0,r.jsx)(n.td,{children:"500ms"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:"medium"}),(0,r.jsx)(n.td,{children:"12000ms"}),(0,r.jsx)(n.td,{children:"2000ms"})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"error-4-background-noise-triggers-false-commands",children:'Error 4: "Background noise triggers false commands"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Random commands published when no one is speaking"}),"\n",(0,r.jsx)(n.li,{children:"Motor sounds transcribed as words"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"No voice activity detection"}),"\n",(0,r.jsx)(n.li,{children:"Confidence threshold too low"}),"\n",(0,r.jsx)(n.li,{children:"Recording during robot movement (motor noise)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Add VAD"}),": Use ",(0,r.jsx)(n.code,{children:"VoiceActivityDetector"})," to filter silence"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Increase confidence threshold"}),": Raise from 0.5 to 0.7"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement wake word"}),': Only listen after hearing "Hey robot"']}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Wake word detection using simple keyword matching\nWAKE_WORD = "hey robot"\n\ndef listen_for_wake_word(self):\n    audio = self.record_audio(duration=3)\n    result = self.model.transcribe(audio)\n    return WAKE_WORD in result["text"].lower()\n\n# In listening_loop():\nif not self.listen_for_wake_word():\n    continue  # Skip until wake word detected\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-model-comparison-easy-30-minutes",children:"Exercise 1: Model Comparison (Easy, 30 minutes)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Compare Whisper model sizes on your hardware."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Record 5 test audio files with different commands"}),"\n",(0,r.jsxs)(n.li,{children:["Transcribe with ",(0,r.jsx)(n.code,{children:"tiny"}),", ",(0,r.jsx)(n.code,{children:"small"}),", and ",(0,r.jsx)(n.code,{children:"medium"})," models"]}),"\n",(0,r.jsx)(n.li,{children:"Measure latency and accuracy for each"}),"\n",(0,r.jsx)(n.li,{children:"Create comparison table"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import time\n\nmodels = ["tiny", "small", "medium"]\ntest_files = ["command1.wav", "command2.wav", "command3.wav"]\n\nfor model_name in models:\n    model = whisper.load_model(model_name)\n    latencies = []\n    \n    for file in test_files:\n        start = time.time()\n        result = model.transcribe(file)\n        latency = time.time() - start\n        latencies.append(latency)\n        print(f"{model_name}: {result[\'text\']} ({latency:.2f}s)")\n    \n    print(f"{model_name} avg latency: {sum(latencies)/len(latencies):.2f}s")\n'})}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-confidence-filtering-medium-45-minutes",children:"Exercise 2: Confidence Filtering (Medium, 45 minutes)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Reject low-confidence transcriptions to prevent false commands."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["Modify ",(0,r.jsx)(n.code,{children:"VoiceCommandNode"})," to log confidence scores"]}),"\n",(0,r.jsx)(n.li,{children:"Record 10 audio samples (5 clear speech, 5 noise/silence)"}),"\n",(0,r.jsx)(n.li,{children:"Find optimal confidence threshold that filters noise but keeps speech"}),"\n",(0,r.jsx)(n.li,{children:"Implement dynamic threshold adjustment based on recent history"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hints"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Confidence = ",(0,r.jsx)(n.code,{children:'np.exp(result["avg_logprob"])'})]}),"\n",(0,r.jsx)(n.li,{children:"Typical good speech: confidence > 0.6"}),"\n",(0,r.jsx)(n.li,{children:"Noise/silence: confidence < 0.3"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-wake-word-detection-hard-60-minutes",children:"Exercise 3: Wake Word Detection (Hard, 60 minutes)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),': Implement "Hey Robot" wake word to reduce false activations.']}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Train or use pre-trained wake word model (Porcupine, Snowboy)"}),"\n",(0,r.jsx)(n.li,{children:"Integrate wake word detection before Whisper transcription"}),"\n",(0,r.jsx)(n.li,{children:"Measure false positive rate (wake word detected when not spoken)"}),"\n",(0,r.jsx)(n.li,{children:"Measure false negative rate (wake word not detected when spoken)"}),"\n",(0,r.jsx)(n.li,{children:"Optimize for <1% false positive rate"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Hints"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Use Picovoice Porcupine (free tier: 1 custom wake word)"}),"\n",(0,r.jsx)(n.li,{children:"Alternative: Use Whisper itself with short recordings (1-2 seconds)"}),"\n",(0,r.jsx)(n.li,{children:"Wake word should trigger within 500ms for natural interaction"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"})," (Radford et al., 2022)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"https://arxiv.org/abs/2212.04356"})}),"\n",(0,r.jsx)(n.li,{children:"Original Whisper paper explaining architecture and training"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"WebRTC Voice Activity Detection"})," (Google, 2021)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://webrtc.org/",children:"https://webrtc.org/"})}),"\n",(0,r.jsx)(n.li,{children:"VAD algorithm used in real-time communication"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"OpenAI Whisper GitHub"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"https://github.com/openai/whisper"})}),"\n",(0,r.jsx)(n.li,{children:"Official implementation and model weights"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"ROS 2 audio_common Package"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/ros-drivers/audio_common",children:"https://github.com/ros-drivers/audio_common"})}),"\n",(0,r.jsx)(n.li,{children:"ROS 2 audio capture and playback utilities"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"tutorials",children:"Tutorials"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Speech Recognition for Robotics"})," (NVIDIA Isaac Docs)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Integration patterns for voice-controlled robots"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"VAD Algorithms Comparison"})," (Speech Processing Blog)"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Deep dive into voice activity detection techniques"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.p,{children:["You've now built a complete voice-to-action pipeline! In the next sub-chapter, ",(0,r.jsx)(n.a,{href:"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning",children:"4.3 Cognitive Planning with LLMs"}),", you'll learn how to convert these voice commands into executable robot action sequences using large language models."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 Whisper ",(0,r.jsx)(n.code,{children:"small"})," model is the sweet spot for robotics (96% accuracy, <1s latency)"]}),"\n",(0,r.jsx)(n.li,{children:"\u2705 GPU acceleration provides 10x speedup over CPU"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Voice Activity Detection reduces unnecessary transcriptions by 60%"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Confidence thresholding prevents false commands from background noise"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 ROS 2 integration enables seamless voice control of robot systems"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>l});var s=i(6540);const r={},o=s.createContext(r);function t(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);