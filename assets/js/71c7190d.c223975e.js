"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[4167],{7753:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"chapter-3-isaac-ai-brain/synthetic-data-overview","title":"Synthetic Data Generation with Isaac Sim","description":"Synthetic data generation is Isaac Sim\'s superpower - it can create unlimited, perfectly labeled training data faster than you can annotate real-world datasets.","source":"@site/docs/chapter-3-isaac-ai-brain/synthetic-data-overview.md","sourceDirName":"chapter-3-isaac-ai-brain","slug":"/chapter-3-isaac-ai-brain/synthetic-data-overview","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-3-isaac-ai-brain/synthetic-data-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-3-isaac-ai-brain/synthetic-data-overview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim: Synthetic Data Generation","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-3-isaac-ai-brain/isaac-sim-synthetic-data"},"next":{"title":"Scene Creation Tutorial","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-3-isaac-ai-brain/scene-creation"}}');var t=a(4848),i=a(8453);const o={},s="Synthetic Data Generation with Isaac Sim",c={},d=[{value:"Core Advantages of Synthetic Data",id:"core-advantages-of-synthetic-data",level:2},{value:"Perfect Labeling",id:"perfect-labeling",level:3},{value:"Isaac Sim Synthetic Data Pipeline",id:"isaac-sim-synthetic-data-pipeline",level:2},{value:"Implementation Walkthrough",id:"implementation-walkthrough",level:2},{value:"Step 1: Configure Multi-Pass Rendering",id:"step-1-configure-multi-pass-rendering",level:3},{value:"Step 2: Define Randomization Domains",id:"step-2-define-randomization-domains",level:3},{value:"Step 3: Configure Data Writers",id:"step-3-configure-data-writers",level:3},{value:"Domain Randomization Advanced Techniques",id:"domain-randomization-advanced-techniques",level:2},{value:"Camera Dynamics",id:"camera-dynamics",level:3},{value:"Humanoid-Specific Considerations",id:"humanoid-specific-considerations",level:2},{value:"Tracking Humanoids in Various Poses",id:"tracking-humanoids-in-various-poses",level:3},{value:"Environmental Difficulty Scaling",id:"environmental-difficulty-scaling",level:3},{value:"Dataset Validation",id:"dataset-validation",level:2},{value:"Automatic Quality Checks",id:"automatic-quality-checks",level:3},{value:"Practical Example: Hour-Long Dataset Generation",id:"practical-example-hour-long-dataset-generation",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"synthetic-data-generation-with-isaac-sim",children:"Synthetic Data Generation with Isaac Sim"})}),"\n",(0,t.jsx)(n.p,{children:"Synthetic data generation is Isaac Sim's superpower - it can create unlimited, perfectly labeled training data faster than you can annotate real-world datasets."}),"\n",(0,t.jsx)(n.h2,{id:"core-advantages-of-synthetic-data",children:"Core Advantages of Synthetic Data"}),"\n",(0,t.jsx)(n.h3,{id:"perfect-labeling",children:"Perfect Labeling"}),"\n",(0,t.jsx)(n.p,{children:"Unlike real-world data collection, synthetic data includes:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"100% accurate ground truth"})," for every pixel"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Infinite variations"})," of the same basic scene"]}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Customizable environmental conditions"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"No privacy concerns"})," in data gathering"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"isaac-sim-synthetic-data-pipeline",children:"Isaac Sim Synthetic Data Pipeline"}),"\n",(0,t.jsx)(n.mermaid,{value:"graph LR\r\n    A[Scene Definition] --\x3e B[Parameter Randomization]\r\n    B --\x3e C[Render Passes]\r\n    C --\x3e D[Annotation Generation]\r\n    D --\x3e E[Dataset Export]\r\n\r\n    C --\x3e C1[RGB Image]\r\n    C --\x3e C2[Depth Map]\r\n    C --\x3e C3[Normal Map]\r\n    C --\x3e C4[Semantic/Instance]\r\n\r\n    D --\x3e D1[COCO Format]\r\n    D --\x3e D2[Pascal VOC]\r\n    D --\x3e D3[Custom XML]"}),"\n",(0,t.jsx)(n.h2,{id:"implementation-walkthrough",children:"Implementation Walkthrough"}),"\n",(0,t.jsx)(n.h3,{id:"step-1-configure-multi-pass-rendering",children:"Step 1: Configure Multi-Pass Rendering"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="synthetic_data_setup.py"',children:'import omni.replicator.core as rep\r\nimport carb\r\n\r\n# Create render products with different outputs\r\nwith rep.new_layer():\r\n    # Scene camera\r\n    camera = rep.create.camera(position=(2, 1.6, 2), look_at=(0, 0, 0))\r\n\r\n    # High-frequency render product for detail\r\n    rgb_product = rep.create.render_product(camera, (1920, 1080), name="rgb")\r\n    depth_product = rep.create.render_product(camera, (1920, 1080), name="depth")\r\n\r\n    # Class and instance segmentation\r\n    semantic_product = rep.create.render_product(camera, (1920, 1080), name="semantic")\r\n    instance_product = rep.create.render_product(camera, (1920, 1080), name="instance")\r\n\r\n    # Surface normals\r\n    normal_product = rep.create.render_product(camera, (1920, 1080), name="normal")\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-define-randomization-domains",children:"Step 2: Define Randomization Domains"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="domain_randomization.py"',children:'# Generate variations automatically\r\ndef create_scene_variations():\r\n    with camera:\r\n        # Camera pose variations\r\n        rep.randomizer.pose_camera_around_target(\r\n            target="/World/warehouse_shelves",\r\n            distance_range=(1.0, 5.0),\r\n            yaw_range=(-180, 180),\r\n            pitch_range=(-30, 30),\r\n            count=1000\r\n        )\r\n\r\n        # Lighting variations\r\n        rep.randomizer.light_orientation(\r\n            "/World/DistantLight",\r\n            rotation_range=(-45, 45)\r\n        )\r\n\r\n        # Material randomization\r\n        materials = rep.get.prims(["/World/pallet", "/World/table"])\r\n        rep.randomizer.material(materials, materials=None)\r\n\r\n        # Object visibility\r\n        objects = rep.get.prims(["/World/crate", "/World/box"])\r\n        rep.randomizer.visibility(objects, probability=0.7)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-configure-data-writers",children:"Step 3: Configure Data Writers"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="dataset_formatters.py"',children:'# Multi-format export configuration\r\ndef setup_dataset_exporters(output_path="/tmp/synthetic"):\r\n    # COCO dataset format (YOLO, Detectron2 support)\r\n    coco_writer = rep.WriterRegistry.get("CocoWriter")\r\n    coco_writer.initialize(\r\n        output_dir=f"{output_path}/coco",\r\n        rgb=True,\r\n        bounding_box_2d=True,\r\n        semantic_segmentation=True,\r\n        instance_segmentation=True,\r\n        instance_id_segmentation=True,\r\n        camera_params=True,\r\n        fov=90.0,\r\n        near_clipping_plane=0.01,\r\n        far_clipping_plane=100.0\r\n    )\r\n\r\n    # Pascal VOC format (Caffe, TensorFlow support)\r\n    voc_writer = rep.WriterRegistry.get("BasicWriter")\r\n    voc_writer.initialize(\r\n        output_dir=f"{output_path}/voc",\r\n        semantic_ids_to_class_names={\r\n            1: "humanoid",\r\n            2: "pallet",\r\n            3: "worker",\r\n            4: "equipment"\r\n        },\r\n        semantic_segmentation=True,\r\n        bounding_box_2d=True,\r\n        class_name_to_rgb={\r\n            "humanoid": (0, 255, 0),\r\n            "pallet": (0, 0, 255),\r\n            "worker": (255, 0, 0),\r\n            "equipment": (255, 255, 0)\r\n        },\r\n        class_name_to_instance_id={\r\n            "humanoid": 1000,  # 1000-1999 for humanoids\r\n            "pallet": 2000,    # 2000-2999 for pallets\r\n            "worker": 3000,\r\n            "equipment": 4000\r\n        }\r\n    )\r\n\r\n    # YOLO format (Galaxy, Ultr theft etc.)\r\n    yolo_writer = rep.WriterRegistry.get("BasicWriter")\r\n    yolo_writer.initialize(\r\n        output_dir=f"{output_path}/yolo",\r\n        rgb=True,\r\n        semantic_segmentation=True,\r\n        bounding_box_2d_normalized=True,\r\n    )\r\n\r\n    # Attach to render products\r\n    coco_writer.attach([rgb_product, semantic_product])\r\n    voc_writer.attach([rgb_product, semantic_product])\r\n    yolo_writer.attach([rgb_product])\n'})}),"\n",(0,t.jsx)(n.h2,{id:"domain-randomization-advanced-techniques",children:"Domain Randomization Advanced Techniques"}),"\n",(0,t.jsx)(n.h3,{id:"camera-dynamics",children:"Camera Dynamics"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="advanced_camera_randomization.py"',children:'# Realistic camera motion patterns\r\ndef create_plausible_trajectories():\r\n    # Simulate human operator movement\r\n    camera_patterns = [\r\n        rep.randomizer.create_camera_path(\r\n            path_type="leaning_against_walls",\r\n            velocity_range=(0.5, 2.0),  # 0.5-2 m/s walk speed\r\n            acceleration_range=(-2.0, 2.0),\r\n            jerk_limits=(-10, 10),\r\n            eye_height_distribution=(1.6, 0.1),  # Gaussian around 1.6m\r\n            head_angle_overlap=15  # degrees\r\n        ),\r\n        rep.randomizer.create_camera_path(\r\n            path_type="obstacle_avoidance_walk",\r\n            clearance_from_objects=0.5,  # 50cm clearance\r\n            preferred_direction_forward=True,\r\n            backwards_angle_limit=30  # degrees\r\n        )\r\n    ]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"humanoid-specific-considerations",children:"Humanoid-Specific Considerations"}),"\n",(0,t.jsx)(n.h3,{id:"tracking-humanoids-in-various-poses",children:"Tracking Humanoids in Various Poses"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="humanoid_data_augmentation.py"',children:'def generate_humanoid_training_data():\r\n    humanoid_prefix = "//World/humanoid_g1"\r\n    poses = [\r\n        "standing_idle", "walking_start", "walking_mid",\r\n        "turning_left", "turning_right", "step_up", "step_down",\r\n        "crouching_down", "raising_arms"\r\n    ]\r\n\r\n    for pose in poses:\r\n        # Set pose\r\n        rep.set_prim_property(\r\n            f"{humanoid_prefix}/skeleton_root/joint_*",\r\n            property_name="xformOp:rotateXYZ",\r\n            value=refer(cfg, key=pose)  # Reference pose configuration\r\n        )\r\n\r\n        # Create occlusion scenarios\r\n        rep.randomizer.visibility_of_occlusion_objects(\r\n            visibility_probability=0.6,\r\n            occlude_percentage_range=(0.1, 0.8)\r\n        )\r\n\r\n        # Body size variations\r\n        height_distribution = np.random.normal(1.7, 0.1)  # 170\xb110cm\r\n        rep.randomizer.scale_single_modification(\r\n            prim_path=humanoid_prefix,\r\n            scale_factor=height_distribution / 1.7\r\n        )\n'})}),"\n",(0,t.jsx)(n.h3,{id:"environmental-difficulty-scaling",children:"Environmental Difficulty Scaling"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="difficulty_scaling.py"',children:'def implement_progressive_difficulty():\r\n    # Beginner: Simple warehouse\r\n    beginner_scenes = [\r\n        {"objects": 5, "occlusion": 0.2, "lighting": "uniform"},\r\n        {"camera_velocity": 0.5, "frame_rate": 30, "resolution": 720}\r\n    ]\r\n\r\n    # Intermediate: More complexity\r\n    intermediate_scenes = [\r\n        {"objects": 15, "occlusion": 0.4, "lighting": "mixed"},\r\n        {"camera_velocity": 1.5, "frame_rate": 60, "resolution": 1080}\r\n    ]\r\n\r\n    # Advanced: Realistic conditions\r\n    advanced_scenes = [\r\n        {"objects": 30, "occlusion": 0.7, "lighting": "dynamic"},\r\n        {"camera_velocity": 2.0, "frame_rate": 120, "resolution": 1440}\r\n    ]\n'})}),"\n",(0,t.jsx)(n.h2,{id:"dataset-validation",children:"Dataset Validation"}),"\n",(0,t.jsx)(n.h3,{id:"automatic-quality-checks",children:"Automatic Quality Checks"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",metastring:'title="data_validation.py"',children:'def validate_generated_dataset(dataset_path="/tmp/synthetic"):\r\n    checks = {\r\n        "sufficient_unique_objects": 0.85,\r\n        "object_visibility_variance": 0.3,\r\n        "no_straight_edge_bboxes": True,\r\n        "reasonable_class_distribution": 0.8\r\n    }\r\n\r\n    validation_results = {}\r\n\r\n    # Check object diversity\r\n    unique_objects = count_unique_objects(dataset_path)\r\n    validation_results["diversity"] = unique_objects["total"] / unique_objects["types"] >= 10\r\n\r\n    # Verify bounding box reasonableness\r\n    bbox_stats = analyze_bounding_boxes(dataset_path)\r\n    validation_results["bboxes"] = bbox_stats["median_area_pct"] > 0.02 and bbox_stats["median_area_pct"] < 0.5\r\n\r\n    # Lighting/exposure variance\r\n    lighting_variance = calculate_lighting_variance(dataset_path)\r\n    validation_results["lighting"] = lighting_variance > 0.15\n'})}),"\n",(0,t.jsx)(n.h2,{id:"practical-example-hour-long-dataset-generation",children:"Practical Example: Hour-Long Dataset Generation"}),"\n",(0,t.jsx)(n.p,{children:"Here's a ready-to-run script that generates 1,000 diverse humanoid images in different warehouse scenarios:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",metastring:'title="quick_dataset_generation.sh"',children:"#!/bin/bash\r\n\r\n# H1 Humanoid Data Dataset\r\n\r\npython3 generate_humanoid_data.py \\\r\n    --output-dir /tmp/humanoid_training_data \\\r\n    --num-samples 1000 \\\r\n    --poses standing walking crouching reaching \\\r\n    --difficulties easy medium hard \\\r\n    --lighting-conditions sunny cloudy dim \\\r\n    --export-formats coco voc \\\r\n    --quality-validation\n"})}),"\n",(0,t.jsxs)(n.p,{children:["This demonstrates Isaac Sim's primary competitive advantage: ",(0,t.jsx)(n.strong,{children:"infinite, well-labeled data"})," at production scales unattainable in real-world collection scenarios.\"}''',},"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>s});var r=a(6540);const t={},i=r.createContext(t);function o(e){const n=r.useContext(i);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);