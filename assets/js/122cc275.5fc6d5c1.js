"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[7348],{1842:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"chapter-4-vla/vla-introduction","title":"4.1 Introduction to VLA Models","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/01-vla-introduction.md","sourceDirName":"chapter-4-vla","slug":"/vla-introduction","permalink":"/physical-ai-and-humanoid-robotics/docs/vla-introduction","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/01-vla-introduction.md","tags":[{"inline":true,"label":"VLA Architecture","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/vla-architecture"},{"inline":true,"label":"Foundation Models","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/foundation-models"},{"inline":true,"label":"Dual-System Design","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/dual-system-design"},{"inline":true,"label":"GR00T","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/gr-00-t"},{"inline":true,"label":"OpenVLA","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/open-vla"}],"version":"current","sidebarPosition":1,"frontMatter":{"id":"vla-introduction","slug":"/vla-introduction","title":"4.1 Introduction to VLA Models","sidebar_label":"4.1 VLA Introduction","sidebar_position":1,"difficulty":"Beginner","readingTime":"12 minutes","tags":["VLA Architecture","Foundation Models","Dual-System Design","GR00T","OpenVLA"]},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 4: VLA Models","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/"},"next":{"title":"Voice-to-Action","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/voice-to-action"}}');var t=i(4848),r=i(8453);const o={id:"vla-introduction",slug:"/vla-introduction",title:"4.1 Introduction to VLA Models",sidebar_label:"4.1 VLA Introduction",sidebar_position:1,difficulty:"Beginner",readingTime:"12 minutes",tags:["VLA Architecture","Foundation Models","Dual-System Design","GR00T","OpenVLA"]},a="4.1 Introduction to Vision-Language-Action Models",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"What Are VLA Models?",id:"what-are-vla-models",level:2},{value:"The Three Pillars of VLA",id:"the-three-pillars-of-vla",level:2},{value:"1. Vision Pipeline",id:"1-vision-pipeline",level:3},{value:"2. Language Processing",id:"2-language-processing",level:3},{value:"3. Action Decoding",id:"3-action-decoding",level:3},{value:"VLA Pipeline Architecture",id:"vla-pipeline-architecture",level:2},{value:"Dual-System Architecture",id:"dual-system-architecture",level:2},{value:"System 1: Reactive Control",id:"system-1-reactive-control",level:3},{value:"System 2: Deliberative Reasoning",id:"system-2-deliberative-reasoning",level:3},{value:"Dual-System Class Diagram",id:"dual-system-class-diagram",level:2},{value:"State-of-the-Art VLA Models",id:"state-of-the-art-vla-models",level:2},{value:"Conceptual VLA Forward Pass",id:"conceptual-vla-forward-pass",level:3},{value:"Loading &amp; Using OpenVLA Model",id:"loading--using-openvla-model",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Diagram Completion (Easy)",id:"exercise-1-diagram-completion-easy",level:3},{value:"Exercise 2: Model Comparison (Medium)",id:"exercise-2-model-comparison-medium",level:3},{value:"Exercise 3: Architecture Design (Hard)",id:"exercise-3-architecture-design-hard",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Next Steps",id:"next-steps",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"41-introduction-to-vision-language-action-models",children:"4.1 Introduction to Vision-Language-Action Models"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this sub-chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Define"})," Vision-Language-Action (VLA) models and explain how they unify perception, natural language, and motor control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Identify"})," the three architectural pillars that form the foundation of every VLA system"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Explain"})," the dual-system architecture pattern used by modern VLA models for reasoning and control"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Compare"})," leading VLA models (GR00T N1.5, OpenVLA, \u03c00.6) based on architecture and use cases"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"what-are-vla-models",children:"What Are VLA Models?"}),"\n",(0,t.jsx)(n.p,{children:"Vision-Language-Action (VLA) models are a paradigm shift in robotics, unifying three essential capabilities into single neural networks. Unlike traditional robotic systems that handle perception, reasoning, and action control as separate, hand-coded components, VLA models learn all three skills jointly from multimodal data."}),"\n",(0,t.jsx)(n.p,{children:'Consider this scenario: A humanoid robot equipped with a VLA model receives the command, "Please clear the dishes from the dining table and put them in the dishwasher." Here\'s what happens internally:'}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision"}),": The model perceives the cluttered table, identifies plates, cups, and utensils"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Language"}),": It understands the task, breaking it into sub-goals (find dishes, pick them up, locate dishwasher, load)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Action"}),": It generates precise motor commands to reach, grasp, and transport each item"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This holistic approach enables ",(0,t.jsx)(n.strong,{children:"generalization"}),'. Show a VLA model dishes in a new kitchen with different colors and positions, and it still succeeds\u2014it has learned the abstract concept of "dishwasher loading" rather than memorizing specific motions.']}),"\n",(0,t.jsx)(n.h2,{id:"the-three-pillars-of-vla",children:"The Three Pillars of VLA"}),"\n",(0,t.jsx)(n.p,{children:"Every VLA model architecture rests upon three pillars that process data in sequence:"}),"\n",(0,t.jsx)(n.h3,{id:"1-vision-pipeline",children:"1. Vision Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Converts visual observations into meaningful representations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"RGB Camera \u2192 Vision Encoder \u2192 Visual Embeddings\n[HW3] \u2192 [Patch Embeddings] \u2192 [Visual Features]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Common encoders include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"CLIP ViT"}),": Vision Transformer pre-trained on image-text pairs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"PaliGemma"}),": Google's vision-language encoder with spatial reasoning"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grounded-SAM2"}),": Segment Anything Model v2 for precise pixel-level understanding"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"2-language-processing",children:"2. Language Processing"}),"\n",(0,t.jsx)(n.p,{children:"Parses natural language instructions into structured understanding:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Natural Language \u2192 Language Encoder \u2192 Text Embeddings\n[Text JSON] \u2192 [Token IDs] \u2192 [Contextual Embeddings]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Popular choices:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Large Language Models"}),": GPT-4, Claude, Llama 3"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Instruction-Tuned Models"}),": Specialized for following robot commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain-Specific"}),": Models fine-tuned on robotics terminology"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"3-action-decoding",children:"3. Action Decoding"}),"\n",(0,t.jsx)(n.p,{children:"Transforms fused multimodal representations into motor commands:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Fused Embeddings \u2192 Action Decoder \u2192 Robot Actions\n[Combined Features] \u2192 [Specialized Decoder] \u2192 [7-DoF End-Effector]\n"})}),"\n",(0,t.jsx)(n.p,{children:"Decoder architectures:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Diffusion Transformers"}),": Probability distributions over actions (\u03c00.6)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Flow Matching"}),": Direct trajectory prediction via learned dynamics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Discrete Motion Primitives"}),": Vocabulary of atomic motions"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"vla-pipeline-architecture",children:"VLA Pipeline Architecture"}),"\n",(0,t.jsx)(n.mermaid,{value:"graph LR\n  A[Camera Input<br/>RGB Image] --\x3e B[CLIP Vision<br/>Encoder]\n  C[Text Command<br/>Pick up the red cup] --\x3e D[Language<br/>Encoder]\n\n  B --\x3e E[Cross-Attention<br/>Module]\n  D --\x3e E\n\n  E --\x3e F[Diffusion<br/>Transformer]\n  F --\x3e G[7-DoF Robot<br/>Actions]\n  F --\x3e H[Gripper<br/>Control]\n\n  style A fill:#E3F2FD,stroke:#1565C0\n  style C fill:#E8F5E9,stroke:#2E7D32\n  style G fill:#FFF3E0,stroke:#E65100\n  style H fill:#FCE4EC,stroke:#880E4F\n  style E fill:#F3E5F5,stroke:#6A1B9A\n  style F fill:#F3E5F5,stroke:#6A1B9A"}),"\n",(0,t.jsx)(n.h2,{id:"dual-system-architecture",children:"Dual-System Architecture"}),"\n",(0,t.jsxs)(n.p,{children:["Modern VLA models employ a ",(0,t.jsx)(n.strong,{children:"dual-system architecture"})," inspired by human cognition:"]}),"\n",(0,t.jsx)(n.h3,{id:"system-1-reactive-control",children:"System 1: Reactive Control"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speed"}),": sub-millisecond responses"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": Low-level motion control, reflex actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Examples"}),": Maintaining balance, adapting to contact forces"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"system-2-deliberative-reasoning",children:"System 2: Deliberative Reasoning"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speed"}),": dozens to hundreds of milliseconds"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task"}),": High-level planning, context understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Examples"}),": Interpreting complex instructions, strategizing approach"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:'This mirrors how humans operate: when coffee spills, System 1 instantly stabilizes your laptop as System 2 processes "Clean up this mess."'}),"\n",(0,t.jsx)(n.h2,{id:"dual-system-class-diagram",children:"Dual-System Class Diagram"}),"\n",(0,t.jsx)(n.mermaid,{value:"classDiagram\n  class System1Controller {\n    +update_rate_ms: 1\n    +process_sensor_inputs()\n    +generate_reactive_actions()\n    -balance_controller\n    -contact_reflex_module\n  }\n\n  class System2Planner {\n    +update_rate_ms: 100\n    +interpret_language_instruction()\n    +reason_about_scene()\n    +generate_action_strategy()\n    -language_model\n    -world_model\n  }\n\n  class VLAModel {\n    +timestamp: float\n    +encode_observation(image)\n    +encode_instruction(text)\n    +fuse_multimodal_data()\n    +decode_actions()\n    -vision_encoder\n    -text_encoder\n    -cross_attention\n    -action_decoder\n  }\n\n  class RobotHardware {\n    +joint_positions: float[]\n    +gripper_state: boolean\n    +receive_commands()\n    +send_observations()\n  }\n\n  System1Controller --\x3e RobotHardware : controls\n  System2Planner --\x3e VLAModel : uses\n  VLAModel --\x3e System1Controller : sends\n  System2Planner --\x3e System1Controller : coordinates\n\n  style System1Controller fill:#FFE0B2\n  style System2Planner fill:#C5E1A5\n  style VLAModel fill:#B2EBF2"}),"\n",(0,t.jsx)(n.h2,{id:"state-of-the-art-vla-models",children:"State-of-the-Art VLA Models"}),"\n",(0,t.jsx)(n.p,{children:"The VLA landscape evolves rapidly. Here's comparison of leading models as of December 2025:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Model"}),(0,t.jsx)(n.th,{children:"Developer"}),(0,t.jsx)(n.th,{children:"Parameters"}),(0,t.jsx)(n.th,{children:"Architecture"}),(0,t.jsx)(n.th,{children:"Training Data"}),(0,t.jsx)(n.th,{children:"Strengths"}),(0,t.jsx)(n.th,{children:"Best For"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"GR00T N1.5"})}),(0,t.jsx)(n.td,{children:"NVIDIA"}),(0,t.jsx)(n.td,{children:"5B"}),(0,t.jsx)(n.td,{children:"Dual-System (VLM + DiT)"}),(0,t.jsx)(n.td,{children:"1M+ teleoperated hours"}),(0,t.jsx)(n.td,{children:"Fine-tunable, Isaac Sim integration"}),(0,t.jsx)(n.td,{children:"Industrial applications, simulation-to-real"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"OpenVLA"})}),(0,t.jsx)(n.td,{children:"UC Berkeley"}),(0,t.jsx)(n.td,{children:"7B"}),(0,t.jsx)(n.td,{children:"Prismatic + DiT"}),(0,t.jsx)(n.td,{children:"RT-1 dataset (130k episodes)"}),(0,t.jsx)(n.td,{children:"Open-source, customizable"}),(0,t.jsx)(n.td,{children:"Research, custom fine-tuning"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"\u03c00.6"})}),(0,t.jsx)(n.td,{children:"Physical Intelligence"}),(0,t.jsx)(n.td,{children:"5B"}),(0,t.jsx)(n.td,{children:"Flow Matching expert"}),(0,t.jsx)(n.td,{children:"Heterogeneous prompts"}),(0,t.jsx)(n.td,{children:"Real-world deployment proven"}),(0,t.jsx)(n.td,{children:"Production household robots"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"ChatVLA-2"})}),(0,t.jsx)(n.td,{children:"Stanford"}),(0,t.jsx)(n.td,{children:"13B"}),(0,t.jsx)(n.td,{children:"Instruction-tuned"}),(0,t.jsx)(n.td,{children:"Diverse internet data"}),(0,t.jsx)(n.td,{children:"Maintains VLM capabilities during training"}),(0,t.jsx)(n.td,{children:"Multi-modal reasoning"})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"conceptual-vla-forward-pass",children:"Conceptual VLA Forward Pass"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Conceptual VLA implementation (educational pseudo-code)\nimport torch\nfrom typing import List, Tuple\n\nclass VisionLanguageAction:\n    def __init__(\n        self,\n        vision_encoder,  # e.g., CLIP ViT\n        text_encoder,    # e.g., Llama, GPT\n        fusion_module,   # Cross-attention layers\n        action_decoder   # Diffusion or flow\n    ):\n        self.vision = vision_encoder\n        self.language = text_encoder\n        self.fusion = fusion_module\n        self.actor = action_decoder\n\n    def forward(\n        self,\n        rgb_image: torch.Tensor,      # [C, H, W]\n        text_command: str,             # "Pick up the red cup"\n        robot_joint_angles: torch.Tensor = None  # [N, 7]\n    ) -> torch.Tensor:\n        # Encode visual observation\n        visual_features = self.vision(rgb_image)\n        encoding_time = time.time()\n\n        # Encode textual instruction\n        text_features = self.language(text_command)\n\n        # Fuse multimodal representations\n        fused_features = self.fusion(\n            vision=visual_features,\n            language=text_features,\n            proprioception=robot_joint_angles\n        )\n\n        # Generate action predictions\n        actions = self.actor(fused_features)\n\n        return actions  # [\u0394x, \u0394y, \u0394z, \u0394roll, \u0394pitch, \u0394yaw, gripper]\n\n# Usage example\nvla_model = VisionLanguageAction(\n    vision_encoder=CLIP_ViT(),\n    text_encoder=Llama2_7B(),\n    fusion_module=CrossAttentionFusion(),\n    action_decoder=DiffusionTransformer()\n)\n\nrobot_actions = vla_model(\n    rgb_image=camera_frame,\n    text_command="Pick up the red cup and place it on the table",\n    robot_joint_angles=current_joints\n)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"loading--using-openvla-model",children:"Loading & Using OpenVLA Model"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Python 3.11+\n# Loading a state-of-the-art VLA model (PyTorch 2.x)\nfrom transformers import VlaModel, VlaProcessor\nfrom PIL import Image\n\n# Load OpenVLA model and processor\nmodel = VlaModel.from_pretrained("yl7900/open-vla-7b")\nprocessor = VlaProcessor.from_pretrained("yl7900/open-vla-7b")\n\n# Prepare inputs\ncamera_image = Image.open("kitchen_scene.jpg")\ninstruction = "Clear the table and load the dishwasher"\n\n# Encode multimodal inputs\ninputs = processor(\n    images=camera_image,\n    text=instruction,\n    return_tensors="pt"\n)\n\n# Generate actions\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n    # Decode actions from model outputs\n    next_action = processor.decode_action(\n        outputs.logits,\n        normalization_stats=dataset_stats\n    )\n\n# Execute on robot\ncurrent_joints = get_robot_state()\ntarget_joints = current_joints + next_action\nmove_robot(target_joints)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-diagram-completion-easy",children:"Exercise 1: Diagram Completion (Easy)"}),"\n",(0,t.jsx)(n.p,{children:"Complete the VLA pipeline diagram by:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Adding missing data dimensions to each stage"}),"\n",(0,t.jsx)(n.li,{children:"Labeling key fusion layer operations"}),"\n",(0,t.jsx)(n.li,{children:"Identifying which components run at different update frequencies"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-model-comparison-medium",children:"Exercise 2: Model Comparison (Medium)"}),"\n",(0,t.jsx)(n.p,{children:"Create a detailed comparison matrix of three VLA models not covered above (research ChatVLA-2, SpatialVLA, or any recent ArXiv papers). Include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Architecture differences"}),"\n",(0,t.jsx)(n.li,{children:"Training dataset composition"}),"\n",(0,t.jsx)(n.li,{children:"Intended deployment scenario"}),"\n",(0,t.jsx)(n.li,{children:"Open-source availability"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-architecture-design-hard",children:"Exercise 3: Architecture Design (Hard)"}),"\n",(0,t.jsx)(n.p,{children:"Design a variant VLA architecture for a specific use case (e.g., autonomous driving, warehouse automation, or household manipulation). Consider:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How to handle the domain's unique challenges"}),"\n",(0,t.jsx)(n.li,{children:"Which existing components to reuse vs. rebuild"}),"\n",(0,t.jsx)(n.li,{children:"Performance requirements and safety constraints"}),"\n",(0,t.jsx)(n.li,{children:"Training data collection strategy"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,t.jsx)(n.p,{children:"For deeper understanding of VLA architectures and current research:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"NVIDIA GR00T Paper"}),' (2025): "Open Foundation Model for Humanoid Robots"']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Technical implementation details"}),"\n",(0,t.jsx)(n.li,{children:"Fine-tuning strategies for specific tasks"}),"\n",(0,t.jsx)(n.li,{children:"Performance benchmarks across robot platforms"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"OpenVLA Repository"})," (GitHub): ",(0,t.jsx)(n.code,{children:"github.com/openvla"})]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Practical implementation examples"}),"\n",(0,t.jsx)(n.li,{children:"Training and fine-tuning code"}),"\n",(0,t.jsx)(n.li,{children:"Community-contributed improvements"}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"VLA Survey"}),' (ArXiv 2024): "Vision-Language-Action Models for Robotics"']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Comprehensive architecture comparison"}),"\n",(0,t.jsx)(n.li,{children:"Training dataset analysis"}),"\n",(0,t.jsx)(n.li,{children:"Future research directions"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.p,{children:["Now that you understand VLA architecture and leading models, let's implement the first practical component: ",(0,t.jsx)(n.strong,{children:"speech recognition"})," that converts natural language instructions into robot commands. In ",(0,t.jsx)(n.a,{href:"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/voice-to-action",children:"sub-chapter 4.2"}),", you'll build a complete voice-to-action pipeline using OpenAI Whisper and ROS 2, culminating in a 15-minute Quick Start that enables voice-controlled robot operation."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var s=i(6540);const t={},r=s.createContext(t);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(r.Provider,{value:n},e.children)}}}]);