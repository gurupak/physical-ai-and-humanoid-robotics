"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[4167],{7753:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"chapter-3-isaac-ai-brain/synthetic-data-overview","title":"Synthetic Data Generation with Isaac Sim","description":"Synthetic data generation is Isaac Sim\'s superpower - it can create unlimited, perfectly labeled training data faster than you can annotate real-world datasets.","source":"@site/docs/chapter-3-isaac-ai-brain/synthetic-data-overview.md","sourceDirName":"chapter-3-isaac-ai-brain","slug":"/chapter-3-isaac-ai-brain/synthetic-data-overview","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-3-isaac-ai-brain/synthetic-data-overview","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-3-isaac-ai-brain/synthetic-data-overview.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Isaac Sim: Synthetic Data Generation","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-3-isaac-ai-brain/isaac-sim-synthetic-data"},"next":{"title":"Scene Creation Tutorial","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-3-isaac-ai-brain/scene-creation"}}');var i=a(4848),r=a(8453);const o={},s="Synthetic Data Generation with Isaac Sim",c={},d=[{value:"Core Advantages of Synthetic Data",id:"core-advantages-of-synthetic-data",level:2},{value:"Perfect Labeling",id:"perfect-labeling",level:3},{value:"Isaac Sim Synthetic Data Pipeline",id:"isaac-sim-synthetic-data-pipeline",level:2},{value:"Implementation Walkthrough",id:"implementation-walkthrough",level:2},{value:"Step 1: Configure Multi-Pass Rendering",id:"step-1-configure-multi-pass-rendering",level:3},{value:"Step 2: Define Randomization Domains",id:"step-2-define-randomization-domains",level:3},{value:"Step 3: Configure Data Writers",id:"step-3-configure-data-writers",level:3},{value:"Domain Randomization Advanced Techniques",id:"domain-randomization-advanced-techniques",level:2},{value:"Camera Dynamics",id:"camera-dynamics",level:3},{value:"Humanoid-Specific Considerations",id:"humanoid-specific-considerations",level:2},{value:"Tracking Humanoids in Various Poses",id:"tracking-humanoids-in-various-poses",level:3},{value:"Environmental Difficulty Scaling",id:"environmental-difficulty-scaling",level:3},{value:"Dataset Validation",id:"dataset-validation",level:2},{value:"Automatic Quality Checks",id:"automatic-quality-checks",level:3},{value:"Practical Example: Hour-Long Dataset Generation",id:"practical-example-hour-long-dataset-generation",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"synthetic-data-generation-with-isaac-sim",children:"Synthetic Data Generation with Isaac Sim"})}),"\n",(0,i.jsx)(n.p,{children:"Synthetic data generation is Isaac Sim's superpower - it can create unlimited, perfectly labeled training data faster than you can annotate real-world datasets."}),"\n",(0,i.jsx)(n.h2,{id:"core-advantages-of-synthetic-data",children:"Core Advantages of Synthetic Data"}),"\n",(0,i.jsx)(n.h3,{id:"perfect-labeling",children:"Perfect Labeling"}),"\n",(0,i.jsx)(n.p,{children:"Unlike real-world data collection, synthetic data includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"100% accurate ground truth"})," for every pixel"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Infinite variations"})," of the same basic scene"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Customizable environmental conditions"})}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"No privacy concerns"})," in data gathering"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"isaac-sim-synthetic-data-pipeline",children:"Isaac Sim Synthetic Data Pipeline"}),"\n",(0,i.jsx)(n.mermaid,{value:"graph LR\n    A[Scene Definition] --\x3e B[Parameter Randomization]\n    B --\x3e C[Render Passes]\n    C --\x3e D[Annotation Generation]\n    D --\x3e E[Dataset Export]\n\n    C --\x3e C1[RGB Image]\n    C --\x3e C2[Depth Map]\n    C --\x3e C3[Normal Map]\n    C --\x3e C4[Semantic/Instance]\n\n    D --\x3e D1[COCO Format]\n    D --\x3e D2[Pascal VOC]\n    D --\x3e D3[Custom XML]"}),"\n",(0,i.jsx)(n.h2,{id:"implementation-walkthrough",children:"Implementation Walkthrough"}),"\n",(0,i.jsx)(n.h3,{id:"step-1-configure-multi-pass-rendering",children:"Step 1: Configure Multi-Pass Rendering"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="synthetic_data_setup.py"',children:'import omni.replicator.core as rep\nimport carb\n\n# Create render products with different outputs\nwith rep.new_layer():\n    # Scene camera\n    camera = rep.create.camera(position=(2, 1.6, 2), look_at=(0, 0, 0))\n\n    # High-frequency render product for detail\n    rgb_product = rep.create.render_product(camera, (1920, 1080), name="rgb")\n    depth_product = rep.create.render_product(camera, (1920, 1080), name="depth")\n\n    # Class and instance segmentation\n    semantic_product = rep.create.render_product(camera, (1920, 1080), name="semantic")\n    instance_product = rep.create.render_product(camera, (1920, 1080), name="instance")\n\n    # Surface normals\n    normal_product = rep.create.render_product(camera, (1920, 1080), name="normal")\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-2-define-randomization-domains",children:"Step 2: Define Randomization Domains"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="domain_randomization.py"',children:'# Generate variations automatically\ndef create_scene_variations():\n    with camera:\n        # Camera pose variations\n        rep.randomizer.pose_camera_around_target(\n            target="/World/warehouse_shelves",\n            distance_range=(1.0, 5.0),\n            yaw_range=(-180, 180),\n            pitch_range=(-30, 30),\n            count=1000\n        )\n\n        # Lighting variations\n        rep.randomizer.light_orientation(\n            "/World/DistantLight",\n            rotation_range=(-45, 45)\n        )\n\n        # Material randomization\n        materials = rep.get.prims(["/World/pallet", "/World/table"])\n        rep.randomizer.material(materials, materials=None)\n\n        # Object visibility\n        objects = rep.get.prims(["/World/crate", "/World/box"])\n        rep.randomizer.visibility(objects, probability=0.7)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"step-3-configure-data-writers",children:"Step 3: Configure Data Writers"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="dataset_formatters.py"',children:'# Multi-format export configuration\ndef setup_dataset_exporters(output_path="/tmp/synthetic"):\n    # COCO dataset format (YOLO, Detectron2 support)\n    coco_writer = rep.WriterRegistry.get("CocoWriter")\n    coco_writer.initialize(\n        output_dir=f"{output_path}/coco",\n        rgb=True,\n        bounding_box_2d=True,\n        semantic_segmentation=True,\n        instance_segmentation=True,\n        instance_id_segmentation=True,\n        camera_params=True,\n        fov=90.0,\n        near_clipping_plane=0.01,\n        far_clipping_plane=100.0\n    )\n\n    # Pascal VOC format (Caffe, TensorFlow support)\n    voc_writer = rep.WriterRegistry.get("BasicWriter")\n    voc_writer.initialize(\n        output_dir=f"{output_path}/voc",\n        semantic_ids_to_class_names={\n            1: "humanoid",\n            2: "pallet",\n            3: "worker",\n            4: "equipment"\n        },\n        semantic_segmentation=True,\n        bounding_box_2d=True,\n        class_name_to_rgb={\n            "humanoid": (0, 255, 0),\n            "pallet": (0, 0, 255),\n            "worker": (255, 0, 0),\n            "equipment": (255, 255, 0)\n        },\n        class_name_to_instance_id={\n            "humanoid": 1000,  # 1000-1999 for humanoids\n            "pallet": 2000,    # 2000-2999 for pallets\n            "worker": 3000,\n            "equipment": 4000\n        }\n    )\n\n    # YOLO format (Galaxy, Ultr theft etc.)\n    yolo_writer = rep.WriterRegistry.get("BasicWriter")\n    yolo_writer.initialize(\n        output_dir=f"{output_path}/yolo",\n        rgb=True,\n        semantic_segmentation=True,\n        bounding_box_2d_normalized=True,\n    )\n\n    # Attach to render products\n    coco_writer.attach([rgb_product, semantic_product])\n    voc_writer.attach([rgb_product, semantic_product])\n    yolo_writer.attach([rgb_product])\n'})}),"\n",(0,i.jsx)(n.h2,{id:"domain-randomization-advanced-techniques",children:"Domain Randomization Advanced Techniques"}),"\n",(0,i.jsx)(n.h3,{id:"camera-dynamics",children:"Camera Dynamics"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="advanced_camera_randomization.py"',children:'# Realistic camera motion patterns\ndef create_plausible_trajectories():\n    # Simulate human operator movement\n    camera_patterns = [\n        rep.randomizer.create_camera_path(\n            path_type="leaning_against_walls",\n            velocity_range=(0.5, 2.0),  # 0.5-2 m/s walk speed\n            acceleration_range=(-2.0, 2.0),\n            jerk_limits=(-10, 10),\n            eye_height_distribution=(1.6, 0.1),  # Gaussian around 1.6m\n            head_angle_overlap=15  # degrees\n        ),\n        rep.randomizer.create_camera_path(\n            path_type="obstacle_avoidance_walk",\n            clearance_from_objects=0.5,  # 50cm clearance\n            preferred_direction_forward=True,\n            backwards_angle_limit=30  # degrees\n        )\n    ]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"humanoid-specific-considerations",children:"Humanoid-Specific Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"tracking-humanoids-in-various-poses",children:"Tracking Humanoids in Various Poses"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="humanoid_data_augmentation.py"',children:'def generate_humanoid_training_data():\n    humanoid_prefix = "//World/humanoid_g1"\n    poses = [\n        "standing_idle", "walking_start", "walking_mid",\n        "turning_left", "turning_right", "step_up", "step_down",\n        "crouching_down", "raising_arms"\n    ]\n\n    for pose in poses:\n        # Set pose\n        rep.set_prim_property(\n            f"{humanoid_prefix}/skeleton_root/joint_*",\n            property_name="xformOp:rotateXYZ",\n            value=refer(cfg, key=pose)  # Reference pose configuration\n        )\n\n        # Create occlusion scenarios\n        rep.randomizer.visibility_of_occlusion_objects(\n            visibility_probability=0.6,\n            occlude_percentage_range=(0.1, 0.8)\n        )\n\n        # Body size variations\n        height_distribution = np.random.normal(1.7, 0.1)  # 170\xb110cm\n        rep.randomizer.scale_single_modification(\n            prim_path=humanoid_prefix,\n            scale_factor=height_distribution / 1.7\n        )\n'})}),"\n",(0,i.jsx)(n.h3,{id:"environmental-difficulty-scaling",children:"Environmental Difficulty Scaling"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="difficulty_scaling.py"',children:'def implement_progressive_difficulty():\n    # Beginner: Simple warehouse\n    beginner_scenes = [\n        {"objects": 5, "occlusion": 0.2, "lighting": "uniform"},\n        {"camera_velocity": 0.5, "frame_rate": 30, "resolution": 720}\n    ]\n\n    # Intermediate: More complexity\n    intermediate_scenes = [\n        {"objects": 15, "occlusion": 0.4, "lighting": "mixed"},\n        {"camera_velocity": 1.5, "frame_rate": 60, "resolution": 1080}\n    ]\n\n    # Advanced: Realistic conditions\n    advanced_scenes = [\n        {"objects": 30, "occlusion": 0.7, "lighting": "dynamic"},\n        {"camera_velocity": 2.0, "frame_rate": 120, "resolution": 1440}\n    ]\n'})}),"\n",(0,i.jsx)(n.h2,{id:"dataset-validation",children:"Dataset Validation"}),"\n",(0,i.jsx)(n.h3,{id:"automatic-quality-checks",children:"Automatic Quality Checks"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="data_validation.py"',children:'def validate_generated_dataset(dataset_path="/tmp/synthetic"):\n    checks = {\n        "sufficient_unique_objects": 0.85,\n        "object_visibility_variance": 0.3,\n        "no_straight_edge_bboxes": True,\n        "reasonable_class_distribution": 0.8\n    }\n\n    validation_results = {}\n\n    # Check object diversity\n    unique_objects = count_unique_objects(dataset_path)\n    validation_results["diversity"] = unique_objects["total"] / unique_objects["types"] >= 10\n\n    # Verify bounding box reasonableness\n    bbox_stats = analyze_bounding_boxes(dataset_path)\n    validation_results["bboxes"] = bbox_stats["median_area_pct"] > 0.02 and bbox_stats["median_area_pct"] < 0.5\n\n    # Lighting/exposure variance\n    lighting_variance = calculate_lighting_variance(dataset_path)\n    validation_results["lighting"] = lighting_variance > 0.15\n'})}),"\n",(0,i.jsx)(n.h2,{id:"practical-example-hour-long-dataset-generation",children:"Practical Example: Hour-Long Dataset Generation"}),"\n",(0,i.jsx)(n.p,{children:"Here's a ready-to-run script that generates 1,000 diverse humanoid images in different warehouse scenarios:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",metastring:'title="quick_dataset_generation.sh"',children:"#!/bin/bash\n\n# H1 Humanoid Data Dataset\n\npython3 generate_humanoid_data.py \\\n    --output-dir /tmp/humanoid_training_data \\\n    --num-samples 1000 \\\n    --poses standing walking crouching reaching \\\n    --difficulties easy medium hard \\\n    --lighting-conditions sunny cloudy dim \\\n    --export-formats coco voc \\\n    --quality-validation\n"})}),"\n",(0,i.jsxs)(n.p,{children:["This demonstrates Isaac Sim's primary competitive advantage: ",(0,i.jsx)(n.strong,{children:"infinite, well-labeled data"})," at production scales unattainable in real-world collection scenarios.\"}''',},"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>s});var t=a(6540);const i={},r=t.createContext(i);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);