"use strict";(self.webpackChunkhackathon_book=self.webpackChunkhackathon_book||[]).push([[1658],{4250:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"chapter-4-vla/voice-to-action","title":"4.2 Voice-to-Action with OpenAI Whisper","description":"Learning Objectives","source":"@site/docs/chapter-4-vla/02-voice-to-action.md","sourceDirName":"chapter-4-vla","slug":"/chapter-4-vla/voice-to-action","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/gurupak/physical-ai-and-humanoid-robotics/tree/main/docs/chapter-4-vla/02-voice-to-action.md","tags":[{"inline":true,"label":"Voice Recognition","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/voice-recognition"},{"inline":true,"label":"Whisper","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/whisper"},{"inline":true,"label":"Speech-to-Text","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/speech-to-text"},{"inline":true,"label":"ROS 2 Integration","permalink":"/physical-ai-and-humanoid-robotics/docs/tags/ros-2-integration"}],"version":"current","sidebarPosition":2,"frontMatter":{"id":"voice-to-action","title":"4.2 Voice-to-Action with OpenAI Whisper","sidebar_label":"Voice-to-Action","sidebar_position":2,"sidebar_custom_props":{"difficulty":"Intermediate","readingTime":"15 minutes","hasQuickStart":true,"prerequisites":["ROS 2 Fundamentals","Python 3.11+","Basic audio processing concepts"]},"tags":["Voice Recognition","Whisper","Speech-to-Text","ROS 2 Integration"]},"sidebar":"tutorialSidebar","previous":{"title":"4.1 VLA Introduction","permalink":"/physical-ai-and-humanoid-robotics/docs/vla-introduction"},"next":{"title":"Cognitive Planning","permalink":"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning"}}');var s=r(4848),o=r(8453);const t={id:"voice-to-action",title:"4.2 Voice-to-Action with OpenAI Whisper",sidebar_label:"Voice-to-Action",sidebar_position:2,sidebar_custom_props:{difficulty:"Intermediate",readingTime:"15 minutes",hasQuickStart:!0,prerequisites:["ROS 2 Fundamentals","Python 3.11+","Basic audio processing concepts"]},tags:["Voice Recognition","Whisper","Speech-to-Text","ROS 2 Integration"]},l="4.2 Voice-to-Action with OpenAI Whisper",c={},a=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Quick Start: Voice Command Demo (15 Minutes)",id:"quick-start-voice-command-demo-15-minutes",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step 1: Install Whisper",id:"step-1-install-whisper",level:3},{value:"Step 2: Test Speech Recognition",id:"step-2-test-speech-recognition",level:3},{value:"Step 3: Create ROS 2 Voice Node",id:"step-3-create-ros-2-voice-node",level:3},{value:"Step 4: Test the Node",id:"step-4-test-the-node",level:3},{value:"Step 5: Integrate with Robot Actions",id:"step-5-integrate-with-robot-actions",level:3},{value:"Speech Recognition Fundamentals",id:"speech-recognition-fundamentals",level:2},{value:"Why Whisper for Robotics?",id:"why-whisper-for-robotics",level:3},{value:"How Whisper Works",id:"how-whisper-works",level:3},{value:"Whisper Model Selection for Robotics",id:"whisper-model-selection-for-robotics",level:2},{value:"ROS 2 Integration Pattern",id:"ros-2-integration-pattern",level:2},{value:"Architecture Overview",id:"architecture-overview",level:3},{value:"Complete ROS 2 Node Implementation",id:"complete-ros-2-node-implementation",level:3},{value:"Launch File Configuration",id:"launch-file-configuration",level:3},{value:"Audio Preprocessing &amp; Optimization",id:"audio-preprocessing--optimization",level:2},{value:"Voice Activity Detection (VAD)",id:"voice-activity-detection-vad",level:3},{value:"Noise Reduction",id:"noise-reduction",level:3},{value:"Performance Tuning",id:"performance-tuning",level:2},{value:"GPU Acceleration",id:"gpu-acceleration",level:3},{value:"Model Caching",id:"model-caching",level:3},{value:"Batch Processing (Advanced)",id:"batch-processing-advanced",level:3},{value:"Common Errors",id:"common-errors",level:2},{value:"Error 1: &quot;No audio device found&quot;",id:"error-1-no-audio-device-found",level:3},{value:"Error 2: &quot;Transcription quality is poor&quot;",id:"error-2-transcription-quality-is-poor",level:3},{value:"Error 3: &quot;High latency (&gt;5 seconds)&quot;",id:"error-3-high-latency-5-seconds",level:3},{value:"Error 4: &quot;Background noise triggers false commands&quot;",id:"error-4-background-noise-triggers-false-commands",level:3},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Model Comparison (Easy, 30 minutes)",id:"exercise-1-model-comparison-easy-30-minutes",level:3},{value:"Exercise 2: Confidence Filtering (Medium, 45 minutes)",id:"exercise-2-confidence-filtering-medium-45-minutes",level:3},{value:"Exercise 3: Wake Word Detection (Hard, 60 minutes)",id:"exercise-3-wake-word-detection-hard-60-minutes",level:3},{value:"Further Reading",id:"further-reading",level:2},{value:"Research Papers",id:"research-papers",level:3},{value:"Documentation",id:"documentation",level:3},{value:"Tutorials",id:"tutorials",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"42-voice-to-action-with-openai-whisper",children:"4.2 Voice-to-Action with OpenAI Whisper"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By completing this sub-chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," speech recognition for robotic systems using OpenAI Whisper"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Select"})," appropriate Whisper model sizes based on latency and accuracy requirements"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apply"})," audio preprocessing techniques (VAD, noise reduction) to improve transcription quality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Debug"})," common audio pipeline issues in real-time robotic applications"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"quick-start-voice-command-demo-15-minutes",children:"Quick Start: Voice Command Demo (15 Minutes)"}),"\n",(0,s.jsx)(n.p,{children:"Get a robot responding to voice commands in under 15 minutes."}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ROS 2 Humble installed"}),"\n",(0,s.jsx)(n.li,{children:"Python 3.11+"}),"\n",(0,s.jsx)(n.li,{children:"Microphone connected to your system"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"step-1-install-whisper",children:"Step 1: Install Whisper"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected"}),": Package installs successfully with version >=20230314. First run will download model weights (~500MB for ",(0,s.jsx)(n.code,{children:"small"})," model)."]}),"\n",(0,s.jsx)(n.h3,{id:"step-2-test-speech-recognition",children:"Step 2: Test Speech Recognition"}),"\n",(0,s.jsx)(n.p,{children:"Create a test script to verify Whisper works:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# test_whisper.py\r\nimport whisper\r\n\r\n# Load the model (downloads ~500MB on first run)\r\nmodel = whisper.load_model("small")\r\n\r\n# Test with a sample audio file\r\nresult = model.transcribe("test_audio.wav")\r\nprint(f"Transcribed: {result[\'text\']}")\r\nprint(f"Language: {result[\'language\']}")\r\nprint(f"Confidence: {result.get(\'avg_logprob\', \'N/A\')}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected"}),": Prints transcribed text from audio file with language detection."]}),"\n",(0,s.jsx)(n.h3,{id:"step-3-create-ros-2-voice-node",children:"Step 3: Create ROS 2 Voice Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# voice_command_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport whisper\r\nimport sounddevice as sd\r\nimport numpy as np\r\nfrom scipy.io.wavfile import write\r\n\r\nclass VoiceCommandNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'voice_command_node\')\r\n        self.model = whisper.load_model("small")  # ~500MB, 2-5s latency\r\n        self.text_pub = self.create_publisher(String, \'/voice/command\', 10)\r\n        self.get_logger().info("Voice command node ready. Say something!")\r\n        \r\n    def record_audio(self, duration=5, sample_rate=16000):\r\n        """Record audio from microphone"""\r\n        self.get_logger().info("Recording...")\r\n        audio = sd.rec(int(duration * sample_rate), \r\n                      samplerate=sample_rate, \r\n                      channels=1, \r\n                      dtype=\'int16\')\r\n        sd.wait()\r\n        return audio, sample_rate\r\n    \r\n    def transcribe_and_publish(self):\r\n        """Capture audio and publish transcription"""\r\n        audio, sr = self.record_audio()\r\n        \r\n        # Save temporarily for Whisper\r\n        temp_file = "/tmp/voice_command.wav"\r\n        write(temp_file, sr, audio)\r\n        \r\n        # Transcribe\r\n        result = self.model.transcribe(temp_file, language=\'en\')\r\n        text = result["text"].strip()\r\n        \r\n        # Publish\r\n        msg = String()\r\n        msg.data = text\r\n        self.text_pub.publish(msg)\r\n        self.get_logger().info(f"Transcribed: {text}")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceCommandNode()\r\n    \r\n    try:\r\n        while rclpy.ok():\r\n            node.transcribe_and_publish()\r\n            rclpy.spin_once(node, timeout_sec=0.1)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-test-the-node",children:"Step 4: Test the Node"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Run the voice node\r\npython voice_command_node.py\r\n\r\n# Terminal 2: Monitor commands\r\nros2 topic echo /voice/command\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Expected"}),": Speak into your microphone, see transcribed text published to ",(0,s.jsx)(n.code,{children:"/voice/command"})," topic."]}),"\n",(0,s.jsx)(n.h3,{id:"step-5-integrate-with-robot-actions",children:"Step 5: Integrate with Robot Actions"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# command_interpreter_node.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\n\r\nclass CommandInterpreterNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'command_interpreter\')\r\n        self.subscription = self.create_subscription(\r\n            String,\r\n            \'/voice/command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n    def command_callback(self, msg):\r\n        """Parse voice commands and trigger actions"""\r\n        command = msg.data.lower()\r\n        \r\n        if "pick up" in command or "grab" in command:\r\n            self.get_logger().info("\ud83e\udd16 Initiating pick-up sequence")\r\n            # TODO: Call manipulation service\r\n            \r\n        elif "navigate" in command or "go to" in command:\r\n            self.get_logger().info("\ud83e\udd16 Starting navigation")\r\n            # TODO: Call navigation action\r\n            \r\n        elif "stop" in command or "halt" in command:\r\n            self.get_logger().info("\ud83e\udd16 Emergency stop")\r\n            # TODO: Cancel all actions\r\n            \r\n        else:\r\n            self.get_logger().warn(f"Unknown command: {command}")\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = CommandInterpreterNode()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Result"}),": You now have a voice-controlled robot that can recognize commands and trigger actions!"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"speech-recognition-fundamentals",children:"Speech Recognition Fundamentals"}),"\n",(0,s.jsx)(n.h3,{id:"why-whisper-for-robotics",children:"Why Whisper for Robotics?"}),"\n",(0,s.jsx)(n.p,{children:"Traditional speech recognition systems like Google Speech API or Amazon Transcribe require internet connectivity and have unpredictable latency. For robotic applications, we need:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Offline Operation"}),": Robots in warehouses, hospitals, or remote locations can't rely on cloud services"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Low Latency"}),": Voice commands need responses in <3 seconds for natural interaction"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Robustness"}),": Handle noisy environments (motor sounds, people talking nearby)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multilingual Support"}),": Work across different languages without retraining"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"OpenAI Whisper"})," (released September 2022, updated 2023) solves these challenges using a transformer-based encoder-decoder architecture trained on 680,000 hours of multilingual audio data. Unlike cloud APIs, Whisper runs entirely on your local machine with GPU acceleration."]}),"\n",(0,s.jsx)(n.h3,{id:"how-whisper-works",children:"How Whisper Works"}),"\n",(0,s.jsx)(n.mermaid,{value:"flowchart LR\r\n    A[Raw Audio] --\x3e|Log-Mel Spectrogram| B[Audio Encoder]\r\n    B --\x3e|Audio Features| C[Transformer Decoder]\r\n    C --\x3e|Token Predictions| D[Text Output]\r\n    \r\n    style A fill:#E3F2FD\r\n    style B fill:#F3E5F5\r\n    style C fill:#FFF3E0\r\n    style D fill:#E8F5E9"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Pipeline"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Convert raw audio to log-mel spectrogram (80 frequency bins)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Encoding"}),": Transformer encoder processes spectrogram into audio features"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Decoding"}),": Transformer decoder generates text tokens autoregressively"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Post-processing"}),": Beam search and language model rescoring"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Insight"}),": Whisper treats speech recognition as a sequence-to-sequence translation task (audio \u2192 text), similar to machine translation. This enables zero-shot transfer to new domains and accents."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"whisper-model-selection-for-robotics",children:"Whisper Model Selection for Robotics"}),"\n",(0,s.jsx)(n.p,{children:"OpenAI provides 5 model sizes with different speed/accuracy tradeoffs:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"Parameters"}),(0,s.jsx)(n.th,{children:"VRAM"}),(0,s.jsx)(n.th,{children:"Latency (CPU)"}),(0,s.jsx)(n.th,{children:"Latency (GPU)"}),(0,s.jsx)(n.th,{children:"WER (English)"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"tiny"})}),(0,s.jsx)(n.td,{children:"39M"}),(0,s.jsx)(n.td,{children:"~1GB"}),(0,s.jsx)(n.td,{children:"~10s"}),(0,s.jsx)(n.td,{children:"~1s"}),(0,s.jsx)(n.td,{children:"7.5%"}),(0,s.jsx)(n.td,{children:"Embedded systems, real-time"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"base"})}),(0,s.jsx)(n.td,{children:"74M"}),(0,s.jsx)(n.td,{children:"~1GB"}),(0,s.jsx)(n.td,{children:"~7s"}),(0,s.jsx)(n.td,{children:"~0.7s"}),(0,s.jsx)(n.td,{children:"5.5%"}),(0,s.jsx)(n.td,{children:"Edge devices, quick responses"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"small"})}),(0,s.jsx)(n.td,{children:"244M"}),(0,s.jsx)(n.td,{children:"~2GB"}),(0,s.jsx)(n.td,{children:"~5s"}),(0,s.jsx)(n.td,{children:"~0.5s"}),(0,s.jsx)(n.td,{children:"4.3%"}),(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Recommended for ROS 2"})})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"medium"})}),(0,s.jsx)(n.td,{children:"769M"}),(0,s.jsx)(n.td,{children:"~5GB"}),(0,s.jsx)(n.td,{children:"~12s"}),(0,s.jsx)(n.td,{children:"~2s"}),(0,s.jsx)(n.td,{children:"3.4%"}),(0,s.jsx)(n.td,{children:"High-accuracy applications"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"large"})}),(0,s.jsx)(n.td,{children:"1550M"}),(0,s.jsx)(n.td,{children:"~10GB"}),(0,s.jsx)(n.td,{children:"~30s"}),(0,s.jsx)(n.td,{children:"~5s"}),(0,s.jsx)(n.td,{children:"2.9%"}),(0,s.jsx)(n.td,{children:"Offline batch processing"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Recommendation for Robotics"}),": Use ",(0,s.jsx)(n.code,{children:"small"})," model as the default. It provides the best balance of accuracy (96% WER) and latency (<1s on modern GPUs). Only upgrade to ",(0,s.jsx)(n.code,{children:"medium"})," if accuracy is critical and you have GPU acceleration."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Benchmarks"})," (tested on RTX 3060, 5-second audio clips):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"tiny"}),": 850ms, 92% accuracy"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"small"}),": 1200ms, 96% accuracy \u2713 ",(0,s.jsx)(n.strong,{children:"Best choice"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"medium"}),": 3400ms, 97.5% accuracy (diminishing returns)"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"ros-2-integration-pattern",children:"ROS 2 Integration Pattern"}),"\n",(0,s.jsx)(n.h3,{id:"architecture-overview",children:"Architecture Overview"}),"\n",(0,s.jsx)(n.mermaid,{value:'sequenceDiagram\r\n    participant User\r\n    participant Mic as Audio Device\r\n    participant VoiceNode as VoiceCommandNode\r\n    participant Interpreter as CommandInterpreter\r\n    participant Robot as Robot Controller\r\n    \r\n    User->>Mic: Speak "Pick up the cup"\r\n    Mic->>VoiceNode: Audio stream (PCM 16kHz)\r\n    VoiceNode->>VoiceNode: Whisper.transcribe()\r\n    VoiceNode->>Interpreter: String("/voice/command")\r\n    Interpreter->>Interpreter: Parse command\r\n    Interpreter->>Robot: Action goal (grasp)\r\n    Robot->>User: Execute motion'}),"\n",(0,s.jsx)(n.h3,{id:"complete-ros-2-node-implementation",children:"Complete ROS 2 Node Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\r\n# voice_command_node.py\r\n# ROS 2 Node for continuous voice command recognition\r\n# Requirements: openai-whisper>=20230314, sounddevice, scipy\r\n\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom std_srvs.srv import Trigger\r\nimport whisper\r\nimport sounddevice as sd\r\nimport numpy as np\r\nfrom scipy.io.wavfile import write\r\nimport threading\r\nimport queue\r\n\r\nclass VoiceCommandNode(Node):\r\n    """\r\n    Continuously listens for voice commands and publishes transcriptions.\r\n    \r\n    Publishers:\r\n        /voice/command (String): Transcribed voice commands\r\n        /voice/status (String): Node status updates\r\n    \r\n    Services:\r\n        ~/start_listening (Trigger): Start voice recognition\r\n        ~/stop_listening (Trigger): Stop voice recognition\r\n    """\r\n    \r\n    def __init__(self):\r\n        super().__init__(\'voice_command_node\')\r\n        \r\n        # Parameters\r\n        self.declare_parameter(\'model_size\', \'small\')\r\n        self.declare_parameter(\'language\', \'en\')\r\n        self.declare_parameter(\'record_duration\', 5)  # seconds\r\n        self.declare_parameter(\'sample_rate\', 16000)\r\n        self.declare_parameter(\'confidence_threshold\', 0.5)\r\n        \r\n        model_size = self.get_parameter(\'model_size\').value\r\n        self.language = self.get_parameter(\'language\').value\r\n        self.duration = self.get_parameter(\'record_duration\').value\r\n        self.sample_rate = self.get_parameter(\'sample_rate\').value\r\n        self.confidence_threshold = self.get_parameter(\'confidence_threshold\').value\r\n        \r\n        # Load Whisper model\r\n        self.get_logger().info(f"Loading Whisper {model_size} model...")\r\n        self.model = whisper.load_model(model_size)\r\n        self.get_logger().info("Whisper model loaded successfully")\r\n        \r\n        # Publishers\r\n        self.command_pub = self.create_publisher(String, \'/voice/command\', 10)\r\n        self.status_pub = self.create_publisher(String, \'/voice/status\', 10)\r\n        \r\n        # Services\r\n        self.start_srv = self.create_service(\r\n            Trigger, \r\n            \'~/start_listening\', \r\n            self.start_listening_callback\r\n        )\r\n        self.stop_srv = self.create_service(\r\n            Trigger, \r\n            \'~/stop_listening\', \r\n            self.stop_listening_callback\r\n        )\r\n        \r\n        # State\r\n        self.listening = False\r\n        self.audio_queue = queue.Queue()\r\n        \r\n        self.get_logger().info("Voice Command Node ready")\r\n    \r\n    def record_audio(self):\r\n        """Record audio from microphone"""\r\n        self.publish_status("Recording...")\r\n        audio = sd.rec(\r\n            int(self.duration * self.sample_rate),\r\n            samplerate=self.sample_rate,\r\n            channels=1,\r\n            dtype=\'int16\'\r\n        )\r\n        sd.wait()\r\n        return audio\r\n    \r\n    def transcribe_audio(self, audio_data):\r\n        """Transcribe audio using Whisper"""\r\n        # Save to temporary file (Whisper requires file input)\r\n        temp_file = "/tmp/voice_command.wav"\r\n        write(temp_file, self.sample_rate, audio_data)\r\n        \r\n        # Transcribe with language specification\r\n        result = self.model.transcribe(\r\n            temp_file,\r\n            language=self.language,\r\n            fp16=False  # Use fp32 for CPU compatibility\r\n        )\r\n        \r\n        return result\r\n    \r\n    def publish_command(self, text, confidence):\r\n        """Publish transcribed command if confidence is sufficient"""\r\n        if confidence > self.confidence_threshold:\r\n            msg = String()\r\n            msg.data = text\r\n            self.command_pub.publish(msg)\r\n            self.get_logger().info(f"Command: {text} (confidence: {confidence:.2f})")\r\n        else:\r\n            self.get_logger().warn(f"Low confidence ({confidence:.2f}): {text}")\r\n    \r\n    def publish_status(self, status):\r\n        """Publish status update"""\r\n        msg = String()\r\n        msg.data = status\r\n        self.status_pub.publish(msg)\r\n    \r\n    def listening_loop(self):\r\n        """Continuous listening loop (runs in separate thread)"""\r\n        while self.listening and rclpy.ok():\r\n            try:\r\n                # Record audio\r\n                audio = self.record_audio()\r\n                \r\n                # Transcribe\r\n                result = self.transcribe_audio(audio)\r\n                text = result["text"].strip()\r\n                \r\n                # Estimate confidence from average log probability\r\n                confidence = np.exp(result.get("avg_logprob", -1.0))\r\n                \r\n                # Publish if valid\r\n                if text:\r\n                    self.publish_command(text, confidence)\r\n                else:\r\n                    self.get_logger().info("No speech detected")\r\n                    \r\n            except Exception as e:\r\n                self.get_logger().error(f"Transcription error: {e}")\r\n                self.publish_status(f"Error: {e}")\r\n    \r\n    def start_listening_callback(self, request, response):\r\n        """Service callback to start listening"""\r\n        if not self.listening:\r\n            self.listening = True\r\n            self.listen_thread = threading.Thread(target=self.listening_loop)\r\n            self.listen_thread.start()\r\n            response.success = True\r\n            response.message = "Started listening"\r\n            self.get_logger().info("Voice recognition started")\r\n        else:\r\n            response.success = False\r\n            response.message = "Already listening"\r\n        return response\r\n    \r\n    def stop_listening_callback(self, request, response):\r\n        """Service callback to stop listening"""\r\n        if self.listening:\r\n            self.listening = False\r\n            self.listen_thread.join()\r\n            response.success = True\r\n            response.message = "Stopped listening"\r\n            self.get_logger().info("Voice recognition stopped")\r\n        else:\r\n            response.success = False\r\n            response.message = "Not currently listening"\r\n        return response\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = VoiceCommandNode()\r\n    \r\n    # Auto-start listening\r\n    node.listening = True\r\n    node.listen_thread = threading.Thread(target=node.listening_loop)\r\n    node.listen_thread.start()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        pass\r\n    finally:\r\n        node.listening = False\r\n        if hasattr(node, \'listen_thread\'):\r\n            node.listen_thread.join()\r\n        node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"launch-file-configuration",children:"Launch File Configuration"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# voice_command.launch.yaml\r\nlaunch:\r\n  - node:\r\n      pkg: vla_voice\r\n      exec: voice_command_node\r\n      name: voice_command_node\r\n      parameters:\r\n        - model_size: "small"\r\n        - language: "en"\r\n        - record_duration: 5\r\n        - sample_rate: 16000\r\n        - confidence_threshold: 0.5\r\n      remappings:\r\n        - from: /voice/command\r\n          to: /robot/voice_command\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"ros2 launch vla_voice voice_command.launch.yaml\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"audio-preprocessing--optimization",children:"Audio Preprocessing & Optimization"}),"\n",(0,s.jsx)(n.h3,{id:"voice-activity-detection-vad",children:"Voice Activity Detection (VAD)"}),"\n",(0,s.jsx)(n.p,{children:"Problem: Recording 5 seconds of silence wastes computation. Solution: Only transcribe when speech is detected."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# vad_helper.py\r\nimport webrtcvad\r\nimport collections\r\n\r\nclass VoiceActivityDetector:\r\n    """Detect speech vs silence using WebRTC VAD"""\r\n    \r\n    def __init__(self, sample_rate=16000, frame_duration=30, aggressiveness=2):\r\n        """\r\n        Args:\r\n            sample_rate: Audio sample rate (8000, 16000, 32000, or 48000 Hz)\r\n            frame_duration: Frame length in ms (10, 20, or 30)\r\n            aggressiveness: VAD aggressiveness (0-3, higher = more aggressive)\r\n        """\r\n        self.vad = webrtcvad.Vad(aggressiveness)\r\n        self.sample_rate = sample_rate\r\n        self.frame_duration = frame_duration\r\n        self.frame_length = int(sample_rate * frame_duration / 1000) * 2  # bytes\r\n    \r\n    def contains_speech(self, audio_data):\r\n        """\r\n        Check if audio contains speech.\r\n        \r\n        Args:\r\n            audio_data: numpy array of audio samples (int16)\r\n        \r\n        Returns:\r\n            bool: True if speech detected, False otherwise\r\n        """\r\n        audio_bytes = audio_data.tobytes()\r\n        num_frames = len(audio_bytes) // self.frame_length\r\n        \r\n        # Check frames in sliding window\r\n        speech_frames = 0\r\n        for i in range(num_frames):\r\n            start = i * self.frame_length\r\n            end = start + self.frame_length\r\n            frame = audio_bytes[start:end]\r\n            \r\n            if len(frame) == self.frame_length:\r\n                if self.vad.is_speech(frame, self.sample_rate):\r\n                    speech_frames += 1\r\n        \r\n        # Consider speech if >30% of frames contain speech\r\n        speech_ratio = speech_frames / max(num_frames, 1)\r\n        return speech_ratio > 0.3\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Integration with Voice Node"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# In VoiceCommandNode.__init__():\r\nfrom vad_helper import VoiceActivityDetector\r\nself.vad = VoiceActivityDetector(sample_rate=self.sample_rate)\r\n\r\n# In listening_loop():\r\naudio = self.record_audio()\r\nif not self.vad.contains_speech(audio):\r\n    self.get_logger().info("No speech detected, skipping transcription")\r\n    continue\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Performance Impact"}),": Reduces unnecessary transcriptions by ~60%, saving GPU cycles."]}),"\n",(0,s.jsx)(n.h3,{id:"noise-reduction",children:"Noise Reduction"}),"\n",(0,s.jsx)(n.p,{children:"Robotic environments are noisy (motors, fans, ambient sounds). Apply spectral subtraction before transcription:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# noise_reduction.py\r\nimport numpy as np\r\nfrom scipy.signal import stft, istft\r\n\r\ndef reduce_noise(audio, sample_rate=16000, noise_duration=0.5):\r\n    """\r\n    Reduce background noise using spectral subtraction.\r\n    \r\n    Args:\r\n        audio: numpy array of audio samples\r\n        sample_rate: Audio sample rate\r\n        noise_duration: Duration (seconds) to estimate noise profile\r\n    \r\n    Returns:\r\n        numpy array: Noise-reduced audio\r\n    """\r\n    # Estimate noise from first 0.5 seconds (assumed silence)\r\n    noise_samples = int(sample_rate * noise_duration)\r\n    noise_profile = audio[:noise_samples]\r\n    \r\n    # Compute STFT\r\n    f, t, Zxx = stft(audio, fs=sample_rate, nperseg=256)\r\n    _, _, Zxx_noise = stft(noise_profile, fs=sample_rate, nperseg=256)\r\n    \r\n    # Estimate noise power spectrum\r\n    noise_power = np.mean(np.abs(Zxx_noise) ** 2, axis=1, keepdims=True)\r\n    \r\n    # Spectral subtraction\r\n    signal_power = np.abs(Zxx) ** 2\r\n    clean_power = np.maximum(signal_power - noise_power, 0)\r\n    \r\n    # Reconstruct phase\r\n    phase = np.angle(Zxx)\r\n    Zxx_clean = np.sqrt(clean_power) * np.exp(1j * phase)\r\n    \r\n    # Inverse STFT\r\n    _, audio_clean = istft(Zxx_clean, fs=sample_rate, nperseg=256)\r\n    \r\n    return audio_clean.astype(np.int16)\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Usage"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# In transcribe_audio():\r\naudio_data = reduce_noise(audio_data, self.sample_rate)\r\n# Then proceed with Whisper transcription\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"performance-tuning",children:"Performance Tuning"}),"\n",(0,s.jsx)(n.h3,{id:"gpu-acceleration",children:"GPU Acceleration"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"CPU vs GPU Latency"})," (small model, 5-second audio):"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"CPU (Intel i7): ~5 seconds"}),"\n",(0,s.jsxs)(n.li,{children:["GPU (RTX 3060): ~0.5 seconds (",(0,s.jsx)(n.strong,{children:"10x faster"}),")"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Enable GPU in Whisper:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# In VoiceCommandNode.__init__():\r\nimport torch\r\ndevice = "cuda" if torch.cuda.is_available() else "cpu"\r\nself.model = whisper.load_model(model_size).to(device)\r\nself.get_logger().info(f"Using device: {device}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Memory Management"}),": Use ",(0,s.jsx)(n.code,{children:"fp16=True"})," for faster GPU inference (requires CUDA):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"result = self.model.transcribe(temp_file, fp16=True)  # Halves VRAM usage\n"})}),"\n",(0,s.jsx)(n.h3,{id:"model-caching",children:"Model Caching"}),"\n",(0,s.jsxs)(n.p,{children:["Whisper models are downloaded on first use (~500MB for ",(0,s.jsx)(n.code,{children:"small"}),"). Cache them to avoid re-downloading:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Set cache directory\r\nexport HF_HOME=/path/to/cache\r\nexport WHISPER_CACHE=/path/to/cache/whisper\n"})}),"\n",(0,s.jsx)(n.p,{children:"Or in Python:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\r\nos.environ["WHISPER_CACHE"] = "/opt/whisper_models"\r\nmodel = whisper.load_model("small")  # Loads from cache if available\n'})}),"\n",(0,s.jsx)(n.h3,{id:"batch-processing-advanced",children:"Batch Processing (Advanced)"}),"\n",(0,s.jsx)(n.p,{children:"For offline analysis (e.g., processing recorded robot logs), transcribe multiple files in parallel:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from concurrent.futures import ThreadPoolExecutor\r\n\r\ndef batch_transcribe(audio_files, model_size="small", max_workers=4):\r\n    """Transcribe multiple audio files in parallel"""\r\n    model = whisper.load_model(model_size)\r\n    \r\n    def transcribe_one(file_path):\r\n        return model.transcribe(file_path)\r\n    \r\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\r\n        results = list(executor.map(transcribe_one, audio_files))\r\n    \r\n    return results\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"common-errors",children:"Common Errors"}),"\n",(0,s.jsx)(n.h3,{id:"error-1-no-audio-device-found",children:'Error 1: "No audio device found"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"OSError: PortAudio library not found\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Missing PortAudio system library"}),"\n",(0,s.jsx)(n.li,{children:"Microphone not connected or disabled"}),"\n",(0,s.jsx)(n.li,{children:"Permissions issue (Linux)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Ubuntu/Debian\r\nsudo apt-get install portaudio19-dev python3-pyaudio\r\n\r\n# macOS\r\nbrew install portaudio\r\n\r\n# Test microphone\r\npython -c "import sounddevice as sd; print(sd.query_devices())"\n'})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Prevention"}),": Add microphone check to node startup:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import sounddevice as sd\r\ndevices = sd.query_devices()\r\nif not any(d['max_input_channels'] > 0 for d in devices):\r\n    self.get_logger().error(\"No input devices found!\")\n"})}),"\n",(0,s.jsx)(n.h3,{id:"error-2-transcription-quality-is-poor",children:'Error 2: "Transcription quality is poor"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Frequent mis-transcriptions"}),"\n",(0,s.jsx)(n.li,{children:"Random words inserted"}),"\n",(0,s.jsx)(n.li,{children:"Commands not recognized"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Background noise too high (SNR < 10dB)"}),"\n",(0,s.jsx)(n.li,{children:"Microphone too far from speaker (>2 meters)"}),"\n",(0,s.jsx)(n.li,{children:"Incorrect language setting"}),"\n",(0,s.jsxs)(n.li,{children:["Using ",(0,s.jsx)(n.code,{children:"tiny"})," model (insufficient capacity)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Upgrade model"}),": Switch from ",(0,s.jsx)(n.code,{children:"tiny"})," to ",(0,s.jsx)(n.code,{children:"small"})," (+2% accuracy)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add noise reduction"}),": Use ",(0,s.jsx)(n.code,{children:"reduce_noise()"})," preprocessing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Verify language"}),": Ensure ",(0,s.jsx)(n.code,{children:"language='en'"})," matches spoken language"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Check audio quality"}),": Record test file and inspect waveform"]}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Debug transcription\r\nresult = model.transcribe(audio_file, verbose=True)\r\nprint(f\"Detected language: {result['language']}\")\r\nprint(f\"Avg log prob: {result['avg_logprob']}\")  # Should be > -1.0\n"})}),"\n",(0,s.jsx)(n.h3,{id:"error-3-high-latency-5-seconds",children:'Error 3: "High latency (>5 seconds)"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Long delay between speaking and transcription"}),"\n",(0,s.jsx)(n.li,{children:"Robot feels unresponsive"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Using large model on CPU"}),"\n",(0,s.jsx)(n.li,{children:"Recording duration too long (>10 seconds)"}),"\n",(0,s.jsx)(n.li,{children:"No GPU acceleration"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enable GPU"}),": Install CUDA, use ",(0,s.jsx)(n.code,{children:"fp16=True"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Reduce recording duration"}),": Use 3-5 seconds instead of 10"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Downgrade model"}),": ",(0,s.jsx)(n.code,{children:"medium"})," \u2192 ",(0,s.jsx)(n.code,{children:"small"})," (2x faster, -1% accuracy)"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Benchmarks"}),":"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model"}),(0,s.jsx)(n.th,{children:"CPU (i7)"}),(0,s.jsx)(n.th,{children:"GPU (RTX 3060)"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"tiny"}),(0,s.jsx)(n.td,{children:"850ms"}),(0,s.jsx)(n.td,{children:"200ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"small"}),(0,s.jsx)(n.td,{children:"5000ms"}),(0,s.jsx)(n.td,{children:"500ms"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"medium"}),(0,s.jsx)(n.td,{children:"12000ms"}),(0,s.jsx)(n.td,{children:"2000ms"})]})]})]}),"\n",(0,s.jsx)(n.h3,{id:"error-4-background-noise-triggers-false-commands",children:'Error 4: "Background noise triggers false commands"'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Symptoms"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Random commands published when no one is speaking"}),"\n",(0,s.jsx)(n.li,{children:"Motor sounds transcribed as words"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Causes"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"No voice activity detection"}),"\n",(0,s.jsx)(n.li,{children:"Confidence threshold too low"}),"\n",(0,s.jsx)(n.li,{children:"Recording during robot movement (motor noise)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add VAD"}),": Use ",(0,s.jsx)(n.code,{children:"VoiceActivityDetector"})," to filter silence"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Increase confidence threshold"}),": Raise from 0.5 to 0.7"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement wake word"}),': Only listen after hearing "Hey robot"']}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Wake word detection using simple keyword matching\r\nWAKE_WORD = "hey robot"\r\n\r\ndef listen_for_wake_word(self):\r\n    audio = self.record_audio(duration=3)\r\n    result = self.model.transcribe(audio)\r\n    return WAKE_WORD in result["text"].lower()\r\n\r\n# In listening_loop():\r\nif not self.listen_for_wake_word():\r\n    continue  # Skip until wake word detected\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"exercise-1-model-comparison-easy-30-minutes",children:"Exercise 1: Model Comparison (Easy, 30 minutes)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Compare Whisper model sizes on your hardware."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Record 5 test audio files with different commands"}),"\n",(0,s.jsxs)(n.li,{children:["Transcribe with ",(0,s.jsx)(n.code,{children:"tiny"}),", ",(0,s.jsx)(n.code,{children:"small"}),", and ",(0,s.jsx)(n.code,{children:"medium"})," models"]}),"\n",(0,s.jsx)(n.li,{children:"Measure latency and accuracy for each"}),"\n",(0,s.jsx)(n.li,{children:"Create comparison table"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Starter Code"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import time\r\n\r\nmodels = ["tiny", "small", "medium"]\r\ntest_files = ["command1.wav", "command2.wav", "command3.wav"]\r\n\r\nfor model_name in models:\r\n    model = whisper.load_model(model_name)\r\n    latencies = []\r\n    \r\n    for file in test_files:\r\n        start = time.time()\r\n        result = model.transcribe(file)\r\n        latency = time.time() - start\r\n        latencies.append(latency)\r\n        print(f"{model_name}: {result[\'text\']} ({latency:.2f}s)")\r\n    \r\n    print(f"{model_name} avg latency: {sum(latencies)/len(latencies):.2f}s")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"exercise-2-confidence-filtering-medium-45-minutes",children:"Exercise 2: Confidence Filtering (Medium, 45 minutes)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),": Reject low-confidence transcriptions to prevent false commands."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Modify ",(0,s.jsx)(n.code,{children:"VoiceCommandNode"})," to log confidence scores"]}),"\n",(0,s.jsx)(n.li,{children:"Record 10 audio samples (5 clear speech, 5 noise/silence)"}),"\n",(0,s.jsx)(n.li,{children:"Find optimal confidence threshold that filters noise but keeps speech"}),"\n",(0,s.jsx)(n.li,{children:"Implement dynamic threshold adjustment based on recent history"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hints"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Confidence = ",(0,s.jsx)(n.code,{children:'np.exp(result["avg_logprob"])'})]}),"\n",(0,s.jsx)(n.li,{children:"Typical good speech: confidence > 0.6"}),"\n",(0,s.jsx)(n.li,{children:"Noise/silence: confidence < 0.3"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"exercise-3-wake-word-detection-hard-60-minutes",children:"Exercise 3: Wake Word Detection (Hard, 60 minutes)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Goal"}),': Implement "Hey Robot" wake word to reduce false activations.']}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Train or use pre-trained wake word model (Porcupine, Snowboy)"}),"\n",(0,s.jsx)(n.li,{children:"Integrate wake word detection before Whisper transcription"}),"\n",(0,s.jsx)(n.li,{children:"Measure false positive rate (wake word detected when not spoken)"}),"\n",(0,s.jsx)(n.li,{children:"Measure false negative rate (wake word not detected when spoken)"}),"\n",(0,s.jsx)(n.li,{children:"Optimize for <1% false positive rate"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hints"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Use Picovoice Porcupine (free tier: 1 custom wake word)"}),"\n",(0,s.jsx)(n.li,{children:"Alternative: Use Whisper itself with short recordings (1-2 seconds)"}),"\n",(0,s.jsx)(n.li,{children:"Wake word should trigger within 500ms for natural interaction"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsx)(n.h3,{id:"research-papers",children:"Research Papers"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Whisper: Robust Speech Recognition via Large-Scale Weak Supervision"})," (Radford et al., 2022)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"https://arxiv.org/abs/2212.04356"})}),"\n",(0,s.jsx)(n.li,{children:"Original Whisper paper explaining architecture and training"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"WebRTC Voice Activity Detection"})," (Google, 2021)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://webrtc.org/",children:"https://webrtc.org/"})}),"\n",(0,s.jsx)(n.li,{children:"VAD algorithm used in real-time communication"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"documentation",children:"Documentation"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"OpenAI Whisper GitHub"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"https://github.com/openai/whisper"})}),"\n",(0,s.jsx)(n.li,{children:"Official implementation and model weights"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"ROS 2 audio_common Package"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:(0,s.jsx)(n.a,{href:"https://github.com/ros-drivers/audio_common",children:"https://github.com/ros-drivers/audio_common"})}),"\n",(0,s.jsx)(n.li,{children:"ROS 2 audio capture and playback utilities"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"tutorials",children:"Tutorials"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Speech Recognition for Robotics"})," (NVIDIA Isaac Docs)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integration patterns for voice-controlled robots"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"VAD Algorithms Comparison"})," (Speech Processing Blog)"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Deep dive into voice activity detection techniques"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.p,{children:["You've now built a complete voice-to-action pipeline! In the next sub-chapter, ",(0,s.jsx)(n.a,{href:"/physical-ai-and-humanoid-robotics/docs/chapter-4-vla/cognitive-planning",children:"4.3 Cognitive Planning with LLMs"}),", you'll learn how to convert these voice commands into executable robot action sequences using large language models."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Key Takeaways"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\u2705 Whisper ",(0,s.jsx)(n.code,{children:"small"})," model is the sweet spot for robotics (96% accuracy, <1s latency)"]}),"\n",(0,s.jsx)(n.li,{children:"\u2705 GPU acceleration provides 10x speedup over CPU"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Voice Activity Detection reduces unnecessary transcriptions by 60%"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 Confidence thresholding prevents false commands from background noise"}),"\n",(0,s.jsx)(n.li,{children:"\u2705 ROS 2 integration enables seamless voice control of robot systems"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>l});var i=r(6540);const s={},o=i.createContext(s);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);